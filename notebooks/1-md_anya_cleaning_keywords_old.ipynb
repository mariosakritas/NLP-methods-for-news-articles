{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to clean the keyword column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import itertools # to flatten lists of lists\n",
    "import collections # to count\n",
    "from rapidfuzz import process as pr\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from data.preprocess_keywords import get_items_with_substring\n",
    "from data.make_datasets import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs for 30s\n",
    "# Load and extract data within time range\n",
    "data_file = '../data/raw/CMS_2010_to_June_2022_ENGLISH.json'\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2022-01-01'\n",
    "\n",
    "df_subset = get_data(data_file, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_unique_kws(lst_lst_keywords):\n",
    "    return len(set(list(itertools.chain(*lst_lst_keywords))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 1: put everything in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lower = [list(map(str.casefold, x)) for x in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: ['NASA', 'OSIRIS-REx', 'Bennu', 'asteroid']\n",
      "after:  ['nasa', 'osiris-rex', 'bennu', 'asteroid']\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "# TODO: entity linking? Keep upper case for names, cities, acronym\n",
    "print('before:', keywords[0])\n",
    "print('after: ', keywords_lower[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 2: split keywords that haven't been split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keywords: kw.split splits the keyword in a list of multiple keywords based on substring, itertools.chain flattens the list of lists\n",
    "keywords_lower_split = [list(itertools.chain(*[kw.split(', ') for kw in lst_kw])) for lst_kw in keywords_lower]\n",
    "keywords_lower_split = [list(itertools.chain(*[kw.split(' - ') for kw in lst_kw])) for lst_kw in keywords_lower_split] # spaces around '-' to not confuse with the ones within words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 13 \n",
      "\n",
      "before: ['transgender', 'intergender', 'zagreb', 'march', 'my body - my temple']\n",
      "after:  ['transgender', 'intergender', 'zagreb', 'march', 'my body', 'my temple'] \n",
      "\n",
      "before: ['smartphone', 'app', 'photos', 'black - b&w film emulator']\n",
      "after:  ['smartphone', 'app', 'photos', 'black', 'b&w film emulator'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_1, _, _ = get_items_with_substring(keywords_lower, ' - ')\n",
    "items_2, _, _ = get_items_with_substring(keywords_lower, ', ')\n",
    "items_with_unsplit_keywords = items_1 + items_2\n",
    "\n",
    "print('Nb of keywords changed:', len(items_with_unsplit_keywords), '\\n')\n",
    "\n",
    "for i in range(2):\n",
    "    print('before:', keywords_lower[items_with_unsplit_keywords[i]])\n",
    "    print('after: ', keywords_lower_split[items_with_unsplit_keywords[i]], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 3: remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lower_split_clean = keywords_lower_split\n",
    "\n",
    "# Replace '\\xa0' and double space with space\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\\xa0', ' '), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('  ', ' '), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "\n",
    "# Remove '\\u2002' and '.', '\" ', '\"', 'keywords: ' (replace with empty)\n",
    "keywords_lower_split_clean = [list(map(lambda x: ''.join(filter(str.isprintable, x)), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('.', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\" ', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\"', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('keywords: ', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of total keywords changed: 83 \n",
      "\n",
      "before: ['keywords: germany', 'refugees', 'asylum-seekers', 'covid-19', 'coronavirus']\n",
      "after:  ['germany', 'refugees', 'asylum-seekers', 'covid-19', 'coronavirus'] \n",
      "\n",
      "before: ['keywords: president martin vizcarra', 'peru', 'peru impeachment']\n",
      "after:  ['president martin vizcarra', 'peru', 'peru impeachment'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_unwanted_characters, _, _ = get_items_with_substring(keywords_lower, substring = 'keywords:')\n",
    "print('Nb of total keywords changed:', get_nb_unique_kws(keywords_lower_split) - get_nb_unique_kws(keywords_lower_split_clean), '\\n')\n",
    "for i in range(2):\n",
    "    print('before:', keywords_lower_split[items_with_unwanted_characters[i]])\n",
    "    print('after: ', keywords_lower_split_clean[items_with_unwanted_characters[i]], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 4: Clean sentences \n",
    "Heuristic: remove keywords that have more than 6 spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract keyword from the sentence instead of deleting it?\n",
    "n_spaces = 6 # if there are more spaces than this number, the keyword is removed\n",
    "keywords_lower_split_clean_short = [[kw for kw in lst_kw if kw.count(' ')<n_spaces] for lst_kw in keywords_lower_split_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 66 \n",
      "\n",
      "before: ['germany', 'gerd müller', 'german federal ministry for economic cooperation and development', 'zambia', 'malawi', 'namibia', 'angela merkel', 'olaf scholz']\n",
      "after:  ['germany', 'gerd müller', 'zambia', 'malawi', 'namibia', 'angela merkel', 'olaf scholz'] \n",
      "\n",
      "before: ['morocco', 'misogyny', 'domestic violence', 'feminism', 'euro-mediterranean foundation of support to human rights defenders']\n",
      "after:  ['morocco', 'misogyny', 'domestic violence', 'feminism'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_sentences = [i for i,lst_kw in enumerate(keywords_lower_split_clean) if any(list(map(lambda x: x.count(' ')>=n_spaces, lst_kw)))]\n",
    "print('Nb of keywords changed:', len(items_with_sentences), '\\n')\n",
    "for i in range(2):\n",
    "    print('before:', keywords_lower_split_clean[items_with_sentences[i]])\n",
    "    print('after: ', keywords_lower_split_clean_short[items_with_sentences[i]], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords before cleaning: 32682\n",
      "Number of unique keywords after cleaning: 30228\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords)))))\n",
    "print('Number of unique keywords after cleaning:', len(set(list(itertools.chain(*keywords_lower_split_clean_short)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(keywords[0:2])\n",
    "#print(keywords_lower_split_clean_short[0:2])\n",
    "\n",
    "#df_2019_2020 = df_subset.copy()\n",
    "#df_2019_2020['keywordStringsClean'] = keywords_lower_split_clean_short\n",
    "\n",
    "#filepath = '../data/interim/clean_keywords_2019-2021_before_FuzzyWuzzy.csv'\n",
    "#df_2019_2020.to_csv(filepath, index=False)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count occurence of each keyword\n",
    "Will be used to know which one to keep in fuzzy wuzzy (the most used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat = list(itertools.chain(*keywords_lower_split_clean_short)) # Flatten list\n",
    "keywords_freq = collections.Counter(keywords_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>3817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germany</th>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19</th>\n",
       "      <td>2641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>china</th>\n",
       "      <td>1669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russia</th>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donald trump</th>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundesliga</th>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              freq\n",
       "coronavirus   3817\n",
       "germany       3299\n",
       "covid-19      2641\n",
       "china         1669\n",
       "russia        1354\n",
       "donald trump  1333\n",
       "asia          1260\n",
       "us            1087\n",
       "eu            1078\n",
       "bundesliga     905"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For visualisation, can be removed\n",
    "keywords_freq_df = pd.DataFrame.from_dict(keywords_freq, orient='index', columns = ['freq'])\n",
    "keywords_freq_df.sort_values(by='freq', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid Fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Runs for 2min !\n",
    "# extract unique ones and remove the empty entry\n",
    "unique_keywords = list(set(keywords_flat))\n",
    "unique_keywords.remove('')\n",
    "\n",
    "# run rapid fuzz\n",
    "ratio_array= pr.cdist(unique_keywords, unique_keywords, score_cutoff = 90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find words correlating together and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sausage</th>\n",
       "      <th>ontario</th>\n",
       "      <th>internet safety</th>\n",
       "      <th>energy production</th>\n",
       "      <th>solar power</th>\n",
       "      <th>london underground</th>\n",
       "      <th>minumum wage</th>\n",
       "      <th>king vajiralongkothailand</th>\n",
       "      <th>un climate summit</th>\n",
       "      <th>gazprom</th>\n",
       "      <th>...</th>\n",
       "      <th>wool</th>\n",
       "      <th>dassault aviation</th>\n",
       "      <th>space weapon</th>\n",
       "      <th>isaac julien</th>\n",
       "      <th>people's vote</th>\n",
       "      <th>tintin</th>\n",
       "      <th>reproductive health</th>\n",
       "      <th>tizen</th>\n",
       "      <th>paul pogba</th>\n",
       "      <th>cash rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sausage  ontario  internet safety  energy production  solar power  \\\n",
       "0    100.0      0.0              0.0                0.0          0.0   \n",
       "1      0.0    100.0              0.0                0.0          0.0   \n",
       "2      0.0      0.0            100.0                0.0          0.0   \n",
       "3      0.0      0.0              0.0              100.0          0.0   \n",
       "4      0.0      0.0              0.0                0.0        100.0   \n",
       "\n",
       "   london underground  minumum wage  king vajiralongkothailand  \\\n",
       "0                 0.0           0.0                        0.0   \n",
       "1                 0.0           0.0                        0.0   \n",
       "2                 0.0           0.0                        0.0   \n",
       "3                 0.0           0.0                        0.0   \n",
       "4                 0.0           0.0                        0.0   \n",
       "\n",
       "   un climate summit  gazprom  ...  wool  dassault aviation  space weapon  \\\n",
       "0                0.0      0.0  ...   0.0                0.0           0.0   \n",
       "1                0.0      0.0  ...   0.0                0.0           0.0   \n",
       "2                0.0      0.0  ...   0.0                0.0           0.0   \n",
       "3                0.0      0.0  ...   0.0                0.0           0.0   \n",
       "4                0.0      0.0  ...   0.0                0.0           0.0   \n",
       "\n",
       "   isaac julien  people's vote  tintin  reproductive health  tizen  \\\n",
       "0           0.0            0.0     0.0                  0.0    0.0   \n",
       "1           0.0            0.0     0.0                  0.0    0.0   \n",
       "2           0.0            0.0     0.0                  0.0    0.0   \n",
       "3           0.0            0.0     0.0                  0.0    0.0   \n",
       "4           0.0            0.0     0.0                  0.0    0.0   \n",
       "\n",
       "   paul pogba  cash rate  \n",
       "0         0.0        0.0  \n",
       "1         0.0        0.0  \n",
       "2         0.0        0.0  \n",
       "3         0.0        0.0  \n",
       "4         0.0        0.0  \n",
       "\n",
       "[5 rows x 30227 columns]"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_array = pd.DataFrame(ratio_array, columns = unique_keywords)\n",
    "\n",
    "# Count number of non zero values in each row\n",
    "nb_non_zero = np.count_nonzero(np.asarray(ratio_array), axis=1) \n",
    "\n",
    "# Save indices of rows with more than 1 non-zero value\n",
    "indices_correlating_rows = [i for i, el in enumerate(list(nb_non_zero)) if el>1]\n",
    "\n",
    "df_array.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Runs for 1min !\n",
    "# Make a list of similar keywords\n",
    "all_similar_kw = []\n",
    "for i in indices_correlating_rows:\n",
    "    \n",
    "    similar_words = [keyword for val, keyword in zip(list(df_array.iloc[i]), unique_keywords) if val!=0]\n",
    "    \n",
    "    # Only adds it if it's not there already\n",
    "    if similar_words not in all_similar_kw:\n",
    "        all_similar_kw.append(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['european fires', 'european firms']\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "items = get_items_with_substring(all_similar_kw, 'european fi')\n",
    "for i in items:\n",
    "    print(all_similar_kw[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eurovision song contest 2019', 'eurovision song contest']\n",
      "['eurovision 2021', 'eurovision 2019']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "items = get_items_with_substring(all_similar_kw, 'eurovi')\n",
    "for i in items:\n",
    "    print(all_similar_kw[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sausage', 'sausages'],\n",
       " ['minumum wage', 'minimum wage'],\n",
       " ['un climate summit', 'climate summit'],\n",
       " ['sex abuse scandals', 'sexual abuse scandals', 'sex abuse scandal'],\n",
       " ['champiosn league',\n",
       "  'champions leage',\n",
       "  'champions league',\n",
       "  'champion league']]"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_similar_kw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6489"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_kws = list(itertools.chain(*all_similar_kw))\n",
    "len(input_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [keyword.split() for keyword in df.firstKeyword] # tokenise: split into words\n",
    "model_ft = FastText(tokens, vector_size = 500) # model_ft.wv['pollution']: get word vector for a specific word; model_ft.wv.similar_by_word('pollution', topn=5): Look for the 5 most similar words\n",
    "\n",
    "word_vectors = [np.array(model_ft.wv[token]) for token in tokens]\n",
    "word_vectors_av = np.array([np.average(word_vector, axis=0) for word_vector in word_vectors]) # if: ['dw','akademie'] = 2 vectors = we average them\n",
    "\n",
    "features = word_vectors_av\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.     ,  94.44444,  94.44444,   0.     ],\n",
       "       [ 94.44444, 100.     ,   0.     ,  90.     ],\n",
       "       [ 94.44444,   0.     , 100.     ,   0.     ],\n",
       "       [  0.     ,  90.     ,   0.     , 100.     ]], dtype=float32)"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['ayatollah khomeini', 'ayatollah khameini', 'ayatollah khomenei', 'ayatollah ali khameini']\n",
    "# rapid fuzz, keeps track of similarities above 90\n",
    "similarity_matrix= pr.cdist(keywords, keywords, score_cutoff = 90) \n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eeach new line is considered as a single keyword:\n",
      "['ayatollah ruhollah khomeini', 'grand ayatollah ruhollah khomeini']\n",
      "['ayatollah khomenei', 'ayatollah khamenei', 'ayatollah khomeini']\n",
      "['ayatollah khomenei', 'ayatollah khamenei', 'ayatollah ali khamenei']\n",
      "['ayatollah khamenei', 'ayatollah ali khamenei', 'ayotallah ali khamenei', 'ayatollah ali khameini']\n",
      "['ayatollah ali khamenei', 'ayotallah ali khamenei']\n",
      "['ayatollah khomenei', 'ayatollah khomeini']\n",
      "['ayatollah ali khamenei', 'ayatollah ali khameini']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Problem, should be considered as the same keyword and not different ones..\n",
    "# In the line above, add a new word to a sublist instead of making a new list of some elements match ?\n",
    "items = get_items_with_substring(all_similar_kw, 'ayatollah')\n",
    "print('Eeach new line is considered as a single keyword:')\n",
    "for i in items:\n",
    "    print(all_similar_kw[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify similar keywords\n",
    "Replace by most frequent one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in the ones which have equal word number, and the ones which don't, becase they need different processing\n",
    "similar_kws_same_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))==1]\n",
    "similar_kws_diff_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['european fires', 'european firms']\n"
     ]
    }
   ],
   "source": [
    "#### TEST\n",
    "\n",
    "# TODO: problem: ['european fires', 'european firms']: word embedings\n",
    "# TODO: problem: [\"new year's\", 'countdown', 'resolutions'] replaced by [\"new year's\", 'countdown', 'solutions']\n",
    "\n",
    "item = get_items_with_substring(similar_kws_same_word_nb, 'european firms')\n",
    "for i in item:\n",
    "    print(similar_kws_same_word_nb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Focus on similar_kws_diff_word_nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Runs for 1min50 !\n",
    "\n",
    "## Focus on similar_kws_same_word_nb\n",
    "\n",
    "# TODO: problem: ['european fires', 'european firms']: word embedings\n",
    "# TODO: problem: [\"new year's\", 'countdown', 'resolutions'] replaced by [\"new year's\", 'countdown', 'solutions']\n",
    "\n",
    "# Replace\n",
    "right_kw = [sim_kws[np.argmax([keywords_freq[word] for word in sim_kws])] for sim_kws in similar_kws_same_word_nb]\n",
    "\n",
    "keywords_flat = list(chain.from_iterable(keywords_lower_split_clean_short))\n",
    "\n",
    "replacement_only = [[right_kw[i] for i, j in enumerate(similar_kws_same_word_nb) if word in j] for word in keywords_flat]\n",
    "\n",
    "keywords_flat_post = [replacement_only[i][0] if replacement_only[i] != [] else keywords_flat[i] for i in range(len(keywords_flat))]\n",
    "\n",
    "def gen_list_of_lists(original_list, new_structure):\n",
    "    assert len(original_list) == sum(new_structure), \\\n",
    "    \"The number of elements in the original list and desired structure don't match\"\n",
    "    list_of_lists = [[original_list[i + sum(new_structure[:j])] for i in range(new_structure[j])] \\\n",
    "                     for j in range(len(new_structure))]\n",
    "    return list_of_lists\n",
    "\n",
    "keywords_replaced = gen_list_of_lists(keywords_flat_post, [len(x) for x in keywords_lower_split_clean_short])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq of: sausage  =  7\n",
      "freq of: sausages  =  3\n",
      "\n",
      "freq of: minumum wage  =  1\n",
      "freq of: minimum wage  =  10\n",
      "\n",
      "kw_before: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khamenei']\n",
      "kw_after: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khomeini'] \n",
      "\n",
      "kw_before: [\"new year's\", 'countdown', 'resolutions']\n",
      "kw_after: [\"new year's\", 'countdown', 'solutions'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example for visualisation, can be removed later\n",
    "for sim_kws in all_similar_kw[0:2]:\n",
    "    for word in sim_kws:\n",
    "        print('freq of:', word, ' = ', keywords_freq[word])\n",
    "    print('')\n",
    "\n",
    "n_show = 2 # how many examples to show\n",
    "\n",
    "i_show = 0\n",
    "for kw_before, kw_after in zip(keywords_lower_split_clean_short, keywords_replaced):\n",
    "    if kw_before != kw_after and i_show < n_show:\n",
    "        print('kw_before:', kw_before)\n",
    "        print('kw_after:', kw_after, '\\n')\n",
    "        i_show += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords before cleaning: 32682\n",
      "Number of unique keywords after first clean: 30228\n",
      "Number of unique keywords after rapidfuzz replacing: 27988\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords)))))\n",
    "print('Number of unique keywords after first clean:', len(set(list(itertools.chain(*keywords_lower_split_clean_short)))))\n",
    "print('Number of unique keywords after rapidfuzz replacing:', len(set(list(itertools.chain(*keywords_replaced)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          lastModifiedDate  \\\n",
       "0  2019-01-01T03:57:28.904Z  2019-01-01T03:57:28.904Z   \n",
       "1  2019-01-01T06:11:50.527Z  2019-01-01T06:11:50.527Z   \n",
       "2  2019-01-01T06:14:35.563Z  2019-01-01T06:14:35.563Z   \n",
       "3  2019-01-01T08:26:11.599Z  2019-01-01T08:26:11.599Z   \n",
       "4  2019-01-01T09:05:00.736Z  2019-01-01T09:05:00.736Z   \n",
       "\n",
       "                                      keywordStrings  \\\n",
       "0                [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "1  [English Channel, migration, boats, illegal im...   \n",
       "2  [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "3                   [Japan, Tokyo, Harajuku, attack]   \n",
       "4  [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "\n",
       "                        keywordStringsCleanAfterFuzz  \n",
       "0                [nasa, osiris-rex, bennu, asteroid]  \n",
       "1  [english channel, migration, boats, illegal im...  \n",
       "2  [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "3                   [japan, tokyo, harajuku, attack]  \n",
       "4  [asia, bangladesh, elections, kamal hossain, s...  "
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ids = list(df_subset['lastModifiedDate'])\n",
    "list_dates = list(df_subset['lastModifiedDate'])\n",
    "list_kws = list(df_subset['keywordStrings'])\n",
    "list_new_kws = keywords_replaced\n",
    "\n",
    "df_2019_2020 = pd.DataFrame(list(zip(list_ids, list_dates, list_kws, list_new_kws)), columns=['id', 'lastModifiedDate', 'keywordStrings', 'keywordStringsCleanAfterFuzz'])\n",
    "\n",
    "df_2019_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/interim/clean_keywords_2019-2021_after_RapidFuzz.json'\n",
    "\n",
    "# storing the data in JSON format\n",
    "df_2019_2020.to_json(filepath, orient = 'split', compression = 'infer', index = 'true')\n",
    " \n",
    "# reading the JSON file\n",
    "# filepath = '../data/interim/clean_keywords_2019-2021_after_FuzzyWuzzy.json'\n",
    "# df_loaded = pd.read_json(filepath, orient ='split', compression = 'infer')\n",
    "# flat_keywords = list(itertools.chain(*list(df['keywordStringsCleanAfterFuzz'])))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
