{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(lst_keywords, wv):\n",
    "    \n",
    "    # Creating the vectorizer \n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit the model with our data (each keyword becomes a feature, some are split)\n",
    "    X = vectorizer.fit_transform(lst_keywords)\n",
    "\n",
    "    # Make an array and fills it in\n",
    "    CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Words in the vocabulary (some keywords are split)\n",
    "    WordsVocab=CountVectorizedData.columns\n",
    "\n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "\n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVectorizedData.shape[0]):\n",
    "\n",
    "        # initiating a sentence with all zeros\n",
    "        sentence = np.zeros(300)\n",
    "\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVectorizedData.iloc[i , :] >= 1]:\n",
    "            if word in wv.index_to_key:\n",
    "                sentence = sentence + wv[word] \n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data = W2Vec_Data.append(pd.DataFrame([sentence]))\n",
    "\n",
    "    return W2Vec_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DW data\n",
    "df_dw = pd.read_json('../data/interim/clean_keywords_2022-01-01_2023-01-01.json', orient ='split', compression = 'infer')\n",
    "\n",
    "# Load Google data\n",
    "df_google = pd.read_json('../data/interim/2022-daily-trending-searches.json', orient ='split', compression = 'infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs for 1min20\n",
    "# Load pre-trained zero shot learning model\n",
    "pipe = pipeline(model=\"facebook/bart-large-mnli\") \n",
    "\n",
    "# Loads word2vec google model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match datasets in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date range in data\n",
    "start_date_dw = pd.to_datetime(df_dw['Date']).min()\n",
    "end_date_dw = pd.to_datetime(df_dw['Date']).max()\n",
    "\n",
    "# Remove rows witn no category\n",
    "df_dw.dropna(subset=['cleanFocusCategory'], inplace = True)\n",
    "df_dw.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Keeps only google data within DW data date range\n",
    "df_google.sort_values(by ='date', inplace = True) \n",
    "mask = (pd.to_datetime(df_google['date']) > start_date_dw) & (pd.to_datetime(df_google['date']) <= end_date_dw)\n",
    "df_google_subset = df_google.loc[mask].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 categories:  ['Migration', 'Health', 'Travel', 'Law and Justice', 'Learning German', 'Digital World', 'Cars and Transportation', 'Media', 'Business', 'Sports', 'Nature and Environment', 'Offbeat', 'Education', 'Human Rights', 'Culture', 'Technology', 'Politics', 'History', 'Society', 'Lifestyle', 'Catastrophe', 'Science', 'Innovation', 'Religion']\n"
     ]
    }
   ],
   "source": [
    "# Target variable (category)\n",
    "focus_category_list = list(set(df_dw['cleanFocusCategory']))\n",
    "print(len(focus_category_list), 'categories: ', focus_category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Topic' by None and remove them for now (TODO)\n",
    "df_google_subset['topic_type'].replace({'Topic': None}, inplace = True)\n",
    "df_google_subset.dropna(subset=['topic_type'], inplace = True)\n",
    "\n",
    "# Make a new column combining those 2\n",
    "df_google_subset['topic_title_type'] = df_google_subset['topic_title'] + ', ' + df_google_subset['topic_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = 1\n",
    "\n",
    "if load: \n",
    "    df_prediction = pd.read_json('../data/interim/zero_shot_prediction_google_keywords_2022-01-01_2023-01-01.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # Runs for 14min (for 100 google keyword lines)\n",
    "    df_prediction = df_google_subset.iloc[0:100][['topic_title', 'topic_type', 'topic_title_type']].copy()\n",
    "\n",
    "    # Runs the model\n",
    "    category_outputs = [pipe(kw, candidate_labels = focus_category_list) for kw in df_prediction['topic_title_type'].to_list()]\n",
    "    labels = list(map(lambda x: x['labels'][0], category_outputs))\n",
    "\n",
    "    # Add a column to dataframe\n",
    "    df_prediction['Predicted category'] = labels\n",
    "    df_prediction.to_json('../data/interim/zero_shot_prediction_google_keywords_2022-01-01_2023-01-01.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess performance: Compare to similar DW keyword - category pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation of DW keywords\n",
    "load = 1\n",
    "\n",
    "if load: \n",
    "    vec_keywords_dw = pd.read_json('../data/interim/vectorised_clean_keywords_2022-01-01_2023-01-01.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # runs during 10 min\n",
    "    lst_keywords_dw = df_dw['keywordStringsCleanAfterFuzz'].astype(str)\n",
    "    vec_keywords_dw = get_vectors(lst_keywords_dw, wv)\n",
    "    vec_keywords_dw.to_json('../data/interim/vectorised_clean_keywords_2022-01-01_2023-01-01.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation of DW category\n",
    "category_dw = focus_category_list\n",
    "vec_category_dw = get_vectors(category_dw, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google topic on DW keywords\n",
    "\n",
    "# Vectorisation of Google keywords\n",
    "lst_keywords_ggl = df_prediction['topic_type'].astype(str) # topic_type or topic_title or topic_title_type (best predictor: topic_type)\n",
    "vec_keywords_ggl = get_vectors(lst_keywords_ggl, wv)\n",
    "\n",
    "# Compute distances: google keywords vs DW keywords \n",
    "ggl_dw_word_distances = cosine_similarity(vec_keywords_ggl, vec_keywords_dw) \n",
    "\n",
    "# Get indices of similar (above threshold) DW keywords\n",
    "distance_threshold = 0.5\n",
    "ind_ggl_to_dw = [[ind for ind in range(len(ggl_word_list)) if ggl_word_list[ind] > distance_threshold] for ggl_word_list in ggl_dw_word_distances]\n",
    "\n",
    "# Extract most frequet category for similar DW keywords\n",
    "most_freq_categories = [df_dw['cleanFocusCategory'][lst_keywords_dw.index[ind_ggl_to_dw[i]]].mode()[0] if len(ind_ggl_to_dw[i]) > 0 else None for i in range(len(ind_ggl_to_dw))]\n",
    "\n",
    "# Make a new column in df\n",
    "df_prediction['Category of similar DW keywords'] = most_freq_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google topic on DW categories \n",
    "\n",
    "# Vectorisation of Google keywords\n",
    "lst_keywords_ggl = df_prediction['topic_type'].astype(str) # topic_type or topic_title or topic_title_type (best predictor: topic_type)\n",
    "vec_keywords_ggl = get_vectors(lst_keywords_ggl, wv)\n",
    "\n",
    "# Compute distances: google keywords vs DW categories\n",
    "ggl_dw_categ_distances = cosine_similarity(vec_keywords_ggl, vec_category_dw) \n",
    "\n",
    "# Get indices of similar (above threshold) DW keywords\n",
    "closest_categ = [[category_dw[dist.argmax()], round(dist.max(),2)] for dist in ggl_dw_categ_distances]\n",
    "\n",
    "# Make a new column in df\n",
    "distance_threshold = 0.2\n",
    "df_prediction['Most similar DW category'] = [row[0] if row[1] > distance_threshold else None for row in closest_categ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Compared to category of similar keywords: 0.67\n",
      "Compared to similar categories: 0.7\n"
     ]
    }
   ],
   "source": [
    "accuracy_sim_dw_keywords = sum(df_prediction['Predicted category'] == df_prediction['Category of similar DW keywords']) / len(df_prediction)\n",
    "accuracy_sim_dw_categ = sum(df_prediction['Predicted category'] == df_prediction['Most similar DW category']) / len(df_prediction)\n",
    "print('Accuracy:')\n",
    "print('Compared to category of similar keywords:', accuracy_sim_dw_keywords) \n",
    "print('Compared to similar categories:', accuracy_sim_dw_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_title</th>\n",
       "      <th>topic_type</th>\n",
       "      <th>topic_title_type</th>\n",
       "      <th>Predicted category</th>\n",
       "      <th>Category of similar DW keywords</th>\n",
       "      <th>Most similar DW category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>National Football League Playoffs</td>\n",
       "      <td>Championship</td>\n",
       "      <td>National Football League Playoffs, Championship</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Yellowstone</td>\n",
       "      <td>Drama series</td>\n",
       "      <td>Yellowstone, Drama series</td>\n",
       "      <td>Media</td>\n",
       "      <td>None</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Secondary school</td>\n",
       "      <td>School category</td>\n",
       "      <td>Secondary school, School category</td>\n",
       "      <td>Education</td>\n",
       "      <td>None</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Week</td>\n",
       "      <td>Unit of time</td>\n",
       "      <td>Week, Unit of time</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>None</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Tampa</td>\n",
       "      <td>City in Florida</td>\n",
       "      <td>Tampa, City in Florida</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Learning German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Pittsburgh Steelers</td>\n",
       "      <td>American football team</td>\n",
       "      <td>Pittsburgh Steelers, American football team</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>National Football League Playoffs</td>\n",
       "      <td>Championship</td>\n",
       "      <td>National Football League Playoffs, Championship</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NFL</td>\n",
       "      <td>League</td>\n",
       "      <td>NFL, League</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Super Bowl</td>\n",
       "      <td>Championship</td>\n",
       "      <td>Super Bowl, Championship</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Kansas City Chiefs</td>\n",
       "      <td>American football team</td>\n",
       "      <td>Kansas City Chiefs, American football team</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           topic_title              topic_type  \\\n",
       "35   National Football League Playoffs            Championship   \n",
       "42                         Yellowstone            Drama series   \n",
       "41                    Secondary school         School category   \n",
       "40                                Week            Unit of time   \n",
       "39                               Tampa         City in Florida   \n",
       "..                                 ...                     ...   \n",
       "194                Pittsburgh Steelers  American football team   \n",
       "200  National Football League Playoffs            Championship   \n",
       "198                                NFL                  League   \n",
       "196                         Super Bowl            Championship   \n",
       "195                 Kansas City Chiefs  American football team   \n",
       "\n",
       "                                    topic_title_type Predicted category  \\\n",
       "35   National Football League Playoffs, Championship             Sports   \n",
       "42                         Yellowstone, Drama series              Media   \n",
       "41                 Secondary school, School category          Education   \n",
       "40                                Week, Unit of time            Offbeat   \n",
       "39                            Tampa, City in Florida            Offbeat   \n",
       "..                                               ...                ...   \n",
       "194      Pittsburgh Steelers, American football team             Sports   \n",
       "200  National Football League Playoffs, Championship             Sports   \n",
       "198                                      NFL, League             Sports   \n",
       "196                         Super Bowl, Championship             Sports   \n",
       "195       Kansas City Chiefs, American football team             Sports   \n",
       "\n",
       "    Category of similar DW keywords Most similar DW category  \n",
       "35                           Sports                   Sports  \n",
       "42                             None                  Offbeat  \n",
       "41                             None                Education  \n",
       "40                             None                  History  \n",
       "39                         Politics          Learning German  \n",
       "..                              ...                      ...  \n",
       "194                          Sports                   Sports  \n",
       "200                          Sports                   Sports  \n",
       "198                          Sports                   Sports  \n",
       "196                          Sports                   Sports  \n",
       "195                          Sports                   Sports  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
