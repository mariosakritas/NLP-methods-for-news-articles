{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 12:32:50.472737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 12:32:51.275440: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-30 12:32:51.275469: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-30 12:32:53.588548: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 12:32:53.588702: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 12:32:53.588712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(lst_keywords, wv):\n",
    "    \n",
    "    # Creating the vectorizer \n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit the model with our data (each keyword becomes a feature, some are split)\n",
    "    X = vectorizer.fit_transform(lst_keywords)\n",
    "\n",
    "    # Make an array and fills it in\n",
    "    CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Words in the vocabulary (some keywords are split)\n",
    "    WordsVocab=CountVectorizedData.columns\n",
    "\n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "\n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVectorizedData.shape[0]):\n",
    "\n",
    "        # initiating a sentence with all zeros\n",
    "        sentence = np.zeros(300)\n",
    "\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVectorizedData.iloc[i , :] >= 1]:\n",
    "            if word in wv.index_to_key:\n",
    "                sentence = sentence + wv[word] \n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data = W2Vec_Data.append(pd.DataFrame([sentence]))\n",
    "\n",
    "    return W2Vec_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DW data\n",
    "df_dw = pd.read_json('../data/interim/clean_keywords_2022-01-01_2023-01-01.json', orient ='split', compression = 'infer')\n",
    "\n",
    "# Load Google data\n",
    "df_google = pd.read_json('../data/interim/2022-daily-trending-searches.json', orient ='split', compression = 'infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs for 1min20\n",
    "# Load pre-trained zero shot learning model\n",
    "pipe = pipeline(model=\"facebook/bart-large-mnli\") \n",
    "\n",
    "# Loads word2vec google model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match datasets in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date range in data\n",
    "start_date_dw = pd.to_datetime(df_dw['Date']).min()\n",
    "end_date_dw = pd.to_datetime(df_dw['Date']).max()\n",
    "\n",
    "# Remove rows witn no category\n",
    "df_dw.dropna(subset=['cleanFocusCategory'], inplace = True)\n",
    "df_dw.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Keeps only google data within DW data date range\n",
    "df_google.sort_values(by ='date', inplace = True) \n",
    "mask = (pd.to_datetime(df_google['date']) > start_date_dw) & (pd.to_datetime(df_google['date']) <= end_date_dw)\n",
    "df_google_subset = df_google.loc[mask].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 categories:  ['Catastrophe', 'Society', 'Education', 'Innovation', 'Health', 'Science', 'Offbeat', 'Business', 'Media', 'Cars and Transportation', 'Technology', 'Travel', 'Digital World', 'Sports', 'Learning German', 'History', 'Law and Justice', 'Lifestyle', 'Religion', 'Culture', 'Migration', 'Human Rights', 'Politics', 'Nature and Environment']\n"
     ]
    }
   ],
   "source": [
    "# Target variable (category)\n",
    "focus_category_list = list(set(df_dw['cleanFocusCategory']))\n",
    "print(len(focus_category_list), 'categories: ', focus_category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Topic' by None and remove them for now (TODO)\n",
    "df_google_subset['topic_type'].replace({'Topic': None}, inplace = True)\n",
    "df_google_subset.dropna(subset=['topic_type'], inplace = True)\n",
    "\n",
    "# Make a new column combining those 2\n",
    "df_google_subset['topic_title_type'] = df_google_subset['topic_title'] + ', ' + df_google_subset['topic_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Google keywords \n",
    "load = 1\n",
    "\n",
    "if load: \n",
    "    df_prediction = pd.read_json('../data/interim/zero_shot_prediction_google_keywords_2022-01-01_2023-01-01_all.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # Runs for 14min (for 100 google keyword lines)\n",
    "    df_prediction = df_google_subset[['topic_title', 'topic_type', 'topic_title_type']].copy()\n",
    "\n",
    "    # Runs the model\n",
    "    category_outputs = [pipe(kw, candidate_labels = focus_category_list) for kw in df_prediction['topic_title_type'].to_list()]\n",
    "    labels = list(map(lambda x: x['labels'][0], category_outputs))\n",
    "\n",
    "    # Add a column to dataframe\n",
    "    df_prediction['Predicted category'] = labels\n",
    "    df_prediction.to_json('../data/interim/zero_shot_prediction_google_keywords_2022-01-01_2023-01-01.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On DW keywords (to assess accuracy)\n",
    "load = 0\n",
    "\n",
    "if load: \n",
    "    df_dw_prediction = pd.read_json('../data/interim/zero_shot_prediction_dw_keywords_2022-01-01_2023-01-01_200.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # 1min for 4 words\n",
    "    # 1hour for 200 words? started at 13:13\n",
    "    df_dw_prediction = df_dw[0:200].copy()\n",
    "\n",
    "    # Runs the model\n",
    "    category_outputs = [pipe(', '.join(kw), candidate_labels = focus_category_list) for kw in df_dw_prediction['keywordStringsCleanAfterFuzz']]\n",
    "    labels = list(map(lambda x: x['labels'][0], category_outputs))\n",
    "\n",
    "    # Add a column to dataframe\n",
    "    df_dw_prediction['Predicted category'] = labels\n",
    "    df_dw_prediction.to_json('../data/interim/zero_shot_prediction_dw_keywords_2022-01-01_2023-01-01_200.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess performance: Compare to similar DW keyword - category pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation of DW keywords\n",
    "load = 1\n",
    "\n",
    "if load: \n",
    "    vec_keywords_dw = pd.read_json('../data/interim/vectorised_clean_keywords_2022-01-01_2023-01-01.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # runs during 10 min\n",
    "    lst_keywords_dw = df_dw['keywordStringsCleanAfterFuzz'].astype(str)\n",
    "    vec_keywords_dw = get_vectors(lst_keywords_dw, wv)\n",
    "    vec_keywords_dw.to_json('../data/interim/vectorised_clean_keywords_2022-01-01_2023-01-01.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation of DW category\n",
    "category_dw = focus_category_list\n",
    "vec_category_dw = get_vectors(category_dw, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google topic on DW keywords\n",
    "\n",
    "# Vectorisation of Google keywords\n",
    "lst_keywords_ggl = df_prediction['topic_type'].astype(str) # topic_type or topic_title or topic_title_type (best predictor: topic_type)\n",
    "vec_keywords_ggl = get_vectors(lst_keywords_ggl, wv)\n",
    "\n",
    "# Compute distances: google keywords vs DW keywords \n",
    "ggl_dw_word_distances = cosine_similarity(vec_keywords_ggl, vec_keywords_dw) \n",
    "\n",
    "# Get indices of similar (above threshold) DW keywords\n",
    "distance_threshold = 0.4\n",
    "ind_ggl_to_dw = [[ind for ind in range(len(ggl_word_list)) if ggl_word_list[ind] > distance_threshold] for ggl_word_list in ggl_dw_word_distances]\n",
    "\n",
    "# Extract most frequet category for similar DW keywords\n",
    "most_freq_categories = [df_dw['cleanFocusCategory'][lst_keywords_dw.index[ind_ggl_to_dw[i]]].mode()[0] if len(ind_ggl_to_dw[i]) > 0 else None for i in range(len(ind_ggl_to_dw))]\n",
    "\n",
    "# Make a new column in df\n",
    "df_prediction['Category of similar DW keywords'] = most_freq_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared to similar categories: 0.47164948453608246\n"
     ]
    }
   ],
   "source": [
    "distance_threshold = 0.2\n",
    "\n",
    "# Vectorisation of Google keywords\n",
    "lst_keywords_ggl = df_prediction['topic_type'].astype(str) # topic_type or topic_title or topic_title_type (best predictor: topic_type)\n",
    "vec_keywords_ggl = get_vectors(lst_keywords_ggl, wv)\n",
    "\n",
    "# Compute distances: google keywords vs DW categories\n",
    "ggl_dw_categ_distances = cosine_similarity(vec_keywords_ggl, vec_category_dw) \n",
    "\n",
    "# Get indices of similar (above threshold) DW keywords\n",
    "closest_categ = [[category_dw[dist.argmax()], round(dist.max(),2)] for dist in ggl_dw_categ_distances]\n",
    "\n",
    "# Make a new column in df\n",
    "df_prediction['Most similar DW category'] = [row[0] if row[1] > distance_threshold else None for row in closest_categ]\n",
    "\n",
    "# Check prediction\n",
    "accuracy_sim_dw_categ = sum(df_prediction['Predicted category'] == df_prediction['Most similar DW category']) / len(df_prediction)\n",
    "print('Compared to similar categories:', accuracy_sim_dw_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Compared to category of similar keywords: 0.3711340206185567\n",
      "Compared to similar categories: 0.47164948453608246\n"
     ]
    }
   ],
   "source": [
    "accuracy_sim_dw_keywords = sum(df_prediction['Predicted category'] == df_prediction['Category of similar DW keywords']) / len(df_prediction)\n",
    "accuracy_sim_dw_categ = sum(df_prediction['Predicted category'] == df_prediction['Most similar DW category']) / len(df_prediction)\n",
    "print('Accuracy:')\n",
    "print('Compared to category of similar keywords:', accuracy_sim_dw_keywords) \n",
    "print('Compared to similar categories:', accuracy_sim_dw_categ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_title</th>\n",
       "      <th>topic_type</th>\n",
       "      <th>topic_title_type</th>\n",
       "      <th>Predicted category</th>\n",
       "      <th>Category of similar DW keywords</th>\n",
       "      <th>Most similar DW category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>National Football League Playoffs</td>\n",
       "      <td>Championship</td>\n",
       "      <td>National Football League Playoffs, Championship</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Yellowstone</td>\n",
       "      <td>Drama series</td>\n",
       "      <td>Yellowstone, Drama series</td>\n",
       "      <td>Media</td>\n",
       "      <td>None</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Secondary school</td>\n",
       "      <td>School category</td>\n",
       "      <td>Secondary school, School category</td>\n",
       "      <td>Education</td>\n",
       "      <td>None</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Week</td>\n",
       "      <td>Unit of time</td>\n",
       "      <td>Week, Unit of time</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>None</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Tampa</td>\n",
       "      <td>City in Florida</td>\n",
       "      <td>Tampa, City in Florida</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Learning German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>Boris Johnson</td>\n",
       "      <td>Member of Parliament of the United Kingdom</td>\n",
       "      <td>Boris Johnson, Member of Parliament of the Uni...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>Thor</td>\n",
       "      <td>Film series</td>\n",
       "      <td>Thor, Film series</td>\n",
       "      <td>Media</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>2022 film</td>\n",
       "      <td>Thor: Love and Thunder, 2022 film</td>\n",
       "      <td>Media</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>Shinzo Abe</td>\n",
       "      <td>Former Prime Minister of Japan</td>\n",
       "      <td>Shinzo Abe, Former Prime Minister of Japan</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Learning German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Country in East Asia</td>\n",
       "      <td>Japan, Country in East Asia</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Digital World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            topic_title  \\\n",
       "35    National Football League Playoffs   \n",
       "42                          Yellowstone   \n",
       "41                     Secondary school   \n",
       "40                                 Week   \n",
       "39                                Tampa   \n",
       "...                                 ...   \n",
       "1963                      Boris Johnson   \n",
       "1967                               Thor   \n",
       "1968             Thor: Love and Thunder   \n",
       "1965                         Shinzo Abe   \n",
       "1966                              Japan   \n",
       "\n",
       "                                      topic_type  \\\n",
       "35                                  Championship   \n",
       "42                                  Drama series   \n",
       "41                               School category   \n",
       "40                                  Unit of time   \n",
       "39                               City in Florida   \n",
       "...                                          ...   \n",
       "1963  Member of Parliament of the United Kingdom   \n",
       "1967                                 Film series   \n",
       "1968                                   2022 film   \n",
       "1965              Former Prime Minister of Japan   \n",
       "1966                        Country in East Asia   \n",
       "\n",
       "                                       topic_title_type Predicted category  \\\n",
       "35      National Football League Playoffs, Championship             Sports   \n",
       "42                            Yellowstone, Drama series              Media   \n",
       "41                    Secondary school, School category          Education   \n",
       "40                                   Week, Unit of time            Offbeat   \n",
       "39                               Tampa, City in Florida            Offbeat   \n",
       "...                                                 ...                ...   \n",
       "1963  Boris Johnson, Member of Parliament of the Uni...           Politics   \n",
       "1967                                  Thor, Film series              Media   \n",
       "1968                  Thor: Love and Thunder, 2022 film              Media   \n",
       "1965         Shinzo Abe, Former Prime Minister of Japan           Politics   \n",
       "1966                        Japan, Country in East Asia            Offbeat   \n",
       "\n",
       "     Category of similar DW keywords Most similar DW category  \n",
       "35                            Sports                   Sports  \n",
       "42                              None                  Offbeat  \n",
       "41                              None                Education  \n",
       "40                              None                  History  \n",
       "39                          Politics          Learning German  \n",
       "...                              ...                      ...  \n",
       "1963                        Politics                  Society  \n",
       "1967                         Culture                  Offbeat  \n",
       "1968                         Culture                  Offbeat  \n",
       "1965                        Politics          Learning German  \n",
       "1966                        Politics            Digital World  \n",
       "\n",
       "[1164 rows x 6 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
