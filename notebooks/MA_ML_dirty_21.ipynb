{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to show how a model (SVC) can be traine dto classify keywords into one of 21 \n",
    "parent DW categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import math\n",
    "import os, sys, pickle, urllib.request\n",
    "import os.path as op\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import gensim\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(yt, yp):\n",
    "    results_pos = {}\n",
    "    results_pos['accuracy'] = accuracy_score(yt, yp)\n",
    "    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='weighted')\n",
    "    results_pos['recall'] = recall\n",
    "    results_pos['precision'] = precision\n",
    "    results_pos['f1score'] = f_beta\n",
    "    return results_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "data_dir = '/home/marios/data/dw-project-data/CMS_2010_to_June_2022_ENGLISH.json'\n",
    "f = open(data_dir)\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "df = json.load(f)\n",
    "df = pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns that are nnot needed\n",
    "df = df[['id', 'keywordStrings', 'thematicFocusCategory', 'lastModifiedDate']]\n",
    "#drop missing values\n",
    "df = df.dropna()\n",
    "#clean Focus category\n",
    "df['thematicFocusCategory'] = df['thematicFocusCategory'].apply(lambda x:x['name'] if x is not None else x)\n",
    "df = df[df['thematicFocusCategory']!=None]\n",
    "df['thematicFocusCategory'] = df['thematicFocusCategory'].astype(str)\n",
    "\n",
    "#replacee secondary categories with primary\n",
    "children_dict = {'Architecture':'Culture', 'Design':'Culture', 'Film':'Culture', 'Arts':'Culture', \n",
    "                 'Literature':'Culture', 'Music':'Culture', 'Dance':'Culture', 'Theater':'Culture',\n",
    "                   'Climate':'Nature and Environment',\n",
    "                  'Conflicts':'Politics', 'Terrorism':'Politics', \n",
    "                  'Corruption':'Law and Justice', 'Crime':'Law and Justice', 'Rule of Law':'Law and Justice',\n",
    "                    'Press Freedom':'Law and Justice', \n",
    "                  'Diversity':'Human Rights', 'Freedom of Speech':'Human Rights', 'Equality':'Human Rights', \n",
    "                'Soccer': 'Sports',\n",
    "                    'Trade':'Business', 'Globalization':'Business', 'Food Security':'Business'\n",
    "}\n",
    "\n",
    "secondary_cts = [val for val in children_dict.keys()]\n",
    "\n",
    "df['thematicFocusCategory'] = df['thematicFocusCategory'].apply(lambda x: children_dict[x] if x in secondary_cts else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4465/2641947804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let's load a pre-trained word2vec model from google- you may need to download this first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m GoogleModel = gensim.models.KeyedVectors.load_word2vec_format('/home/marios/local_data_s2ds/GoogleNews-vectors-negative300.bin',\n\u001b[0;32m----> 3\u001b[0;31m                                                                binary=True,)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1723\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1724\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m         )\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m             _word2vec_read_binary(\n\u001b[0;32m-> 2070\u001b[0;31m                 \u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_chunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2071\u001b[0m             )\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         processed_words, chunk = _add_bytes_to_kv(\n\u001b[0;32m-> 1965\u001b[0;31m             kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[0m\u001b[1;32m   1966\u001b[0m         \u001b[0mtot_processed_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocessed_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_chunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbinary_chunk_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_add_bytes_to_kv\u001b[0;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[0;31m# Some binary files are reported to have obsolete new line in the beginning of word, remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1946\u001b[0;31m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1947\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbytes_per_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's load a pre-trained word2vec model from google- you may need to download this first\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format('/home/marios/local_data_s2ds/GoogleNews-vectors-negative300.bin',\n",
    "                                                               binary=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning German: 75\n",
      "Offbeat: 38\n",
      "Innovation: 5\n"
     ]
    }
   ],
   "source": [
    "#drop categories which are rare \n",
    "elements_count = collections.Counter(df.thematicFocusCategory)\n",
    "# printing the element and the frequency\n",
    "for key, value in elements_count.items():\n",
    "    if value <100:\n",
    "        print(f\"{key}: {value}\")\n",
    "        df = df[df.thematicFocusCategory != key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Africalink, Top Story, Africa on the Move, Making a Difference, Behind the Headlines, Your Say, Crossroads Generation',\n",
       " 'Commerzbank, job cuts, administration, retail banking',\n",
       " \"Moody's, Turkey, ratings agency, junk status\",\n",
       " 'Syria, Aleppo, war crimes, water, UNICEF',\n",
       " 'Conflict Zone, Talk, link']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creeate corpus which will be passed on for vectorization\n",
    "# heree one token will bee a string containing all the keywors associateed with one article. \n",
    "# you can change this accordingly. \n",
    "corpus = [l for l in df['keywordStrings'].apply(lambda x: ', '.join(x))]\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79795, 38605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marios/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>01</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>08</th>\n",
       "      <th>0rg</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>øystein</th>\n",
       "      <th>út</th>\n",
       "      <th>überall</th>\n",
       "      <th>ünal</th>\n",
       "      <th>ünker</th>\n",
       "      <th>ľudmila</th>\n",
       "      <th>şehriban</th>\n",
       "      <th>štefániková</th>\n",
       "      <th>żurek</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38605 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  007  01  03  04  05  08  0rg  10  100  ...  øystein  út  überall  \\\n",
       "0    0    0   0   0   0   0   0    0   0    0  ...        0   0        0   \n",
       "1    0    0   0   0   0   0   0    0   0    0  ...        0   0        0   \n",
       "2    0    0   0   0   0   0   0    0   0    0  ...        0   0        0   \n",
       "3    0    0   0   0   0   0   0    0   0    0  ...        0   0        0   \n",
       "4    0    0   0   0   0   0   0    0   0    0  ...        0   0        0   \n",
       "\n",
       "   ünal  ünker  ľudmila  şehriban  štefániková  żurek  Category  \n",
       "0     0      0        0         0            0      0   History  \n",
       "1     0      0        0         0            0      0  Business  \n",
       "2     0      0        0         0            0      0  Business  \n",
       "3     0      0        0         0            0      0  Politics  \n",
       "4     0      0        0         0            0      0  Politics  \n",
       "\n",
       "[5 rows x 38605 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorization of text\n",
    "# Creating the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# Converting the text to numeric data\n",
    "X = vectorizer.fit_transform(corpus) \n",
    "# Preparing Data frame For machine learning\n",
    "# Priority column acts as a target variable and other columns as predictors\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "CountVectorizedData['Category']=df['thematicFocusCategory'].values\n",
    "print(CountVectorizedData.shape)\n",
    "CountVectorizedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUmber of words aftere count vectorization: 38604\n"
     ]
    }
   ],
   "source": [
    "WordsVocab=CountVectorizedData.columns[:-1]\n",
    "print(f'Number of words after count vectorization: {len(WordsVocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function makes use thee pretrained google model to get word embeddings onn the vectorizeed input \n",
    "def FunctionText2Vec(inpTextData):\n",
    "    # Converting the text to numeric data\n",
    "    X = vectorizer.transform(inpTextData)\n",
    "    CountVecData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVecData.shape[0]):\n",
    "        # initiating a sentence with all zeros\n",
    "        Sentence = np.zeros(300)\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVecData.iloc[i , :]>=1]:\n",
    "            #print(word)\n",
    "            if word in GoogleModel.key_to_index.keys():    \n",
    "                Sentence=Sentence+GoogleModel[word]\n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data=W2Vec_Data.append(pd.DataFrame([Sentence]))\n",
    "    return(W2Vec_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marios/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#this takes aboout an hour to run on my local machine, onnly rerun if you want to overwrite\n",
    "W2Vec_Data=FunctionText2Vec(corpus)\n",
    "output_dir = '/home/marios/local_data_s2ds/'\n",
    "file_name = 'w2v_data_ALL_dirty.npy'\n",
    "np.save(op.join(output_dir, file_name), W2Vec_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved embeddings if you have them \n",
    "word_embedding_dir = '/home/marios/local_data_s2ds/w2v_data_ALL_dirty.npy'\n",
    "W2Vec_Data = pd.DataFrame(np.load(word_embedding_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299662</td>\n",
       "      <td>0.283447</td>\n",
       "      <td>-0.123535</td>\n",
       "      <td>0.349365</td>\n",
       "      <td>-0.561035</td>\n",
       "      <td>-0.381592</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>-1.204590</td>\n",
       "      <td>1.055176</td>\n",
       "      <td>1.244751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>-0.700806</td>\n",
       "      <td>-0.354126</td>\n",
       "      <td>-0.878662</td>\n",
       "      <td>-0.905640</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>-0.482178</td>\n",
       "      <td>0.404663</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200317</td>\n",
       "      <td>0.228638</td>\n",
       "      <td>-0.694740</td>\n",
       "      <td>0.358398</td>\n",
       "      <td>-0.019562</td>\n",
       "      <td>-0.473206</td>\n",
       "      <td>-0.455322</td>\n",
       "      <td>-0.468185</td>\n",
       "      <td>0.352570</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106201</td>\n",
       "      <td>-0.200439</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>0.594482</td>\n",
       "      <td>0.263367</td>\n",
       "      <td>-0.120850</td>\n",
       "      <td>0.468994</td>\n",
       "      <td>-0.409668</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110260</td>\n",
       "      <td>-0.127075</td>\n",
       "      <td>-0.201050</td>\n",
       "      <td>0.694519</td>\n",
       "      <td>-0.411407</td>\n",
       "      <td>0.086578</td>\n",
       "      <td>0.196777</td>\n",
       "      <td>-0.756042</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>0.215088</td>\n",
       "      <td>...</td>\n",
       "      <td>1.164688</td>\n",
       "      <td>-0.711182</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>-0.189392</td>\n",
       "      <td>0.060669</td>\n",
       "      <td>0.704670</td>\n",
       "      <td>0.462891</td>\n",
       "      <td>-0.134277</td>\n",
       "      <td>0.358215</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.166748</td>\n",
       "      <td>0.611023</td>\n",
       "      <td>0.495667</td>\n",
       "      <td>0.155273</td>\n",
       "      <td>-0.469604</td>\n",
       "      <td>0.094238</td>\n",
       "      <td>0.381165</td>\n",
       "      <td>-0.810547</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>0.612061</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231689</td>\n",
       "      <td>-0.383667</td>\n",
       "      <td>0.481567</td>\n",
       "      <td>-0.483643</td>\n",
       "      <td>0.057495</td>\n",
       "      <td>-0.515137</td>\n",
       "      <td>-0.443268</td>\n",
       "      <td>0.560913</td>\n",
       "      <td>0.461304</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.175186</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>0.268250</td>\n",
       "      <td>0.485107</td>\n",
       "      <td>-0.498779</td>\n",
       "      <td>0.637207</td>\n",
       "      <td>0.168701</td>\n",
       "      <td>-0.880371</td>\n",
       "      <td>0.427612</td>\n",
       "      <td>0.146729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191406</td>\n",
       "      <td>-0.310791</td>\n",
       "      <td>-0.050293</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>-0.698975</td>\n",
       "      <td>-0.377197</td>\n",
       "      <td>-1.096680</td>\n",
       "      <td>0.198547</td>\n",
       "      <td>-0.334229</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.299662  0.283447 -0.123535  0.349365 -0.561035 -0.381592  0.598633   \n",
       "1  0.200317  0.228638 -0.694740  0.358398 -0.019562 -0.473206 -0.455322   \n",
       "2  0.110260 -0.127075 -0.201050  0.694519 -0.411407  0.086578  0.196777   \n",
       "3  0.166748  0.611023  0.495667  0.155273 -0.469604  0.094238  0.381165   \n",
       "4  0.175186 -0.080078  0.268250  0.485107 -0.498779  0.637207  0.168701   \n",
       "\n",
       "          7         8         9  ...       291       292       293       294  \\\n",
       "0 -1.204590  1.055176  1.244751  ...  0.004211 -0.700806 -0.354126 -0.878662   \n",
       "1 -0.468185  0.352570  0.208008  ... -0.106201 -0.200439 -0.055298  0.016968   \n",
       "2 -0.756042  0.694824  0.215088  ...  1.164688 -0.711182  0.079590 -0.189392   \n",
       "3 -0.810547  0.070190  0.612061  ... -0.231689 -0.383667  0.481567 -0.483643   \n",
       "4 -0.880371  0.427612  0.146729  ... -0.191406 -0.310791 -0.050293  0.201172   \n",
       "\n",
       "        295       296       297       298       299  Category  \n",
       "0 -0.905640 -0.141602 -0.482178  0.404663  0.433105   History  \n",
       "1  0.594482  0.263367 -0.120850  0.468994 -0.409668  Business  \n",
       "2  0.060669  0.704670  0.462891 -0.134277  0.358215  Business  \n",
       "3  0.057495 -0.515137 -0.443268  0.560913  0.461304  Politics  \n",
       "4 -0.698975 -0.377197 -1.096680  0.198547 -0.334229  Politics  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Adding the target variable\n",
    "W2Vec_Data.reset_index(inplace=True, drop=True)\n",
    "W2Vec_Data['Category']=CountVectorizedData['Category']\n",
    " \n",
    "# Assigning to DataForML variable\n",
    "DataForML=W2Vec_Data.copy()\n",
    "DataForML.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's code the y columnns\n",
    "# let's changee names to numbers \n",
    "primary_categories = np.unique(DataForML.Category).tolist()\n",
    "\n",
    "primary_categories = sorted(primary_categories)\n",
    "prim_cat_dict = {}\n",
    "for i, cat in enumerate(primary_categories):\n",
    "    prim_cat_dict[cat] = i+1\n",
    "\n",
    "DataForML.Category = DataForML.Category.apply(lambda x: prim_cat_dict[x] if x in prim_cat_dict else x)\n",
    "DataForML.Category  = DataForML.Category.astype(int)\n",
    "np.unique(DataForML.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable=DataForML.columns[-1]\n",
    "Predictors=DataForML.columns[:-1]\n",
    "X=DataForML[Predictors].values\n",
    "y=DataForML[TargetVariable].values\n",
    "\n",
    "PredictorScaler=MinMaxScaler()\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    "# Generating the standardized values of X\n",
    "X=PredictorScalerFit.transform(X)\n",
    " \n",
    "# Split the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 20314\n",
      "19: 20314\n",
      "15: 20314\n",
      "10: 20314\n",
      "14: 20314\n",
      "1: 20314\n",
      "18: 20314\n",
      "16: 20314\n",
      "21: 20314\n",
      "3: 20314\n",
      "11: 20314\n",
      "2: 20314\n",
      "12: 20314\n",
      "7: 20314\n",
      "17: 20314\n",
      "9: 20314\n",
      "6: 20314\n",
      "8: 20314\n",
      "20: 20314\n",
      "5: 20314\n",
      "13: 20314\n"
     ]
    }
   ],
   "source": [
    "#let's balance classes- if you like \n",
    "smote_sampler = SMOTE(random_state = 5)\n",
    "X_smo_train, y_smo_train = smote_sampler.fit_resample(X_train, y_train)\n",
    "X_smo_test, y_smo_test = smote_sampler.fit_resample(X_test, y_test)\n",
    "elements_count = collections.Counter(y_smo_train)\n",
    "for key, value in elements_count.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5990548528026042,\n",
       " 'recall': 0.5990548528026042,\n",
       " 'precision': 0.5937706185645542,\n",
       " 'f1score': 0.5848436441831409}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try random forest on this data - not the best model\n",
    "\n",
    "model = RandomForestClassifier(max_depth= 10, max_features = 'auto', n_estimators= 20)\n",
    "model.fit(X_smo_train, y_smo_train)\n",
    "preds = model.predict(X_smo_test)\n",
    "evaluate_metrics(y_smo_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a SVC- performs better- need to try with balanced classes (maybe balance both train and teest data?)\n",
    "model = SVC(C=20, kernel='rbf')\n",
    "model.fit(X_smo_train, y_smo_train)\n",
    "preds = model.predict(X_smo_test)\n",
    "evaluate_metrics(y_smo_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you'd like to optimize hyperparameters- run this over the weekend maybe\n",
    "params_grid = {\n",
    "    'C': [25, 50, 150],\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "model = SVC()\n",
    "# Define a GridSearchCV to search the best parameters\n",
    "grid_search_balanced = GridSearchCV(estimator = model, \n",
    "                           param_grid = params_grid, \n",
    "                           scoring='f1',\n",
    "                           cv = 3, verbose = 1)\n",
    "# Search the best parameters with training data\n",
    "model_fit_balanced = grid_search_balanced.fit(X_train, y_train)\n",
    "best_params_balanced = grid_search_balanced.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally let's try to train only on one keyword\n",
    "# we need to reload the dataset, and remake emeddings only with one keyword at a time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
