{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to clean the keyword column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import itertools # to flatten lists of lists\n",
    "import collections # to count\n",
    "from rapidfuzz import process as pr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/raw/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "  \n",
    "# returns JSON object as a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "# convert to data frame\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the subset of the data for 1 Jan 2019 - 1 Jan 2020 based on lastModifiedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 03:57:28.904000+00:00\n",
      "2022-01-01 02:35:51.098000+00:00\n",
      "60278\n",
      "150367\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by='lastModifiedDate') #sort dataframe\n",
    "\n",
    "datetimes = pd.to_datetime(df['lastModifiedDate'])\n",
    "df['ts_lastModifiedDate']=datetimes\n",
    "#df.iloc[ts_start]['ts_lastModifiedDate']\n",
    "\n",
    "#find start index for subset 2019-2022\n",
    "ts_start=datetimes[(datetimes > pd.Timestamp(year=2019, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2019, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_start)\n",
    "\n",
    "#find end date for subset 2019-2022\n",
    "ts_end=datetimes[(datetimes > pd.Timestamp(year=2022, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2022, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_end)\n",
    "\n",
    "start_date=datetimes[datetimes == ts_start]\n",
    "end_date=datetimes[datetimes == ts_end]\n",
    "\n",
    "#find index for the chosen start and end dates\n",
    "start_index=start_date.index[0]\n",
    "print(start_index)\n",
    "df[df.index == start_date.index[0]]\n",
    "\n",
    "end_index=end_date.index[0]\n",
    "print(end_index)\n",
    "df[df.index == end_date.index[0]]\n",
    "\n",
    "df_subset=df[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'shortTitle', 'text', 'teaser', 'shortTeaser', 'kicker',\n",
       "       'regions', 'keywords', 'keywordStrings', 'thematicFocusCategory',\n",
       "       'navigations', 'categories', 'departments', 'firstPublicationDate',\n",
       "       'lastModifiedDate', 'contentDate', 'relatedAutoTopics', 'contentLinks',\n",
       "       'articles', 'isOpinion', 'geographicLocations', 'contentAssociations',\n",
       "       'mainContentImageLink', 'images', 'externalLinks', 'topStory',\n",
       "       'language', 'ts_lastModifiedDate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(df_subset.keywordStrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used mainly for visualisation, get indices of keywords with a certain substring\n",
    "def get_items_with_substring(lst_lst_keywords, substring):\n",
    "    indices = [i for i, lst_kw in enumerate(lst_lst_keywords) if any(list(map(lambda x: substring in x, lst_kw)))]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 1: put everything in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lower = [list(map(str.casefold, x)) for x in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: ['NASA', 'OSIRIS-REx', 'Bennu', 'asteroid']\n",
      "after:  ['nasa', 'osiris-rex', 'bennu', 'asteroid']\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "# TODO: entity linking? Keep upper case for names, cities, acronym\n",
    "print('before:', keywords[0])\n",
    "print('after: ', keywords_lower[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 2: split keywords that haven't been split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keywords: kw.split splits the keyword in a list of multiple keywords based on substring, itertools.chain flattens the list of lists\n",
    "keywords_lower_split = [list(itertools.chain(*[kw.split(', ') for kw in lst_kw])) for lst_kw in keywords_lower]\n",
    "keywords_lower_split = [list(itertools.chain(*[kw.split(' - ') for kw in lst_kw])) for lst_kw in keywords_lower_split] # spaces around '-' to not confuse with the ones within words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 13 \n",
      "\n",
      "before: ['transgender', 'intergender', 'zagreb', 'march', 'my body - my temple']\n",
      "after:  ['transgender', 'intergender', 'zagreb', 'march', 'my body', 'my temple'] \n",
      "\n",
      "before: ['smartphone', 'app', 'photos', 'black - b&w film emulator']\n",
      "after:  ['smartphone', 'app', 'photos', 'black', 'b&w film emulator'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_unsplit_keywords = get_items_with_substring(keywords_lower, ' - ') + get_items_with_substring(keywords_lower, ', ')\n",
    "print('Nb of keywords changed:', len(items_with_unsplit_keywords), '\\n')\n",
    "for i in range(2):\n",
    "    print('before:', keywords_lower[items_with_unsplit_keywords[i]])\n",
    "    print('after: ', keywords_lower_split[items_with_unsplit_keywords[i]], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 3: remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '\\u2002' and '.', '\" ', '\"', 'keywords: ' (replace with empty)\n",
    "keywords_lower_split_clean = keywords_lower_split\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\\xa0', ' '), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: ''.join(filter(str.isprintable, x)), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('.', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\" ', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\"', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('keywords: ', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "\n",
    "# Replace '\\xa0' with space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 3 \n",
      "\n",
      "before: ['keywords: germany', 'refugees', 'asylum-seekers', 'covid-19', 'coronavirus']\n",
      "after:  ['germany', 'refugees', 'asylum-seekers', 'covid-19', 'coronavirus'] \n",
      "\n",
      "before: ['keywords: president martin vizcarra', 'peru', 'peru impeachment']\n",
      "after:  ['president martin vizcarra', 'peru', 'peru impeachment'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_unwanted_characters = get_items_with_substring(keywords_lower, substring = 'keywords:')\n",
    "print('Nb of keywords changed:', len(items_with_unwanted_characters), '\\n')\n",
    "for i in range(2):\n",
    "    print('before:', keywords_lower_split[items_with_unwanted_characters[i]])\n",
    "    print('after: ', keywords_lower_split_clean[items_with_unwanted_characters[i]], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 4: Clean sentences \n",
    "Heuristic: remove keywords that have more than 6 spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract keyword from the sentence instead of deleting it?\n",
    "n_spaces = 8 # if there are more spaces than this number, the keyword is removed\n",
    "keywords_lower_split_clean_short = [[kw for kw in lst_kw if kw.count(' ')<n_spaces] for lst_kw in keywords_lower_split_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 10 \n",
      "\n",
      "before: ['venezuela', 'aid', 'juan guaido', 'nicolas maduro', 'foreign aid', 'international federation of red cross and red crescent societies']\n",
      "after:  ['venezuela', 'aid', 'juan guaido', 'nicolas maduro', 'foreign aid'] \n",
      "\n",
      "before: ['edgar hilsenrath', 'hilsenrath', 'holocaust literature', 'german authors', 'jewish authors', 'german-jewish authors', 'german literature holocaust authors', 'holocausta humorous take on the holocaust is taboo in germany', 'but one author gains fame in doing so']\n",
      "after:  ['edgar hilsenrath', 'hilsenrath', 'holocaust literature', 'german authors', 'jewish authors', 'german-jewish authors', 'german literature holocaust authors', 'but one author gains fame in doing so'] \n",
      "\n",
      "before: ['france', 'international day for the elimination of violence against women', 'spain', 'turkey', 'domestic violence', 'femicide']\n",
      "after:  ['france', 'spain', 'turkey', 'domestic violence', 'femicide'] \n",
      "\n",
      "before: ['ameroon', 'open letter', 'civil society', 'pressure', 'international community', 'the global campaign for peace & justice in cameroon']\n",
      "after:  ['ameroon', 'open letter', 'civil society', 'pressure', 'international community'] \n",
      "\n",
      "before: ['habitat', 'global ideas', 'conservation', 'species', 'biodiversity', 'fish', 'eels', 'convention on the conservation of migratory species of wild animals']\n",
      "after:  ['habitat', 'global ideas', 'conservation', 'species', 'biodiversity', 'fish', 'eels'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_sentences = [i for i,lst_kw in enumerate(keywords_lower_split_clean) if any(list(map(lambda x: x.count(' ')>=n_spaces, lst_kw)))]\n",
    "print('Nb of keywords changed:', len(items_with_sentences), '\\n')\n",
    "for i in range(5):\n",
    "    print('before:', keywords_lower_split_clean[items_with_sentences[i]])\n",
    "    print('after: ', keywords_lower_split_clean_short[items_with_sentences[i]], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords before cleaning: 32682\n",
      "Number of unique keywords before cleaning: 30279\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords)))))\n",
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords_lower_split_clean_short)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(keywords[0:2])\n",
    "#print(keywords_lower_split_clean_short[0:2])\n",
    "\n",
    "#df_2019_2020 = df_subset.copy()\n",
    "#df_2019_2020['keywordStringsClean'] = keywords_lower_split_clean_short\n",
    "\n",
    "#filepath = '../data/interim/clean_keywords_2019-2021_before_FuzzyWuzzy.csv'\n",
    "#df_2019_2020.to_csv(filepath, index=False)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count occurence of each keyword\n",
    "Will be used to know which one to keep in fuzzy wuzzy (the most used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat = list(itertools.chain(*keywords_lower_split_clean_short)) # Flatten list\n",
    "keywords_freq = collections.Counter(keywords_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>3817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germany</th>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19</th>\n",
       "      <td>2641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>china</th>\n",
       "      <td>1669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russia</th>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donald trump</th>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundesliga</th>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              freq\n",
       "coronavirus   3817\n",
       "germany       3299\n",
       "covid-19      2641\n",
       "china         1669\n",
       "russia        1354\n",
       "donald trump  1333\n",
       "asia          1260\n",
       "us            1087\n",
       "eu            1078\n",
       "bundesliga     905"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For visualisation, can be removed\n",
    "keywords_freq_df = pd.DataFrame.from_dict(keywords_freq, orient='index', columns = ['freq'])\n",
    "keywords_freq_df.sort_values(by='freq', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid Fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique ones and remove the empty entry\n",
    "unique_keywords = list(set(keywords_flat))\n",
    "unique_keywords.remove('')\n",
    "\n",
    "# run rapid fuzz\n",
    "ratio_array= pr.cdist(unique_keywords, unique_keywords, score_cutoff = 90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find words correlating together and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mt fuji</th>\n",
       "      <th>videometrics</th>\n",
       "      <th>dubai airshow</th>\n",
       "      <th>islamists</th>\n",
       "      <th>koala</th>\n",
       "      <th>treatment</th>\n",
       "      <th>landscape photography</th>\n",
       "      <th>nada</th>\n",
       "      <th>esther salas</th>\n",
       "      <th>workforce</th>\n",
       "      <th>...</th>\n",
       "      <th>eindhoven</th>\n",
       "      <th>decomposition</th>\n",
       "      <th>1933 seizure of power</th>\n",
       "      <th>andrei kovacs</th>\n",
       "      <th>lightning bolt</th>\n",
       "      <th>commercial whaling</th>\n",
       "      <th>monsters</th>\n",
       "      <th>dirty harry</th>\n",
       "      <th>lgbt+</th>\n",
       "      <th>eu migration pact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mt fuji  videometrics  dubai airshow  islamists  koala  treatment  \\\n",
       "0    100.0           0.0            0.0        0.0    0.0        0.0   \n",
       "1      0.0         100.0            0.0        0.0    0.0        0.0   \n",
       "2      0.0           0.0          100.0        0.0    0.0        0.0   \n",
       "3      0.0           0.0            0.0      100.0    0.0        0.0   \n",
       "4      0.0           0.0            0.0        0.0  100.0        0.0   \n",
       "\n",
       "   landscape photography  nada  esther salas  workforce  ...  eindhoven  \\\n",
       "0                    0.0   0.0           0.0        0.0  ...        0.0   \n",
       "1                    0.0   0.0           0.0        0.0  ...        0.0   \n",
       "2                    0.0   0.0           0.0        0.0  ...        0.0   \n",
       "3                    0.0   0.0           0.0        0.0  ...        0.0   \n",
       "4                    0.0   0.0           0.0        0.0  ...        0.0   \n",
       "\n",
       "   decomposition  1933 seizure of power  andrei kovacs  lightning bolt  \\\n",
       "0            0.0                    0.0            0.0             0.0   \n",
       "1            0.0                    0.0            0.0             0.0   \n",
       "2            0.0                    0.0            0.0             0.0   \n",
       "3            0.0                    0.0            0.0             0.0   \n",
       "4            0.0                    0.0            0.0             0.0   \n",
       "\n",
       "   commercial whaling  monsters  dirty harry  lgbt+  eu migration pact  \n",
       "0                 0.0       0.0          0.0    0.0                0.0  \n",
       "1                 0.0       0.0          0.0    0.0                0.0  \n",
       "2                 0.0       0.0          0.0    0.0                0.0  \n",
       "3                 0.0       0.0          0.0    0.0                0.0  \n",
       "4                 0.0       0.0          0.0    0.0                0.0  \n",
       "\n",
       "[5 rows x 30278 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_array = pd.DataFrame(ratio_array, columns = unique_keywords)\n",
    "\n",
    "# Count number of non zero values in each row\n",
    "nb_non_zero = np.count_nonzero(np.asarray(ratio_array), axis=1) \n",
    "\n",
    "# Save indices of rows with more than 1 non-zero value\n",
    "indices_correlating_rows = [i for i, el in enumerate(list(nb_non_zero)) if el>1]\n",
    "\n",
    "df_array.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of similar keywords\n",
    "# TODO: problem: ['european fires', 'european firms']\n",
    "all_similar_kw = []\n",
    "for i in indices_correlating_rows:\n",
    "    similar_words = [keyword for val, keyword in zip(list(df_array.iloc[i]), unique_keywords) if val!=0]\n",
    "    all_similar_kw.append(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4900"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_similar_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify similar keywords\n",
    "Replace by most frequent one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Runs for 14min !!\n",
    "# # Split in the ones which have equal word number, and the ones which don'try\n",
    "# # TODO: Problem: UN climate summit, US department of justice: only change if same number of words? Not change it if it's a country? ENTITY LINKING? Or remove country name?\n",
    "# # TODO: Optimise? Faster?\n",
    "# # TODO: Word embedinngs? MD\n",
    "\n",
    "# similar_kws_same_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))==1]\n",
    "# similar_kws_diff_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))!=1]\n",
    "\n",
    "# # Replace\n",
    "# keywords_replaced = keywords_lower_split_clean_short\n",
    "\n",
    "# for sim_kws in similar_kws_same_word_nb:\n",
    "    \n",
    "#     # Make list of frequencies for those similar kws\n",
    "#     sim_kws_freq = [keywords_freq[word] for word in sim_kws]\n",
    "    \n",
    "#     for kw in sim_kws:\n",
    "\n",
    "#         # the new keyword is the one with the highest frequency\n",
    "#         right_kw = sim_kws[sim_kws_freq.index(max(sim_kws_freq))]\n",
    "\n",
    "#         # replace similar keywords by the most frequent one\n",
    "#         keywords_replaced = [list(map(lambda x: right_kw if x==kw else x, lst_kw)) for lst_kw in keywords_replaced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs for 14min !!\n",
    "# Split in the ones which have equal word number, and the ones which don'try\n",
    "# TODO: Problem: UN climate summit, US department of justice: only change if same number of words? Not change it if it's a country? ENTITY LINKING? Or remove country name?\n",
    "# TODO: Optimise? Faster?\n",
    "# TODO: Word embedinngs? MD\n",
    "\n",
    "similar_kws_same_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))==1]\n",
    "similar_kws_diff_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))!=1]\n",
    "\n",
    "# Replace\n",
    "keywords_replaced = keywords_lower_split_clean_short\n",
    "\n",
    "#for sim_kws in similar_kws_same_word_nb:\n",
    "    \n",
    "    # Make list of frequencies for those similar kws\n",
    "    #sim_kws_freq = [[keywords_freq[word] for word in sim_kws] for sim_kws in similar_kws_same_word_nb]\n",
    "\n",
    "#argmaxright_kw = [np.argmax([keywords_freq[word] for word in sim_kws]) for sim_kws in similar_kws_same_word_nb[0:5]]\n",
    "#maxright_kw = [max([keywords_freq[word] for word in sim_kws]) for sim_kws in similar_kws_same_word_nb[0:5]]\n",
    "#np.random.choice(np.flatnonzero(b == b.max()))\n",
    "right_kw = [sim_kws[np.argmax([keywords_freq[word] for word in sim_kws])] for sim_kws in similar_kws_same_word_nb]\n",
    "from itertools import chain\n",
    "keywords_replaced_flat = list(chain.from_iterable(keywords_replaced))\n",
    "replacement_only = [[right_kw[i] for i, j in enumerate(similar_kws_same_word_nb) if word in j] for word in keywords_replaced_flat]\n",
    "keywords_replaced_flat_post = [replacement_only[i][0] if replacement_only[i] != [] else keywords_replaced_flat[i] for i in range(len(keywords_replaced_flat))]\n",
    "\n",
    "def gen_list_of_lists(original_list, new_structure):\n",
    "    assert len(original_list) == sum(new_structure), \\\n",
    "    \"The number of elements in the original list and desired structure don't match\"\n",
    "        \n",
    "    list_of_lists = [[original_list[i + sum(new_structure[:j])] for i in range(new_structure[j])] \\\n",
    "                     for j in range(len(new_structure))]\n",
    "        \n",
    "    return list_of_lists\n",
    "\n",
    "keywords_replaced_unflat_post = gen_list_of_lists(keywords_replaced_flat_post, [len(x) for x in keywords_replaced])\n",
    "#keywords_replaced = [list(map(lambda x:right_kw,lst_kw)) for lst_kw in keywords_replaced]\n",
    "#keywords_replaced = [[[list(map(lambda x: right_kw if x==kw else x, lst_kw)) for kw in sim_kws] for sim_kws in similar_kws_same_word_nb]for lst_kw in keywords_replaced]\n",
    "#right_kw = [keywords_freq[sim_kws].index(max([keywords_freq[word] for word in sim_kws])) for sim_kws in similar_kws_same_word_nb[0:5]]\n",
    "#right_kw = [[max(range(len(values)), key=values.__getitem__) for word in sim_kws])) for sim_kws in similar_kws_same_word_nb[0:5]]\n",
    "#max(range(len(values)), key=values.__getitem__)\n",
    "#right_kw = [[max(keywords_freq[word]) for word in sim_kws] for sim_kws in similar_kws_same_word_nb[0:5]]\n",
    "#keywords_replaced = [list(map(lambda x: right_kw if x==kw else x, lst_kw)) for lst_kw in keywords_replaced[0:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_replaced = keywords_replaced_unflat_post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_freq['ayatollah khamenei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_freq['ayatollah khomeini']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords_test=[]\n",
    "# keywords_test.append(['georges', 'clemeanceau'])\n",
    "# keywords_test.append(['philippe', 'petain'])\n",
    "# keywords_test.append(['leon', 'blum'])\n",
    "\n",
    "# all_similar_kw_test = []\n",
    "# all_similar_kw_test.append(['OSEF','JMEF'])\n",
    "# all_similar_kw_test.append(['blum','bloom'])\n",
    "# all_similar_kw_test.append(['putain','petain'])\n",
    "# all_similar_kw_test.append(['TAMERE','TESMORTS'])\n",
    "\n",
    "# right_kw_test = []\n",
    "# right_kw_test.append('JMEF')\n",
    "# right_kw_test.append('bloom')\n",
    "# right_kw_test.append('putain')\n",
    "\n",
    "# keywords_test_list=[]\n",
    "# keywords_test_list.append('georges')\n",
    "# keywords_test_list.append('clemeanceau')\n",
    "# keywords_test_list.append('philippe')\n",
    "# keywords_test_list.append('petain')\n",
    "# keywords_test_list.append('leon')\n",
    "# keywords_test_list.append('blum')\n",
    "\n",
    "# keywords_test_flat = list(chain.from_iterable(keywords_test))\n",
    "# replacement_only = [[right_kw_test[i] for i, j in enumerate(all_similar_kw_test) if word in j] for word in keywords_test_flat]\n",
    "# keywords_test_flat_post = [replacement_only[i][0] if replacement_only[i] != [] else keywords_test_flat[i] for i in range(len(keywords_test_flat))]\n",
    "\n",
    "# def gen_list_of_lists(original_list, new_structure):\n",
    "#     assert len(original_list) == sum(new_structure), \\\n",
    "#     \"The number of elements in the original list and desired structure don't match\"\n",
    "        \n",
    "#     list_of_lists = [[original_list[i + sum(new_structure[:j])] for i in range(new_structure[j])] \\\n",
    "#                      for j in range(len(new_structure))]\n",
    "        \n",
    "#     return list_of_lists\n",
    "\n",
    "# keywords_test_unflat_post = gen_list_of_lists(keywords_test_flat_post, [len(x) for x in keywords_test])\n",
    "#list_index = [[i for i, lst in enumerate(all_similar_kw_test) if word in lst][0] for word in keywords_test[0]]\n",
    "#keywords_test_FT_list = [right_kw_test[i] if word in all_similar_kw_test[i] else word for word in keywords_test_list for i in range(3)]\n",
    "#new_words = ['broccoli' if word == 'chicken' else word for word in words] [\"foo\", \"bar\", \"baz\"].index(\"bar\")\n",
    "#keywords_test_FT_list = [right_kw_test[i] if word in j else word for i,j in enumerate(all_similar_kw_test)]\n",
    "#keywords_test_FT = [[[word.replace(right_kw_test[i], word) for i, x in enumerate(all_similar_kw_test) if word in x] for word in list] for list in keywords_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vide = [[right_kw_test[i] for i, j in enumerate(all_similar_kw_test) if word in j] for word in keywords_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEE = [test_vide[i][0] if test_vide[i] != [] else keywords_test_list[i] for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq of: pretzel  =  1\n",
      "freq of: pretzels  =  2\n",
      "\n",
      "freq of: road trip  =  1\n",
      "freq of: roadtrip  =  1\n",
      "\n",
      "kw_before: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khamenei']\n",
      "kw_after: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khomeini'] \n",
      "\n",
      "kw_before: [\"new year's\", 'countdown', 'resolutions']\n",
      "kw_after: [\"new year's\", 'countdown', 'solutions'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example for visualisation, can be removed later\n",
    "for sim_kws in all_similar_kw[0:2]:\n",
    "    for word in sim_kws:\n",
    "        print('freq of:', word, ' = ', keywords_freq[word])\n",
    "    print('')\n",
    "\n",
    "n_show = 2 # how many examples to show\n",
    "\n",
    "i_show = 0\n",
    "for kw_before, kw_after in zip(keywords_lower_split_clean_short, keywords_replaced):\n",
    "    if kw_before != kw_after and i_show < n_show:\n",
    "        print('kw_before:', kw_before)\n",
    "        print('kw_after:', kw_after, '\\n')\n",
    "        i_show += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords before cleaning: 32682\n",
      "Number of unique keywords after first clean: 30279\n",
      "Number of unique keywords after rapidfuzz replacing: 28056\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords)))))\n",
    "print('Number of unique keywords after first clean:', len(set(list(itertools.chain(*keywords_lower_split_clean_short)))))\n",
    "print('Number of unique keywords after rapidfuzz replacing:', len(set(list(itertools.chain(*keywords_replaced)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lastModifiedDate  \\\n",
       "0  2019-01-01T03:57:28.904Z   \n",
       "1  2019-01-01T06:11:50.527Z   \n",
       "2  2019-01-01T06:14:35.563Z   \n",
       "3  2019-01-01T08:26:11.599Z   \n",
       "4  2019-01-01T09:05:00.736Z   \n",
       "\n",
       "                                      keywordStrings  \\\n",
       "0                [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "1  [English Channel, migration, boats, illegal im...   \n",
       "2  [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "3                   [Japan, Tokyo, Harajuku, attack]   \n",
       "4  [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "\n",
       "                        keywordStringsCleanAfterFuzz  \n",
       "0                [nasa, osiris-rex, bennu, asteroid]  \n",
       "1  [english channel, migration, boats, illegal im...  \n",
       "2  [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "3                   [japan, tokyo, harajuku, attack]  \n",
       "4  [asia, bangladesh, elections, kamal hossain, s...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add article ID\n",
    "list_dates = list(df_subset['lastModifiedDate'])\n",
    "list_kws = list(df_subset['keywordStrings'])\n",
    "list_new_kws = keywords_replaced\n",
    "\n",
    "df_2019_2020 = pd.DataFrame(list(zip(list_dates, list_kws, list_new_kws)), columns=['lastModifiedDate', 'keywordStrings', 'keywordStringsCleanAfterFuzz'])\n",
    "\n",
    "df_2019_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/interim/clean_keywords_2019-2021_after_RapidFuzzFT.json'\n",
    "\n",
    "# storing the data in JSON format\n",
    "df_2019_2020.to_json(filepath, orient = 'split', compression = 'infer', index = 'true')\n",
    " \n",
    "# reading the JSON file\n",
    "# filepath = '../data/interim/clean_keywords_2019-2021_after_FuzzyWuzzy.json'\n",
    "# df_loaded = pd.read_json(filepath, orient ='split', compression = 'infer')\n",
    "# flat_keywords = list(itertools.chain(*list(df['keywordStringsCleanAfterFuzz'])))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords after rapidfuzz replacing: 4900\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords after rapidfuzz replacing:', len(set(list(itertools.chain(*all_similar_kw)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data = [list(x) for x in set(tuple(x) for x in all_similar_kw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2842"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# wv = gensim.models.KeyedVectors.load_word2vec_format(\"/home/ferdinand_t/Downloads/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33830, 21982)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magda_d/venv/lib64/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>007</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100m</th>\n",
       "      <th>103</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>...</th>\n",
       "      <th>övp</th>\n",
       "      <th>özdemir</th>\n",
       "      <th>özil</th>\n",
       "      <th>özlem</th>\n",
       "      <th>út</th>\n",
       "      <th>überall</th>\n",
       "      <th>ünal</th>\n",
       "      <th>ünker</th>\n",
       "      <th>şehriban</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21982 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   007  04  05  10  100  100m  103  11  110  111  ...  övp  özdemir  özil  \\\n",
       "0    0   0   0   0    0     0    0   0    0    0  ...    0        0     0   \n",
       "1    0   0   0   0    0     0    0   0    0    0  ...    0        0     0   \n",
       "2    0   0   0   0    0     0    0   0    0    0  ...    0        0     0   \n",
       "3    0   0   0   0    0     0    0   0    0    0  ...    0        0     0   \n",
       "4    0   0   0   0    0     0    0   0    0    0  ...    0        0     0   \n",
       "\n",
       "   özlem  út  überall  ünal  ünker  şehriban          lastModifiedDate  \n",
       "0      0   0        0     0      0         0  2019-01-01T03:57:28.904Z  \n",
       "1      0   0        0     0      0         0  2019-01-01T06:11:50.527Z  \n",
       "2      0   0        0     0      0         0  2019-01-01T06:14:35.563Z  \n",
       "3      0   0        0     0      0         0  2019-01-01T08:26:11.599Z  \n",
       "4      0   0        0     0      0         0  2019-01-01T09:05:00.736Z  \n",
       "\n",
       "[5 rows x 21982 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorization of text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "# Ticket Data\n",
    "corpus = df_2019_2020['keywordStringsCleanAfterFuzz'].astype(str)\n",
    " \n",
    "# Creating the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    " \n",
    "# Converting the text to numeric data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    " \n",
    "#print(vectorizer.get_feature_names())\n",
    " \n",
    "# Preparing Data frame For machine learning\n",
    "# Priority column acts as a target variable and other columns as predictors\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "CountVectorizedData['lastModifiedDate']= df_2019_2020['lastModifiedDate'].values\n",
    "print(CountVectorizedData.shape)\n",
    "CountVectorizedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [nasa, osiris-rex, bennu, asteroid]\n",
       "1    [english channel, migration, boats, illegal im...\n",
       "Name: keywordStringsCleanAfterFuzz, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019_2020['keywordStringsCleanAfterFuzz'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magda_d/venv/lib64/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asteroid</th>\n",
       "      <th>bennu</th>\n",
       "      <th>boats</th>\n",
       "      <th>bolsonaro</th>\n",
       "      <th>brazil</th>\n",
       "      <th>channel</th>\n",
       "      <th>chicago</th>\n",
       "      <th>economics</th>\n",
       "      <th>english</th>\n",
       "      <th>guedes</th>\n",
       "      <th>hamilton</th>\n",
       "      <th>illegal</th>\n",
       "      <th>immigration</th>\n",
       "      <th>jair</th>\n",
       "      <th>migration</th>\n",
       "      <th>mourao</th>\n",
       "      <th>nasa</th>\n",
       "      <th>osiris</th>\n",
       "      <th>paulo</th>\n",
       "      <th>rex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asteroid  bennu  boats  bolsonaro  brazil  channel  chicago  economics  \\\n",
       "0         1      1      0          0       0        0        0          0   \n",
       "1         0      0      1          0       0        1        0          0   \n",
       "2         0      0      0          1       1        0        1          1   \n",
       "\n",
       "   english  guedes  hamilton  illegal  immigration  jair  migration  mourao  \\\n",
       "0        0       0         0        0            0     0          0       0   \n",
       "1        1       0         0        1            1     0          1       0   \n",
       "2        0       1         1        0            0     1          0       1   \n",
       "\n",
       "   nasa  osiris  paulo  rex  \n",
       "0     1       1      0    1  \n",
       "1     0       0      0    0  \n",
       "2     0       0      1    0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ticket Data\n",
    "corpus = df_2019_2020['keywordStringsCleanAfterFuzz'][0:3].astype(str)\n",
    " \n",
    "# Creating the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    " \n",
    "# Converting the text to numeric data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    " \n",
    "#print(vectorizer.get_feature_names())\n",
    " \n",
    "# Preparing Data frame For machine learning\n",
    "# Priority column acts as a target variable and other columns as predictors\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "#CountVectorizedData['lastModifiedDate']= df_2019_2020['lastModifiedDate'].values\n",
    "print(CountVectorizedData.shape)\n",
    "CountVectorizedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          ['nasa', 'osiris-rex', 'bennu', 'asteroid']\n",
       "1    ['english channel', 'migration', 'boats', 'ill...\n",
       "2    ['brazil', 'jair bolsonaro', 'chicago economic...\n",
       "Name: keywordStringsCleanAfterFuzz, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsVocab=CountVectorizedData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsVocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionText2Vec(inpTextData):\n",
    "    # Converting the text to numeric data\n",
    "    X = vectorizer.transform(inpTextData)\n",
    "    CountVecData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "    \n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVecData.shape[0]):\n",
    " \n",
    "        # initiating a sentence with all zeros\n",
    "        Sentence = np.zeros(300)\n",
    " \n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVecData.iloc[i , :]>=1]:\n",
    "            #print(word)\n",
    "            if word in wv.key_to_index.keys():    \n",
    "                Sentence=Sentence+wv[word]\n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data=W2Vec_Data.append(pd.DataFrame([Sentence]))\n",
    "    return(W2Vec_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVecData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7092/1174736242.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCountVecData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33829\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVecData' is not defined"
     ]
    }
   ],
   "source": [
    "CountVecData.iloc[33829 , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2019_2020['keywordStringsCleanAfterFuzz'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magda_d/venv/lib64/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "W2Vec_Data=FunctionText2Vec(df_2019_2020['keywordStringsCleanAfterFuzz'][0:3].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.182617</td>\n",
       "      <td>0.047119</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>0.467773</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>-0.589355</td>\n",
       "      <td>0.196259</td>\n",
       "      <td>-0.612854</td>\n",
       "      <td>0.220215</td>\n",
       "      <td>-0.068726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154785</td>\n",
       "      <td>0.447266</td>\n",
       "      <td>-0.489746</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>-0.051758</td>\n",
       "      <td>-0.420166</td>\n",
       "      <td>-0.286865</td>\n",
       "      <td>0.342285</td>\n",
       "      <td>0.759033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.154053</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.263184</td>\n",
       "      <td>-0.719360</td>\n",
       "      <td>-0.577271</td>\n",
       "      <td>-0.123596</td>\n",
       "      <td>0.309570</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.320068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.215561</td>\n",
       "      <td>0.845909</td>\n",
       "      <td>0.062622</td>\n",
       "      <td>0.300781</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>0.032059</td>\n",
       "      <td>-0.284668</td>\n",
       "      <td>0.453857</td>\n",
       "      <td>0.143555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.426941</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.209290</td>\n",
       "      <td>1.017578</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.282715</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>-0.980957</td>\n",
       "      <td>-0.179443</td>\n",
       "      <td>0.467773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016602</td>\n",
       "      <td>0.634399</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.607666</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>-0.103760</td>\n",
       "      <td>-0.198181</td>\n",
       "      <td>-1.099854</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>0.262787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.182617  0.047119  0.154297  0.467773 -0.137695 -0.589355  0.196259   \n",
       "0 -0.154053  0.113770  0.041992 -0.263184 -0.719360 -0.577271 -0.123596   \n",
       "0 -0.426941  0.242188  0.209290  1.017578  0.179688  0.282715 -0.306396   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.612854  0.220215 -0.068726  ...  0.154785  0.447266 -0.489746  0.336731   \n",
       "0  0.309570  0.201172  0.320068  ...  0.016357  0.215561  0.845909  0.062622   \n",
       "0 -0.980957 -0.179443  0.467773  ... -0.016602  0.634399  0.228271  0.607666   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.023804 -0.051758 -0.420166 -0.286865  0.342285  0.759033  \n",
       "0  0.300781  0.254883  0.032059 -0.284668  0.453857  0.143555  \n",
       "0 -0.173828 -0.103760 -0.198181 -1.099854 -0.002441  0.262787  \n",
       "\n",
       "[3 rows x 300 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2Vec_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [nasa, osiris-rex, bennu, asteroid]\n",
       "1    [english channel, migration, boats, illegal im...\n",
       "2    [brazil, jair bolsonaro, chicago economics, ha...\n",
       "Name: keywordStringsCleanAfterFuzz, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_2019_2020['keywordStringsCleanAfterFuzz'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(W2Vec_Data.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
