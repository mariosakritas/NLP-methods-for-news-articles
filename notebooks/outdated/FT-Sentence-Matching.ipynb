{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import torch\n",
    "# labels  = [\"Architecture\", \"Arts\", \"Business\", \"Cars and Transportation\", \"Catastrophe\", \"Climate\", \"Conflicts\", \"Corruption\", \"Crime\", \"Culture\", \"Dance\", \"Design\", \"Digital World\", \"Diversity\", \"Education\", \"Equality\", \"Film\", \"Food Security\", \"Freedom of Speech\", \"Globalization\", \"Health\", \"History\", \"Human Rights\", \"Innovation\", \"Law and Justice\", \"Learning German\", \"Lifestyle\", \"Literature\", \"Media\", \"Migration\", \"Music\", \"Nature and Environment\", \"Offbeat\", \"Politics\", \"Press Freedom\", \"Religion\", \"Rule of Law\", \"Science\", \"Soccer\", \"Society\", \"Sports\", \"Technology\", \"Terrorism\", \"Theater\", \"Trade\", \"Travel\"]\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# #Compute embedding for both lists\n",
    "# embedding = []\n",
    "# for i in range(45):\n",
    "#     labels_to_embedding = model.encode(labels[i], convert_to_tensor=True)\n",
    "#     embedding.append(labels_to_embedding)\n",
    "\n",
    "# embedding = torch.cat(embedding,46)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "labels  = [\"Architecture\", \"Arts\", \"Business\", \"Cars and Transportation\", \"Catastrophe\", \"Climate\", \"Conflicts\", \"Corruption\", \"Crime\", \"Culture\", \"Dance\", \"Design\", \"Digital World\", \"Diversity\", \"Education\", \"Equality\", \"Film\", \"Food Security\", \"Freedom of Speech\", \"Globalization\", \"Health\", \"History\", \"Human Rights\", \"Innovation\", \"Law and Justice\", \"Learning German\", \"Lifestyle\", \"Literature\", \"Media\", \"Migration\", \"Music\", \"Nature and Environment\", \"Offbeat\", \"Politics\", \"Press Freedom\", \"Religion\", \"Rule of Law\", \"Science\", \"Soccer\", \"Society\", \"Sports\", \"Technology\", \"Terrorism\", \"Theater\", \"Trade\", \"Travel\"]\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "embedding_0 = model.encode(labels[0], convert_to_tensor=True)\n",
    "embedding_1 = model.encode(labels[1], convert_to_tensor=True)\n",
    "embedding_2 = model.encode(labels[2], convert_to_tensor=True)\n",
    "embedding_3 = model.encode(labels[3], convert_to_tensor=True)\n",
    "embedding_4 = model.encode(labels[4], convert_to_tensor=True)\n",
    "embedding_5 = model.encode(labels[5], convert_to_tensor=True)\n",
    "embedding_6 = model.encode(labels[6], convert_to_tensor=True)\n",
    "embedding_7 = model.encode(labels[7], convert_to_tensor=True)\n",
    "embedding_8 = model.encode(labels[8], convert_to_tensor=True)\n",
    "embedding_9 = model.encode(labels[9], convert_to_tensor=True)\n",
    "embedding_10 = model.encode(labels[10], convert_to_tensor=True)\n",
    "embedding_11 = model.encode(labels[11], convert_to_tensor=True)\n",
    "embedding_12 = model.encode(labels[12], convert_to_tensor=True)\n",
    "embedding_13 = model.encode(labels[13], convert_to_tensor=True)\n",
    "embedding_14 = model.encode(labels[14], convert_to_tensor=True)\n",
    "embedding_15 = model.encode(labels[15], convert_to_tensor=True)\n",
    "embedding_16 = model.encode(labels[16], convert_to_tensor=True)\n",
    "embedding_17 = model.encode(labels[17], convert_to_tensor=True)\n",
    "embedding_18 = model.encode(labels[18], convert_to_tensor=True)\n",
    "embedding_19 = model.encode(labels[19], convert_to_tensor=True)\n",
    "embedding_20 = model.encode(labels[20], convert_to_tensor=True)\n",
    "embedding_21 = model.encode(labels[21], convert_to_tensor=True)\n",
    "embedding_22 = model.encode(labels[22], convert_to_tensor=True)\n",
    "embedding_23 = model.encode(labels[23], convert_to_tensor=True)\n",
    "embedding_24 = model.encode(labels[24], convert_to_tensor=True)\n",
    "embedding_25 = model.encode(labels[25], convert_to_tensor=True)\n",
    "embedding_26 = model.encode(labels[26], convert_to_tensor=True)\n",
    "embedding_27 = model.encode(labels[27], convert_to_tensor=True)\n",
    "embedding_28 = model.encode(labels[28], convert_to_tensor=True)\n",
    "embedding_29 = model.encode(labels[29], convert_to_tensor=True)\n",
    "embedding_30 = model.encode(labels[30], convert_to_tensor=True)\n",
    "embedding_31 = model.encode(labels[31], convert_to_tensor=True)\n",
    "embedding_32 = model.encode(labels[32], convert_to_tensor=True)\n",
    "embedding_33 = model.encode(labels[33], convert_to_tensor=True)\n",
    "embedding_34 = model.encode(labels[34], convert_to_tensor=True)\n",
    "embedding_35 = model.encode(labels[35], convert_to_tensor=True)\n",
    "embedding_36 = model.encode(labels[36], convert_to_tensor=True)\n",
    "embedding_37 = model.encode(labels[37], convert_to_tensor=True)\n",
    "embedding_38 = model.encode(labels[38], convert_to_tensor=True)\n",
    "embedding_39 = model.encode(labels[39], convert_to_tensor=True)\n",
    "embedding_40 = model.encode(labels[40], convert_to_tensor=True)\n",
    "embedding_41 = model.encode(labels[41], convert_to_tensor=True)\n",
    "embedding_42 = model.encode(labels[42], convert_to_tensor=True)\n",
    "embedding_43 = model.encode(labels[43], convert_to_tensor=True)\n",
    "embedding_44 = model.encode(labels[44], convert_to_tensor=True)\n",
    "embedding_45 = model.encode(labels[45], convert_to_tensor=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lists = [embedding_0,embedding_1,embedding_2,embedding_3,embedding_4,embedding_5,embedding_6,embedding_7,embedding_8,embedding_9,embedding_10,embedding_11,embedding_12,embedding_13,embedding_14,embedding_15,embedding_16,embedding_17,embedding_18,embedding_19,embedding_20,embedding_21,embedding_22,embedding_23,embedding_24,embedding_25,embedding_26,embedding_27,embedding_28,embedding_29,embedding_30,embedding_31,embedding_32,embedding_33,embedding_34,embedding_35,embedding_36,embedding_37,embedding_38,embedding_39,embedding_40,embedding_41,embedding_42,embedding_43,embedding_44,embedding_45]\n",
    "embeds = pd.concat([pd.Series(x) for x in lists], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# Opening JSON file\n",
    "f = open('/home/ferdinand_t/data/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanFocusCategory'] = df['thematicFocusCategory'].apply(lambda x: x['name'] if x is not None else x)\n",
    "df = df[['keywordStrings', 'cleanFocusCategory']]\n",
    "df_clean = df.copy()\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean_sentence = df_clean.copy()\n",
    "df_clean_sentence['keywordStrings'] = [' '.join(map(str, l)) for l in df_clean['keywordStrings']]\n",
    "df_clean_sentence['cleanFocusCategory'] = df_clean['cleanFocusCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df_clean_sentence['keywordStrings'], df_clean_sentence['cleanFocusCategory'], df_clean_sentence.index, test_size=0.33, random_state=0,stratify=df_clean_sentence['cleanFocusCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = df_clean_sentence['keywordStrings'][indices_test]\n",
    "\n",
    "sentences2 = labels\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = df_clean_sentence['cleanFocusCategory'][indices_train].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand_t/venv/lib64/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_cosine = cosine_scores * prop[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predicted_label = []\n",
    "for i in range(sentences1.shape[0]):\n",
    "    predicted_label.append(labels[np.argmax(test_cosine[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_sentence_test = pd.DataFrame()\n",
    "df_clean_sentence_test['keywordStrings'] = df_clean_sentence['keywordStrings'][indices_test]\n",
    "df_clean_sentence_test['cleanFocusCategory'] = df_clean_sentence['cleanFocusCategory'][indices_test]\n",
    "df_clean_sentence_test['predictedFocusCategory'] = predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3532913696344608"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(df_clean_sentence_test['cleanFocusCategory'],df_clean_sentence_test['predictedFocusCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# Opening JSON file\n",
    "f = open('/home/ferdinand_t/data/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "df['cleanFocusCategory'] = df['thematicFocusCategory'].apply(lambda x: x['name'] if x is not None else x)\n",
    "df = df[['teaser', 'cleanFocusCategory']]\n",
    "df_clean = df.copy()\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean_sentence = df_clean.copy()\n",
    "#df_clean_sentence['keywordStrings'] = [' '.join(map(str, l)) for l in df_clean['keywordStrings']]\n",
    "#df_clean_sentence['cleanFocusCategory'] = df_clean['cleanFocusCategory']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df_clean_sentence['teaser'], df_clean_sentence['cleanFocusCategory'], df_clean_sentence.index, test_size=0.33, random_state=0,stratify=df_clean_sentence['cleanFocusCategory'])\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = df_clean_sentence['teaser'][indices_test]\n",
    "\n",
    "sentences2 = labels\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "import numpy as np\n",
    "predicted_label = []\n",
    "for i in range(sentences1.shape[0]):\n",
    "    predicted_label.append(labels[np.argmax(cosine_scores[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_teaser_test = pd.DataFrame()\n",
    "df_clean_teaser_test['teaser'] = df_clean_sentence['teaser'][indices_test]\n",
    "df_clean_teaser_test['cleanFocusCategory'] = df_clean_sentence['cleanFocusCategory'][indices_test]\n",
    "df_clean_teaser_test['predictedFocusCategory'] = predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "teaser                    4805\n",
       "cleanFocusCategory        4805\n",
       "predictedFocusCategory    4805\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_teaser_test[df_clean_teaser_test['cleanFocusCategory'] == df_clean_teaser_test['predictedFocusCategory']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26372, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_teaser_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# Opening JSON file\n",
    "f = open('/home/ferdinand_t/data/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "df['cleanFocusCategory'] = df['thematicFocusCategory'].apply(lambda x: x['name'] if x is not None else x)\n",
    "df = df[['text', 'cleanFocusCategory']]\n",
    "df_clean = df.copy()\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean_sentence = df_clean.copy()\n",
    "#df_clean_sentence['keywordStrings'] = [' '.join(map(str, l)) for l in df_clean['keywordStrings']]\n",
    "#df_clean_sentence['cleanFocusCategory'] = df_clean['cleanFocusCategory']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df_clean_sentence['text'], df_clean_sentence['cleanFocusCategory'], df_clean_sentence.index, test_size=0.33, random_state=0,stratify=df_clean_sentence['cleanFocusCategory'])\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = df_clean_sentence['text'][indices_test]\n",
    "\n",
    "sentences2 = labels\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "import numpy as np\n",
    "predicted_label = []\n",
    "for i in range(sentences1.shape[0]):\n",
    "    predicted_label.append(labels[np.argmax(cosine_scores[i])])\n",
    "\n",
    "df_clean_text_test = pd.DataFrame()\n",
    "df_clean_text_test['text'] = df_clean_sentence['text'][indices_test]\n",
    "df_clean_text_test['cleanFocusCategory'] = df_clean_sentence['cleanFocusCategory'][indices_test]\n",
    "df_clean_text_test['predictedFocusCategory'] = predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                      4764\n",
       "cleanFocusCategory        4764\n",
       "predictedFocusCategory    4764\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_text_test[df_clean_text_test['cleanFocusCategory'] == df_clean_text_test['predictedFocusCategory']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26372, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_text_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "277df8790e3444fd20644b288c4310666d03521f19085f1ded8ea3953ac3e990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
