{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 11:34:00.191186: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-03 11:34:01.208858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-03 11:34:01.208880: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-03 11:34:04.436212: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-03 11:34:04.436372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-03 11:34:04.436383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(lst_keywords, wv):\n",
    "    \n",
    "    # Creating the vectorizer \n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit the model with our data (each keyword becomes a feature, some are split)\n",
    "    X = vectorizer.fit_transform(lst_keywords)\n",
    "\n",
    "    # Make an array and fills it in\n",
    "    CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Words in the vocabulary (some keywords are split)\n",
    "    WordsVocab=CountVectorizedData.columns\n",
    "\n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "\n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVectorizedData.shape[0]):\n",
    "\n",
    "        # initiating a sentence with all zeros\n",
    "        sentence = np.zeros(300)\n",
    "\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVectorizedData.iloc[i , :] >= 1]:\n",
    "            if word in wv.index_to_key:\n",
    "                sentence = sentence + wv[word] \n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data = W2Vec_Data.append(pd.DataFrame([sentence]))\n",
    "\n",
    "    return W2Vec_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DW data\n",
    "df_dw = pd.read_json('../data/interim/clean_keywords_2022-01-01_2023-01-01.json', orient ='split', compression = 'infer')\n",
    "\n",
    "# Load Google data\n",
    "df_google = pd.read_json('../data/interim/2022-daily-trending-searches.json', orient ='split', compression = 'infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google = pd.read_json('../data/interim/2022-daily-trending-searches.json', orient ='split', compression = 'infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>formattedValue</th>\n",
       "      <th>link</th>\n",
       "      <th>topic_mid</th>\n",
       "      <th>topic_title</th>\n",
       "      <th>topic_type</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3550</td>\n",
       "      <td>+3,550%</td>\n",
       "      <td>/trends/explore?q=/m/015cqh&amp;date=2022-01-01+20...</td>\n",
       "      <td>/m/015cqh</td>\n",
       "      <td>Journey</td>\n",
       "      <td>Band</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1350</td>\n",
       "      <td>+1,350%</td>\n",
       "      <td>/trends/explore?q=/m/0bdxs5&amp;date=2022-01-01+20...</td>\n",
       "      <td>/m/0bdxs5</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>American singer-songwriter</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>+1,000%</td>\n",
       "      <td>/trends/explore?q=/m/02cgn0&amp;date=2022-01-01+20...</td>\n",
       "      <td>/m/02cgn0</td>\n",
       "      <td>Countdown</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>+1,000%</td>\n",
       "      <td>/trends/explore?q=/m/01wqvm&amp;date=2022-01-01+20...</td>\n",
       "      <td>/m/01wqvm</td>\n",
       "      <td>Rose Parade</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>950</td>\n",
       "      <td>+950%</td>\n",
       "      <td>/trends/explore?q=/g/11n7k56n1w&amp;date=2022-01-0...</td>\n",
       "      <td>/g/11n7k56n1w</td>\n",
       "      <td>2021 Times Square Ball Drop</td>\n",
       "      <td>Event</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4217</th>\n",
       "      <td>100</td>\n",
       "      <td>+100%</td>\n",
       "      <td>/trends/explore?q=/m/0jm_&amp;date=2022-12-31+2022...</td>\n",
       "      <td>/m/0jm_</td>\n",
       "      <td>American football</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4218</th>\n",
       "      <td>80</td>\n",
       "      <td>+80%</td>\n",
       "      <td>/trends/explore?q=/g/11b77qrp3l&amp;date=2022-12-3...</td>\n",
       "      <td>/g/11b77qrp3l</td>\n",
       "      <td>2022</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4219</th>\n",
       "      <td>70</td>\n",
       "      <td>+70%</td>\n",
       "      <td>/trends/explore?q=/m/02hnnn&amp;date=2022-12-31+20...</td>\n",
       "      <td>/m/02hnnn</td>\n",
       "      <td>Bowl game</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>60</td>\n",
       "      <td>+60%</td>\n",
       "      <td>/trends/explore?q=/m/01mtb&amp;date=2022-12-31+202...</td>\n",
       "      <td>/m/01mtb</td>\n",
       "      <td>Cooking</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>60</td>\n",
       "      <td>+60%</td>\n",
       "      <td>/trends/explore?q=/m/0kc6x&amp;date=2022-12-31+202...</td>\n",
       "      <td>/m/0kc6x</td>\n",
       "      <td>ESPN</td>\n",
       "      <td>Cable company</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4222 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value formattedValue                                               link  \\\n",
       "0      3550        +3,550%  /trends/explore?q=/m/015cqh&date=2022-01-01+20...   \n",
       "1      1350        +1,350%  /trends/explore?q=/m/0bdxs5&date=2022-01-01+20...   \n",
       "2      1000        +1,000%  /trends/explore?q=/m/02cgn0&date=2022-01-01+20...   \n",
       "3      1000        +1,000%  /trends/explore?q=/m/01wqvm&date=2022-01-01+20...   \n",
       "4       950          +950%  /trends/explore?q=/g/11n7k56n1w&date=2022-01-0...   \n",
       "...     ...            ...                                                ...   \n",
       "4217    100          +100%  /trends/explore?q=/m/0jm_&date=2022-12-31+2022...   \n",
       "4218     80           +80%  /trends/explore?q=/g/11b77qrp3l&date=2022-12-3...   \n",
       "4219     70           +70%  /trends/explore?q=/m/02hnnn&date=2022-12-31+20...   \n",
       "4220     60           +60%  /trends/explore?q=/m/01mtb&date=2022-12-31+202...   \n",
       "4221     60           +60%  /trends/explore?q=/m/0kc6x&date=2022-12-31+202...   \n",
       "\n",
       "          topic_mid                  topic_title                  topic_type  \\\n",
       "0         /m/015cqh                      Journey                        Band   \n",
       "1         /m/0bdxs5                  Miley Cyrus  American singer-songwriter   \n",
       "2         /m/02cgn0                    Countdown                       Topic   \n",
       "3         /m/01wqvm                  Rose Parade                       Topic   \n",
       "4     /g/11n7k56n1w  2021 Times Square Ball Drop                       Event   \n",
       "...             ...                          ...                         ...   \n",
       "4217        /m/0jm_            American football                      Sports   \n",
       "4218  /g/11b77qrp3l                         2022                       Topic   \n",
       "4219      /m/02hnnn                    Bowl game                       Topic   \n",
       "4220       /m/01mtb                      Cooking                       Topic   \n",
       "4221       /m/0kc6x                         ESPN               Cable company   \n",
       "\n",
       "           date location  \n",
       "0    2022-01-01       US  \n",
       "1    2022-01-01       US  \n",
       "2    2022-01-01       US  \n",
       "3    2022-01-01       US  \n",
       "4    2022-01-01       US  \n",
       "...         ...      ...  \n",
       "4217 2022-12-31       US  \n",
       "4218 2022-12-31       US  \n",
       "4219 2022-12-31       US  \n",
       "4220 2022-12-31       US  \n",
       "4221 2022-12-31       US  \n",
       "\n",
       "[4222 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs for 1min20\n",
    "# Load pre-trained zero shot learning model\n",
    "pipe = pipeline(model=\"facebook/bart-large-mnli\") \n",
    "\n",
    "# Loads word2vec google model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match datasets in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date range in data\n",
    "start_date_dw = pd.to_datetime(df_dw['Date']).min()\n",
    "end_date_dw = pd.to_datetime(df_dw['Date']).max()\n",
    "\n",
    "# Remove rows witn no category\n",
    "df_dw.dropna(subset=['cleanFocusCategory'], inplace = True)\n",
    "df_dw.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Keeps only google data within DW data date range\n",
    "df_google.sort_values(by ='date', inplace = True) \n",
    "mask = (pd.to_datetime(df_google['date']) > start_date_dw) & (pd.to_datetime(df_google['date']) <= end_date_dw)\n",
    "df_google_subset = df_google.loc[mask].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 categories:  ['Catastrophe', 'Society', 'Education', 'Innovation', 'Health', 'Science', 'Offbeat', 'Business', 'Media', 'Cars and Transportation', 'Technology', 'Travel', 'Digital World', 'Sports', 'Learning German', 'History', 'Law and Justice', 'Lifestyle', 'Religion', 'Culture', 'Migration', 'Human Rights', 'Politics', 'Nature and Environment']\n"
     ]
    }
   ],
   "source": [
    "# Target variable (category)\n",
    "focus_category_list = list(set(df_dw['cleanFocusCategory']))\n",
    "print(len(focus_category_list), 'categories: ', focus_category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Topic' by None and remove them for now (TODO)\n",
    "df_google_subset['topic_type'].replace({'Topic': None}, inplace = True)\n",
    "df_google_subset.dropna(subset=['topic_type'], inplace = True)\n",
    "\n",
    "# Make a new column combining those 2\n",
    "df_google_subset['topic_title_type'] = df_google_subset['topic_title'] + ', ' + df_google_subset['topic_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Google keywords \n",
    "load = 1\n",
    "\n",
    "if load: \n",
    "    df_prediction = pd.read_json('../data/interim/zero_shot_prediction_google_keywords_2022-01-01_2023-01-01_all.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # Runs for 14min (for 100 google keyword lines)\n",
    "    df_prediction = df_google_subset[['topic_title', 'topic_type', 'topic_title_type']].copy()\n",
    "\n",
    "    # Runs the model\n",
    "    category_outputs = [pipe(kw, candidate_labels = focus_category_list) for kw in df_prediction['topic_title_type'].to_list()]\n",
    "    labels = list(map(lambda x: x['labels'][0], category_outputs))\n",
    "\n",
    "    # Add a column to dataframe\n",
    "    df_prediction['Predicted category'] = labels\n",
    "    df_prediction.to_json('../data/interim/zero_shot_prediction_google_keywords_2022-01-01_2023-01-01.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On DW keywords (to assess accuracy)\n",
    "load = 0\n",
    "\n",
    "if load: \n",
    "    df_dw_prediction = pd.read_json('../data/interim/zero_shot_prediction_dw_keywords_2022-01-01_2023-01-01_200.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # 1min for 4 words\n",
    "    # 55min for 200 articles\n",
    "    df_dw_prediction = df_dw[0:200].copy()\n",
    "\n",
    "    # Runs the model\n",
    "    category_outputs = [pipe(', '.join(kw), candidate_labels = focus_category_list) for kw in df_dw_prediction['keywordStringsCleanAfterFuzz']]\n",
    "    labels = list(map(lambda x: x['labels'][0], category_outputs))\n",
    "\n",
    "    # Add a column to dataframe\n",
    "    df_dw_prediction['Predicted category'] = labels\n",
    "    df_dw_prediction.to_json('../data/interim/zero_shot_prediction_dw_keywords_2022-01-01_2023-01-01_200.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>Date</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "      <th>cleanFocusCategory</th>\n",
       "      <th>Predicted category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60306089</td>\n",
       "      <td>2022-01-01T02:35:51.098Z</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[Belgium, explosion, apartment building]</td>\n",
       "      <td>[belgium, explosion, apartment building]</td>\n",
       "      <td>Catastrophe</td>\n",
       "      <td>Catastrophe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60305852</td>\n",
       "      <td>2022-01-01T03:10:21.161Z</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[Srebrenica genocide, Bosnian War, Yugoslav Wa...</td>\n",
       "      <td>[srebrenica genocide, bosnian war, yugoslav wa...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Catastrophe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60084276</td>\n",
       "      <td>2022-01-01T07:19:32.181Z</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[sport, football, Qatar 2022, World Cup, FIFA ...</td>\n",
       "      <td>[sports, football, qatar 2022, world cup, fifa...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60223754</td>\n",
       "      <td>2022-01-01T10:27:58.617Z</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[UN, OCHA, aid, Yemen, Afghanistan, Martin Gri...</td>\n",
       "      <td>[un, ocha, aid, yemen, afghanistan, martin gri...</td>\n",
       "      <td>Society</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60306645</td>\n",
       "      <td>2022-01-01T12:28:59.500Z</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>[irish language, European Union, official eu l...</td>\n",
       "      <td>[irish language, european union, official eu l...</td>\n",
       "      <td>Society</td>\n",
       "      <td>Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>60359581</td>\n",
       "      <td>2022-01-07T16:37:57.195Z</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>[female executives, executive boards, DAX, gen...</td>\n",
       "      <td>[female executives, executive boards, dax, gen...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>60361354</td>\n",
       "      <td>2022-01-07T16:59:27.766Z</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>[Coronavirus, COVID-19, Austria, Karl Nehammer]</td>\n",
       "      <td>[coronavirus, covid-19, austria, karl nehammer]</td>\n",
       "      <td>Health</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>60360285</td>\n",
       "      <td>2022-01-07T17:16:27.808Z</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>[Russia, Kazakhstan, Almaty, Moscow, unrest, p...</td>\n",
       "      <td>[russia, kazakhstan, almaty, moscow, unrest, p...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>60361419</td>\n",
       "      <td>2022-01-07T17:47:16.685Z</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>[US special forces, Albania]</td>\n",
       "      <td>[us special forces, albania]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>60361211</td>\n",
       "      <td>2022-01-07T18:08:52.421Z</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>[Poland, Pegasus, NSO group, spying, spyware, ...</td>\n",
       "      <td>[poland, pegasus, nso group, spying, spyware, ...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          lastModifiedDate       Date  \\\n",
       "0    60306089  2022-01-01T02:35:51.098Z 2022-01-01   \n",
       "1    60305852  2022-01-01T03:10:21.161Z 2022-01-01   \n",
       "2    60084276  2022-01-01T07:19:32.181Z 2022-01-01   \n",
       "3    60223754  2022-01-01T10:27:58.617Z 2022-01-01   \n",
       "4    60306645  2022-01-01T12:28:59.500Z 2022-01-01   \n",
       "..        ...                       ...        ...   \n",
       "195  60359581  2022-01-07T16:37:57.195Z 2022-01-07   \n",
       "196  60361354  2022-01-07T16:59:27.766Z 2022-01-07   \n",
       "197  60360285  2022-01-07T17:16:27.808Z 2022-01-07   \n",
       "198  60361419  2022-01-07T17:47:16.685Z 2022-01-07   \n",
       "199  60361211  2022-01-07T18:08:52.421Z 2022-01-07   \n",
       "\n",
       "                                        keywordStrings  \\\n",
       "0             [Belgium, explosion, apartment building]   \n",
       "1    [Srebrenica genocide, Bosnian War, Yugoslav Wa...   \n",
       "2    [sport, football, Qatar 2022, World Cup, FIFA ...   \n",
       "3    [UN, OCHA, aid, Yemen, Afghanistan, Martin Gri...   \n",
       "4    [irish language, European Union, official eu l...   \n",
       "..                                                 ...   \n",
       "195  [female executives, executive boards, DAX, gen...   \n",
       "196    [Coronavirus, COVID-19, Austria, Karl Nehammer]   \n",
       "197  [Russia, Kazakhstan, Almaty, Moscow, unrest, p...   \n",
       "198                       [US special forces, Albania]   \n",
       "199  [Poland, Pegasus, NSO group, spying, spyware, ...   \n",
       "\n",
       "                          keywordStringsCleanAfterFuzz cleanFocusCategory  \\\n",
       "0             [belgium, explosion, apartment building]        Catastrophe   \n",
       "1    [srebrenica genocide, bosnian war, yugoslav wa...           Politics   \n",
       "2    [sports, football, qatar 2022, world cup, fifa...             Sports   \n",
       "3    [un, ocha, aid, yemen, afghanistan, martin gri...            Society   \n",
       "4    [irish language, european union, official eu l...            Society   \n",
       "..                                                 ...                ...   \n",
       "195  [female executives, executive boards, dax, gen...           Business   \n",
       "196    [coronavirus, covid-19, austria, karl nehammer]             Health   \n",
       "197  [russia, kazakhstan, almaty, moscow, unrest, p...           Politics   \n",
       "198                       [us special forces, albania]           Politics   \n",
       "199  [poland, pegasus, nso group, spying, spyware, ...           Politics   \n",
       "\n",
       "    Predicted category  \n",
       "0          Catastrophe  \n",
       "1          Catastrophe  \n",
       "2               Sports  \n",
       "3              Offbeat  \n",
       "4                Media  \n",
       "..                 ...  \n",
       "195           Business  \n",
       "196             Health  \n",
       "197            Offbeat  \n",
       "198            Offbeat  \n",
       "199            Offbeat  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dw_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "      <th>cleanFocusCategory</th>\n",
       "      <th>Predicted category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[belgium, explosion, apartment building]</td>\n",
       "      <td>Catastrophe</td>\n",
       "      <td>Catastrophe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[srebrenica genocide, bosnian war, yugoslav wa...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Catastrophe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sports, football, qatar 2022, world cup, fifa...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[un, ocha, aid, yemen, afghanistan, martin gri...</td>\n",
       "      <td>Society</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[irish language, european union, official eu l...</td>\n",
       "      <td>Society</td>\n",
       "      <td>Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[female executives, executive boards, dax, gen...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[coronavirus, covid-19, austria, karl nehammer]</td>\n",
       "      <td>Health</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[russia, kazakhstan, almaty, moscow, unrest, p...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[us special forces, albania]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[poland, pegasus, nso group, spying, spyware, ...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          keywordStringsCleanAfterFuzz cleanFocusCategory  \\\n",
       "0             [belgium, explosion, apartment building]        Catastrophe   \n",
       "1    [srebrenica genocide, bosnian war, yugoslav wa...           Politics   \n",
       "2    [sports, football, qatar 2022, world cup, fifa...             Sports   \n",
       "3    [un, ocha, aid, yemen, afghanistan, martin gri...            Society   \n",
       "4    [irish language, european union, official eu l...            Society   \n",
       "..                                                 ...                ...   \n",
       "195  [female executives, executive boards, dax, gen...           Business   \n",
       "196    [coronavirus, covid-19, austria, karl nehammer]             Health   \n",
       "197  [russia, kazakhstan, almaty, moscow, unrest, p...           Politics   \n",
       "198                       [us special forces, albania]           Politics   \n",
       "199  [poland, pegasus, nso group, spying, spyware, ...           Politics   \n",
       "\n",
       "    Predicted category  \n",
       "0          Catastrophe  \n",
       "1          Catastrophe  \n",
       "2               Sports  \n",
       "3              Offbeat  \n",
       "4                Media  \n",
       "..                 ...  \n",
       "195           Business  \n",
       "196             Health  \n",
       "197            Offbeat  \n",
       "198            Offbeat  \n",
       "199            Offbeat  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dw_prediction[['keywordStringsCleanAfterFuzz', 'cleanFocusCategory', 'Predicted category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared to similar categories: 0.265\n"
     ]
    }
   ],
   "source": [
    "# Check prediction\n",
    "accuracy_sim_dw_categ = sum(df_dw_prediction['Predicted category'] == df_dw_prediction['cleanFocusCategory']) / len(df_dw_prediction)\n",
    "print('Compared to similar categories:', accuracy_sim_dw_categ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess performance: Compare to similar DW keyword - category pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation of DW keywords\n",
    "load = 1\n",
    "\n",
    "if load: \n",
    "    vec_keywords_dw = pd.read_json('../data/interim/vectorised_clean_keywords_2022-01-01_2023-01-01.json', orient ='split', compression = 'infer')\n",
    "else:\n",
    "    # runs during 10 min\n",
    "    lst_keywords_dw = df_dw['keywordStringsCleanAfterFuzz'].astype(str)\n",
    "    vec_keywords_dw = get_vectors(lst_keywords_dw, wv)\n",
    "    vec_keywords_dw.to_json('../data/interim/vectorised_clean_keywords_2022-01-01_2023-01-01.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation of DW category\n",
    "category_dw = focus_category_list\n",
    "vec_category_dw = get_vectors(category_dw, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google topic on DW keywords\n",
    "\n",
    "# Vectorisation of Google keywords\n",
    "lst_keywords_ggl = df_prediction['topic_type'].astype(str) # topic_type or topic_title or topic_title_type (best predictor: topic_type)\n",
    "vec_keywords_ggl = get_vectors(lst_keywords_ggl, wv)\n",
    "\n",
    "# Compute distances: google keywords vs DW keywords \n",
    "ggl_dw_word_distances = cosine_similarity(vec_keywords_ggl, vec_keywords_dw) \n",
    "\n",
    "# Get indices of similar (above threshold) DW keywords\n",
    "distance_threshold = 0.4\n",
    "ind_ggl_to_dw = [[ind for ind in range(len(ggl_word_list)) if ggl_word_list[ind] > distance_threshold] for ggl_word_list in ggl_dw_word_distances]\n",
    "\n",
    "# Extract most frequet category for similar DW keywords\n",
    "most_freq_categories = [df_dw['cleanFocusCategory'][lst_keywords_dw.index[ind_ggl_to_dw[i]]].mode()[0] if len(ind_ggl_to_dw[i]) > 0 else None for i in range(len(ind_ggl_to_dw))]\n",
    "\n",
    "# Make a new column in df\n",
    "df_prediction['Category of similar DW keywords'] = most_freq_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared to similar categories: 0.47164948453608246\n"
     ]
    }
   ],
   "source": [
    "distance_threshold = 0.2\n",
    "\n",
    "# Vectorisation of Google keywords\n",
    "lst_keywords_ggl = df_prediction['topic_type'].astype(str) # topic_type or topic_title or topic_title_type (best predictor: topic_type)\n",
    "vec_keywords_ggl = get_vectors(lst_keywords_ggl, wv)\n",
    "\n",
    "# Compute distances: google keywords vs DW categories\n",
    "ggl_dw_categ_distances = cosine_similarity(vec_keywords_ggl, vec_category_dw) \n",
    "\n",
    "# Get indices of similar (above threshold) DW keywords\n",
    "closest_categ = [[category_dw[dist.argmax()], round(dist.max(),2)] for dist in ggl_dw_categ_distances]\n",
    "\n",
    "# Make a new column in df\n",
    "df_prediction['Most similar DW category'] = [row[0] if row[1] > distance_threshold else None for row in closest_categ]\n",
    "\n",
    "# Check prediction\n",
    "accuracy_sim_dw_categ = sum(df_prediction['Predicted category'] == df_prediction['Most similar DW category']) / len(df_prediction)\n",
    "print('Compared to similar categories:', accuracy_sim_dw_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Compared to category of similar keywords: 0.3711340206185567\n",
      "Compared to similar categories: 0.47164948453608246\n"
     ]
    }
   ],
   "source": [
    "accuracy_sim_dw_keywords = sum(df_prediction['Predicted category'] == df_prediction['Category of similar DW keywords']) / len(df_prediction)\n",
    "accuracy_sim_dw_categ = sum(df_prediction['Predicted category'] == df_prediction['Most similar DW category']) / len(df_prediction)\n",
    "print('Accuracy:')\n",
    "print('Compared to category of similar keywords:', accuracy_sim_dw_keywords) \n",
    "print('Compared to similar categories:', accuracy_sim_dw_categ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_title</th>\n",
       "      <th>topic_type</th>\n",
       "      <th>topic_title_type</th>\n",
       "      <th>Predicted category</th>\n",
       "      <th>Category of similar DW keywords</th>\n",
       "      <th>Most similar DW category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>National Football League Playoffs</td>\n",
       "      <td>Championship</td>\n",
       "      <td>National Football League Playoffs, Championship</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Yellowstone</td>\n",
       "      <td>Drama series</td>\n",
       "      <td>Yellowstone, Drama series</td>\n",
       "      <td>Media</td>\n",
       "      <td>None</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Secondary school</td>\n",
       "      <td>School category</td>\n",
       "      <td>Secondary school, School category</td>\n",
       "      <td>Education</td>\n",
       "      <td>None</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Week</td>\n",
       "      <td>Unit of time</td>\n",
       "      <td>Week, Unit of time</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>None</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Tampa</td>\n",
       "      <td>City in Florida</td>\n",
       "      <td>Tampa, City in Florida</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Learning German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>Boris Johnson</td>\n",
       "      <td>Member of Parliament of the United Kingdom</td>\n",
       "      <td>Boris Johnson, Member of Parliament of the Uni...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>Thor</td>\n",
       "      <td>Film series</td>\n",
       "      <td>Thor, Film series</td>\n",
       "      <td>Media</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>Thor: Love and Thunder</td>\n",
       "      <td>2022 film</td>\n",
       "      <td>Thor: Love and Thunder, 2022 film</td>\n",
       "      <td>Media</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>Shinzo Abe</td>\n",
       "      <td>Former Prime Minister of Japan</td>\n",
       "      <td>Shinzo Abe, Former Prime Minister of Japan</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Learning German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Country in East Asia</td>\n",
       "      <td>Japan, Country in East Asia</td>\n",
       "      <td>Offbeat</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Digital World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            topic_title  \\\n",
       "35    National Football League Playoffs   \n",
       "42                          Yellowstone   \n",
       "41                     Secondary school   \n",
       "40                                 Week   \n",
       "39                                Tampa   \n",
       "...                                 ...   \n",
       "1963                      Boris Johnson   \n",
       "1967                               Thor   \n",
       "1968             Thor: Love and Thunder   \n",
       "1965                         Shinzo Abe   \n",
       "1966                              Japan   \n",
       "\n",
       "                                      topic_type  \\\n",
       "35                                  Championship   \n",
       "42                                  Drama series   \n",
       "41                               School category   \n",
       "40                                  Unit of time   \n",
       "39                               City in Florida   \n",
       "...                                          ...   \n",
       "1963  Member of Parliament of the United Kingdom   \n",
       "1967                                 Film series   \n",
       "1968                                   2022 film   \n",
       "1965              Former Prime Minister of Japan   \n",
       "1966                        Country in East Asia   \n",
       "\n",
       "                                       topic_title_type Predicted category  \\\n",
       "35      National Football League Playoffs, Championship             Sports   \n",
       "42                            Yellowstone, Drama series              Media   \n",
       "41                    Secondary school, School category          Education   \n",
       "40                                   Week, Unit of time            Offbeat   \n",
       "39                               Tampa, City in Florida            Offbeat   \n",
       "...                                                 ...                ...   \n",
       "1963  Boris Johnson, Member of Parliament of the Uni...           Politics   \n",
       "1967                                  Thor, Film series              Media   \n",
       "1968                  Thor: Love and Thunder, 2022 film              Media   \n",
       "1965         Shinzo Abe, Former Prime Minister of Japan           Politics   \n",
       "1966                        Japan, Country in East Asia            Offbeat   \n",
       "\n",
       "     Category of similar DW keywords Most similar DW category  \n",
       "35                            Sports                   Sports  \n",
       "42                              None                  Offbeat  \n",
       "41                              None                Education  \n",
       "40                              None                  History  \n",
       "39                          Politics          Learning German  \n",
       "...                              ...                      ...  \n",
       "1963                        Politics                  Society  \n",
       "1967                         Culture                  Offbeat  \n",
       "1968                         Culture                  Offbeat  \n",
       "1965                        Politics          Learning German  \n",
       "1966                        Politics            Digital World  \n",
       "\n",
       "[1164 rows x 6 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
