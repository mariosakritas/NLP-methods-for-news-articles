{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/interim/clean_keywords_2019-01-01_2022-01-01.json'\n",
    "df = pd.read_json(filepath, orient ='split', compression = 'infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8054/1398779016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loads word2vec google model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib64/python3.7/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1723\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1724\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m         )\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m             _word2vec_read_binary(\n\u001b[0;32m-> 2070\u001b[0;31m                 \u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_chunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2071\u001b[0m             )\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         processed_words, chunk = _add_bytes_to_kv(\n\u001b[0;32m-> 1965\u001b[0;31m             kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[0m\u001b[1;32m   1966\u001b[0m         \u001b[0mtot_processed_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocessed_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_chunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbinary_chunk_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib64/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_add_bytes_to_kv\u001b[0;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[0;31m# Some binary files are reported to have obsolete new line in the beginning of word, remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1946\u001b[0;31m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1947\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbytes_per_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loads word2vec google model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words are represented with a vector of dimension:  300\n",
      "Number of words (vocabulary) in the model: 3000000\n",
      "Example of words: ['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']\n"
     ]
    }
   ],
   "source": [
    "print('Words are represented with a vector of dimension: ', wv.vector_size)\n",
    "print('Number of words (vocabulary) in the model:', len(wv))\n",
    "print('Example of words:', wv.index_to_key[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['january', \"three kings' day\", 'epiphany', 'magi', 'tradition', 'europe']\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords\n",
    "keywords_str = df['keywordStringsCleanAfterFuzz'][19:20].astype(str)\n",
    "for kw in keywords_str:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>epiphany</th>\n",
       "      <th>europe</th>\n",
       "      <th>january</th>\n",
       "      <th>kings</th>\n",
       "      <th>magi</th>\n",
       "      <th>tradition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  epiphany  europe  january  kings  magi  tradition\n",
       "0    1         1       1        1      1     1          1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the vectorizer \n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the model with our data (each keyword becomes a feature, some are split)\n",
    "X = vectorizer.fit_transform(keywords_str)\n",
    "\n",
    "# Make an array and fills it in\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Words in the vocabulary (some keywords are split)\n",
    "WordsVocab=CountVectorizedData.columns\n",
    "\n",
    "CountVectorizedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bennu\n",
      "osiris\n",
      "bolsonaro\n",
      "guedes\n",
      "jair\n",
      "mourao\n"
     ]
    }
   ],
   "source": [
    "# Creating empty dataframe to hold sentences\n",
    "W2Vec_Data=pd.DataFrame()\n",
    "\n",
    "# Looping through each row for the data\n",
    "for i in range(CountVectorizedData.shape[0]):\n",
    "\n",
    "    # initiating a sentence with all zeros\n",
    "    sentence = np.zeros(300)\n",
    "\n",
    "    # Looping thru each word in the sentence and if its present in \n",
    "    # the Word2Vec model then storing its vector\n",
    "    for word in WordsVocab[CountVectorizedData.iloc[i , :] >= 1]:\n",
    "        if word in wv.index_to_key:   \n",
    "            sentence = sentence + wv[word]\n",
    "        else:\n",
    "            print(word) \n",
    "    # Appending the sentence to the dataframe\n",
    "    W2Vec_Data = W2Vec_Data.append(pd.DataFrame([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.182617</td>\n",
       "      <td>0.047119</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>0.467773</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>-0.589355</td>\n",
       "      <td>0.196259</td>\n",
       "      <td>-0.612854</td>\n",
       "      <td>0.220215</td>\n",
       "      <td>-0.068726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154785</td>\n",
       "      <td>0.447266</td>\n",
       "      <td>-0.489746</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>-0.051758</td>\n",
       "      <td>-0.420166</td>\n",
       "      <td>-0.286865</td>\n",
       "      <td>0.342285</td>\n",
       "      <td>0.759033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.154053</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.263184</td>\n",
       "      <td>-0.719360</td>\n",
       "      <td>-0.577271</td>\n",
       "      <td>-0.123596</td>\n",
       "      <td>0.309570</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.320068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.215561</td>\n",
       "      <td>0.845909</td>\n",
       "      <td>0.062622</td>\n",
       "      <td>0.300781</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>0.032059</td>\n",
       "      <td>-0.284668</td>\n",
       "      <td>0.453857</td>\n",
       "      <td>0.143555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.426941</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.209290</td>\n",
       "      <td>1.017578</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.282715</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>-0.980957</td>\n",
       "      <td>-0.179443</td>\n",
       "      <td>0.467773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016602</td>\n",
       "      <td>0.634399</td>\n",
       "      <td>0.228271</td>\n",
       "      <td>0.607666</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>-0.103760</td>\n",
       "      <td>-0.198181</td>\n",
       "      <td>-1.099854</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>0.262787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.182617  0.047119  0.154297  0.467773 -0.137695 -0.589355  0.196259   \n",
       "0 -0.154053  0.113770  0.041992 -0.263184 -0.719360 -0.577271 -0.123596   \n",
       "0 -0.426941  0.242188  0.209290  1.017578  0.179688  0.282715 -0.306396   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.612854  0.220215 -0.068726  ...  0.154785  0.447266 -0.489746  0.336731   \n",
       "0  0.309570  0.201172  0.320068  ...  0.016357  0.215561  0.845909  0.062622   \n",
       "0 -0.980957 -0.179443  0.467773  ... -0.016602  0.634399  0.228271  0.607666   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.023804 -0.051758 -0.420166 -0.286865  0.342285  0.759033  \n",
       "0  0.300781  0.254883  0.032059 -0.284668  0.453857  0.143555  \n",
       "0 -0.173828 -0.103760 -0.198181 -1.099854 -0.002441  0.262787  \n",
       "\n",
       "[3 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2Vec_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "      <th>cleanFocusCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46912921</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46911356</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "      <td>Law and Justice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46909694</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46912694</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "      <td>Law and Justice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46910092</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33824</th>\n",
       "      <td>60304782</td>\n",
       "      <td>2021-12-31T18:47:57.479Z</td>\n",
       "      <td>[Putin, New Year, Coronavirus, Navalny, Ukraine]</td>\n",
       "      <td>[putin, new year, coronavirus, navalny, ukraine]</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33825</th>\n",
       "      <td>60299904</td>\n",
       "      <td>2021-12-31T19:06:43.423Z</td>\n",
       "      <td>[Germany, Olaf Scholz, New Year, New Year's ad...</td>\n",
       "      <td>[germany, olaf scholz, new year, new year's ad...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33826</th>\n",
       "      <td>60300458</td>\n",
       "      <td>2021-12-31T20:27:51.092Z</td>\n",
       "      <td>[Colorado, wildfires, Boulder County, evacuati...</td>\n",
       "      <td>[colorado, wildfires, boulder county, evacuati...</td>\n",
       "      <td>Catastrophe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33827</th>\n",
       "      <td>60267980</td>\n",
       "      <td>2021-12-31T20:32:20.303Z</td>\n",
       "      <td>[RCEP, Regional Comprehensive Economic Partner...</td>\n",
       "      <td>[rcep, regional comprehensive economic partner...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33828</th>\n",
       "      <td>60304913</td>\n",
       "      <td>2021-12-31T20:38:11.201Z</td>\n",
       "      <td>[France, birth control, contraception, contrac...</td>\n",
       "      <td>[france, birth control, contraception, contrac...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33829 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id          lastModifiedDate  \\\n",
       "0      46912921  2019-01-01T03:57:28.904Z   \n",
       "1      46911356  2019-01-01T06:11:50.527Z   \n",
       "2      46909694  2019-01-01T06:14:35.563Z   \n",
       "3      46912694  2019-01-01T08:26:11.599Z   \n",
       "4      46910092  2019-01-01T09:05:00.736Z   \n",
       "...         ...                       ...   \n",
       "33824  60304782  2021-12-31T18:47:57.479Z   \n",
       "33825  60299904  2021-12-31T19:06:43.423Z   \n",
       "33826  60300458  2021-12-31T20:27:51.092Z   \n",
       "33827  60267980  2021-12-31T20:32:20.303Z   \n",
       "33828  60304913  2021-12-31T20:38:11.201Z   \n",
       "\n",
       "                                          keywordStrings  \\\n",
       "0                    [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "1      [English Channel, migration, boats, illegal im...   \n",
       "2      [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "3                       [Japan, Tokyo, Harajuku, attack]   \n",
       "4      [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "...                                                  ...   \n",
       "33824   [Putin, New Year, Coronavirus, Navalny, Ukraine]   \n",
       "33825  [Germany, Olaf Scholz, New Year, New Year's ad...   \n",
       "33826  [Colorado, wildfires, Boulder County, evacuati...   \n",
       "33827  [RCEP, Regional Comprehensive Economic Partner...   \n",
       "33828  [France, birth control, contraception, contrac...   \n",
       "\n",
       "                            keywordStringsCleanAfterFuzz cleanFocusCategory  \n",
       "0                    [nasa, osiris-rex, bennu, asteroid]            Science  \n",
       "1      [english channel, migration, boats, illegal im...    Law and Justice  \n",
       "2      [brazil, jair bolsonaro, chicago economics, ha...           Politics  \n",
       "3                       [japan, tokyo, harajuku, attack]    Law and Justice  \n",
       "4      [asia, bangladesh, elections, kamal hossain, s...           Politics  \n",
       "...                                                  ...                ...  \n",
       "33824   [putin, new year, coronavirus, navalny, ukraine]           Politics  \n",
       "33825  [germany, olaf scholz, new year, new year's ad...           Politics  \n",
       "33826  [colorado, wildfires, boulder county, evacuati...        Catastrophe  \n",
       "33827  [rcep, regional comprehensive economic partner...           Business  \n",
       "33828  [france, birth control, contraception, contrac...             Health  \n",
       "\n",
       "[33829 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
