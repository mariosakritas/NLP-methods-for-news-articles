{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# # Opening JSON file\n",
    "# f = open('/home/ferdinand_t/data/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "# # returns JSON object as\n",
    "# # a dictionary\n",
    "# data = json.load(f)\n",
    "# df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# df['firstKeyword'] = df['keywords'].apply(lambda x: x[0]['name'] if len(x) != 0 else None)\n",
    "# # #df['secondKeyword'] = df['keywords'].apply(lambda x: x[1]['name'] if len(x) > 1 else None)\n",
    "# # #df['thirdKeyword'] = df['keywords'].apply(lambda x: x[2]['name'] if len(x) > 2 else None)\n",
    "# # #df['fourthKeyword'] = df['keywords'].apply(lambda x: x[3]['name'] if len(x) > 3 else None)\n",
    "\n",
    "# df['cleanFocusCategory'] = df['thematicFocusCategory'].apply(lambda x: x['name'] if x is not None else x)\n",
    "\n",
    "# # #df = df[['firstKeyword', 'secondKeyword', 'thirdKeyword', 'fourthKeyword', 'thematicFocusCategory', 'cleanFocusCategory']]\n",
    "\n",
    "# df = df[['firstKeyword', 'keywordStrings', 'cleanFocusCategory']]\n",
    "\n",
    "# df_clean = df.copy()\n",
    "# df_clean = df_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data...\n",
      "Loading data DONE. Number of articles is 175659\n",
      "Extracting data DONE. Number of articles from 2019-01-01 to 2022-05-01 is 37470\n",
      "Cleaning step 1 out of 2 DONE. Number of unique keywords went from 35255 to 32552\n",
      "Cleaning step 2 out of 2 DONE. Number of unique keywords went from 32552 to 30124\n",
      "Finished. Data stored in ../data/interim/clean_keywords_2019-01-01_2022-05-01.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "# Add src folder to the path\n",
    "sys.path.append('../src/')\n",
    "from data.preprocess_keywords import make_cleaned_keywords_df\n",
    "from data.make_datasets import get_data\n",
    "# Specify wanted time range\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2022-05-01'\n",
    "# Path to data\n",
    "data_file = '../data/raw/CMS_2010_to_June_2022_ENGLISH.json'\n",
    "# Load and extract data within time range\n",
    "df_subset = get_data(data_file, start_date, end_date)\n",
    "# Cleans keywords and saves data as a dataframe\n",
    "make_cleaned_keywords_df(df_subset, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filepath = '../data/interim/clean_keywords_' + start_date + '_' + end_date + '.json'\n",
    "df = pd.read_json(filepath, orient ='split', compression = 'infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>Date</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>cleanFocusParentCategory</th>\n",
       "      <th>cleanFocusCategory</th>\n",
       "      <th>teaser</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60278</th>\n",
       "      <td>46912921</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>Science</td>\n",
       "      <td>Science</td>\n",
       "      <td>The OSIRIS-REx spacecraft had arrived at the l...</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60279</th>\n",
       "      <td>46911356</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>Law and Justice</td>\n",
       "      <td>Law and Justice</td>\n",
       "      <td>The UK is withdrawing patrol ships from overse...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60280</th>\n",
       "      <td>46909694</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Brazil is inaugurating President Jair Bolsonar...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60281</th>\n",
       "      <td>46912694</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>Law and Justice</td>\n",
       "      <td>Crime</td>\n",
       "      <td>A man with an \"intent to murder\" has driven a ...</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60282</th>\n",
       "      <td>46910092</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>In an exclusive interview with DW, Kamal Hossa...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173911</th>\n",
       "      <td>61644122</td>\n",
       "      <td>2022-04-30T12:48:52.512Z</td>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>[New Mexico, wildfires, Southwest, United States]</td>\n",
       "      <td>Nature and Environment</td>\n",
       "      <td>Climate</td>\n",
       "      <td>Thousands of residents of mountain towns in Ne...</td>\n",
       "      <td>[new mexico, wildfires, southwest, united states]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173910</th>\n",
       "      <td>61643472</td>\n",
       "      <td>2022-04-30T12:49:44.354Z</td>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>[Mexico, migration, Joe Biden, Andres Manuel L...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Joe Biden and Andres Manuel Lopez Obrador disc...</td>\n",
       "      <td>[mexico, migration, joe biden, andres manuel l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173866</th>\n",
       "      <td>61623155</td>\n",
       "      <td>2022-04-30T14:20:13.329Z</td>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>[football]</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>The Italian-Dutch \"superagent\" Mino Raiola has...</td>\n",
       "      <td>[football]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173907</th>\n",
       "      <td>61642710</td>\n",
       "      <td>2022-04-30T14:20:14.415Z</td>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>[Boris Becker, tennis, finance, bankruptcy, Ge...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Boris Becker has been sentenced to prison in t...</td>\n",
       "      <td>[boris becker, tennis, finance, bankruptcy, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173920</th>\n",
       "      <td>61647523</td>\n",
       "      <td>2022-04-30T19:10:42.575Z</td>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>[Wolfsburg, Women's Football, Barcelona]</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "      <td>After losing 5-1 in Barcelona, Wolfsburg were ...</td>\n",
       "      <td>[wolfsburg, womens football, barcelona]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37470 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id          lastModifiedDate       Date  \\\n",
       "60278   46912921  2019-01-01T03:57:28.904Z 2019-01-01   \n",
       "60279   46911356  2019-01-01T06:11:50.527Z 2019-01-01   \n",
       "60280   46909694  2019-01-01T06:14:35.563Z 2019-01-01   \n",
       "60281   46912694  2019-01-01T08:26:11.599Z 2019-01-01   \n",
       "60282   46910092  2019-01-01T09:05:00.736Z 2019-01-01   \n",
       "...          ...                       ...        ...   \n",
       "173911  61644122  2022-04-30T12:48:52.512Z 2022-04-30   \n",
       "173910  61643472  2022-04-30T12:49:44.354Z 2022-04-30   \n",
       "173866  61623155  2022-04-30T14:20:13.329Z 2022-04-30   \n",
       "173907  61642710  2022-04-30T14:20:14.415Z 2022-04-30   \n",
       "173920  61647523  2022-04-30T19:10:42.575Z 2022-04-30   \n",
       "\n",
       "                                           keywordStrings  \\\n",
       "60278                 [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "60279   [English Channel, migration, boats, illegal im...   \n",
       "60280   [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "60281                    [Japan, Tokyo, Harajuku, attack]   \n",
       "60282   [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "...                                                   ...   \n",
       "173911  [New Mexico, wildfires, Southwest, United States]   \n",
       "173910  [Mexico, migration, Joe Biden, Andres Manuel L...   \n",
       "173866                                         [football]   \n",
       "173907  [Boris Becker, tennis, finance, bankruptcy, Ge...   \n",
       "173920           [Wolfsburg, Women's Football, Barcelona]   \n",
       "\n",
       "       cleanFocusParentCategory cleanFocusCategory  \\\n",
       "60278                   Science            Science   \n",
       "60279           Law and Justice    Law and Justice   \n",
       "60280                  Politics           Politics   \n",
       "60281           Law and Justice              Crime   \n",
       "60282                  Politics           Politics   \n",
       "...                         ...                ...   \n",
       "173911   Nature and Environment            Climate   \n",
       "173910                 Politics           Politics   \n",
       "173866                   Sports             Sports   \n",
       "173907                   Sports             Sports   \n",
       "173920                   Sports             Sports   \n",
       "\n",
       "                                                   teaser  \\\n",
       "60278   The OSIRIS-REx spacecraft had arrived at the l...   \n",
       "60279   The UK is withdrawing patrol ships from overse...   \n",
       "60280   Brazil is inaugurating President Jair Bolsonar...   \n",
       "60281   A man with an \"intent to murder\" has driven a ...   \n",
       "60282   In an exclusive interview with DW, Kamal Hossa...   \n",
       "...                                                   ...   \n",
       "173911  Thousands of residents of mountain towns in Ne...   \n",
       "173910  Joe Biden and Andres Manuel Lopez Obrador disc...   \n",
       "173866  The Italian-Dutch \"superagent\" Mino Raiola has...   \n",
       "173907  Boris Becker has been sentenced to prison in t...   \n",
       "173920  After losing 5-1 in Barcelona, Wolfsburg were ...   \n",
       "\n",
       "                             keywordStringsCleanAfterFuzz  \n",
       "60278                 [nasa, osiris-rex, bennu, asteroid]  \n",
       "60279   [english channel, migration, boats, illegal im...  \n",
       "60280   [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "60281                    [japan, tokyo, harajuku, attack]  \n",
       "60282   [asia, bangladesh, elections, kamal hossain, s...  \n",
       "...                                                   ...  \n",
       "173911  [new mexico, wildfires, southwest, united states]  \n",
       "173910  [mexico, migration, joe biden, andres manuel l...  \n",
       "173866                                         [football]  \n",
       "173907  [boris becker, tennis, finance, bankruptcy, ge...  \n",
       "173920            [wolfsburg, womens football, barcelona]  \n",
       "\n",
       "[37470 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['firstKeyword'] = df['keywordStringsCleanAfterFuzz'].apply(lambda x: x[0] if len(x) != 0 else None)\n",
    "# #df['secondKeyword'] = df['keywords'].apply(lambda x: x[1]['name'] if len(x) > 1 else None)\n",
    "# #df['thirdKeyword'] = df['keywords'].apply(lambda x: x[2]['name'] if len(x) > 2 else None)\n",
    "# #df['fourthKeyword'] = df['keywords'].apply(lambda x: x[3]['name'] if len(x) > 3 else None)\n",
    "\n",
    "#df['cleanFocusCategory'] = df['thematicFocusCategory'].apply(lambda x: x['name'] if x is not None else x)\n",
    "\n",
    "# #df = df[['firstKeyword', 'secondKeyword', 'thirdKeyword', 'fourthKeyword', 'thematicFocusCategory', 'cleanFocusCategory']]\n",
    "\n",
    "df = df[['firstKeyword', 'keywordStringsCleanAfterFuzz', 'cleanFocusCategory']]\n",
    "\n",
    "df_clean = df.copy()\n",
    "df_clean = df_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstKeyword</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "      <th>cleanFocusCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60278</th>\n",
       "      <td>nasa</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60279</th>\n",
       "      <td>english channel</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "      <td>Law and Justice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60280</th>\n",
       "      <td>brazil</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60281</th>\n",
       "      <td>japan</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60282</th>\n",
       "      <td>asia</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173911</th>\n",
       "      <td>new mexico</td>\n",
       "      <td>[new mexico, wildfires, southwest, united states]</td>\n",
       "      <td>Climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173910</th>\n",
       "      <td>mexico</td>\n",
       "      <td>[mexico, migration, joe biden, andres manuel l...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173866</th>\n",
       "      <td>football</td>\n",
       "      <td>[football]</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173907</th>\n",
       "      <td>boris becker</td>\n",
       "      <td>[boris becker, tennis, finance, bankruptcy, ge...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173920</th>\n",
       "      <td>wolfsburg</td>\n",
       "      <td>[wolfsburg, womens football, barcelona]</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37074 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           firstKeyword                       keywordStringsCleanAfterFuzz  \\\n",
       "60278              nasa                [nasa, osiris-rex, bennu, asteroid]   \n",
       "60279   english channel  [english channel, migration, boats, illegal im...   \n",
       "60280            brazil  [brazil, jair bolsonaro, chicago economics, ha...   \n",
       "60281             japan                   [japan, tokyo, harajuku, attack]   \n",
       "60282              asia  [asia, bangladesh, elections, kamal hossain, s...   \n",
       "...                 ...                                                ...   \n",
       "173911       new mexico  [new mexico, wildfires, southwest, united states]   \n",
       "173910           mexico  [mexico, migration, joe biden, andres manuel l...   \n",
       "173866         football                                         [football]   \n",
       "173907     boris becker  [boris becker, tennis, finance, bankruptcy, ge...   \n",
       "173920        wolfsburg            [wolfsburg, womens football, barcelona]   \n",
       "\n",
       "       cleanFocusCategory  \n",
       "60278             Science  \n",
       "60279     Law and Justice  \n",
       "60280            Politics  \n",
       "60281               Crime  \n",
       "60282            Politics  \n",
       "...                   ...  \n",
       "173911            Climate  \n",
       "173910           Politics  \n",
       "173866             Sports  \n",
       "173907             Sports  \n",
       "173920             Sports  \n",
       "\n",
       "[37074 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48492031058438906"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df_clean['keywordStringsCleanAfterFuzz'], df_clean['cleanFocusCategory'], df_clean.index, test_size=0.33, random_state=0, stratify=df_clean['cleanFocusCategory'])\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features_train = tfidf.fit_transform(X_train.astype(str))\n",
    "\n",
    "features_test = tfidf.transform(df_clean['firstKeyword'][indices_test])\n",
    "#labels_test = y_test\n",
    "model = LinearSVC()\n",
    "model.fit(features_train, y_train)\n",
    "y_pred = model.predict(features_test)\n",
    "accuracy_score(y_test[indices_test],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['firstKeyword'] = df_subset['keywords'].apply(lambda x: x[0]['name'] if len(x) != 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_subset[['firstKeyword', 'keywordStrings', 'cleanFocusCategory']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_clean = df_subset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37077, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37074, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.480385746976136"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(df_subset_clean['keywordStrings'], df_subset_clean['cleanFocusCategory'], df_subset_clean.index, test_size=0.33, random_state=0, stratify=df_subset_clean['cleanFocusCategory'])\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features_train = tfidf.fit_transform(X_train.astype(str))\n",
    "\n",
    "features_test = tfidf.transform(df_subset_clean['firstKeyword'][indices_test])\n",
    "#labels_test = y_test\n",
    "model = LinearSVC()\n",
    "model.fit(features_train, y_train)\n",
    "y_pred = model.predict(features_test)\n",
    "accuracy_score(y_test[indices_test],y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "277df8790e3444fd20644b288c4310666d03521f19085f1ded8ea3953ac3e990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
