{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook pipeline to dw-google analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import functools\n",
    "import operator\n",
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pytrends\n",
    "from pytrends.request import TrendReq as UTrendReq\n",
    "from datetime import date\n",
    "import datetime as d\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from rapidfuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df \n",
    "\n",
    "\n",
    "filepath = '../data/interim/clean_keywords_2019-2021_after_RapidFuzz.json'\n",
    "\n",
    "df = pd.read_json(filepath, orient ='split', compression = 'infer')\n",
    "df_clean =df\n",
    "flat_keywords=list(itertools.chain(*list(df['keywordStringsCleanAfterFuzz'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'donald trump'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input any keyword \n",
    "\n",
    "input_keyword = 'Donald Trump'\n",
    "start_date='2019-01-01'\n",
    "end_date='2021-01-01'\n",
    "#interactive version\n",
    "# pre_keyword = remove_spaces(str(input('Please input keyword to be analyzed:\\n')).lower())\n",
    "# start_date = str(input('Please input start date (YYYY-MM-DD):\\n'))\n",
    "# end_date = str(input('Please input end date (YYYY-MM-DD):\\n'))\n",
    "#df_clean = truncate_data(df_clean, start_date, end_date)\n",
    "\n",
    "#match it to out database\n",
    "keyword=process.extractOne(input_keyword, flat_keywords)[0]\n",
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          lastModifiedDate  \\\n",
       "0  2019-01-01T03:57:28.904Z  2019-01-01T03:57:28.904Z   \n",
       "1  2019-01-01T06:11:50.527Z  2019-01-01T06:11:50.527Z   \n",
       "2  2019-01-01T06:14:35.563Z  2019-01-01T06:14:35.563Z   \n",
       "3  2019-01-01T08:26:11.599Z  2019-01-01T08:26:11.599Z   \n",
       "4  2019-01-01T09:05:00.736Z  2019-01-01T09:05:00.736Z   \n",
       "\n",
       "                                      keywordStrings  \\\n",
       "0                [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "1  [English Channel, migration, boats, illegal im...   \n",
       "2  [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "3                   [Japan, Tokyo, Harajuku, attack]   \n",
       "4  [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "\n",
       "                        keywordStringsCleanAfterFuzz  \n",
       "0                [nasa, osiris-rex, bennu, asteroid]  \n",
       "1  [english channel, migration, boats, illegal im...  \n",
       "2  [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "3                   [japan, tokyo, harajuku, attack]  \n",
       "4  [asia, bangladesh, elections, kamal hossain, s...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Ferdinand's stuff to resample dataframe\n",
    "# df['lastModifiedDate'] = pd.to_datetime(df['lastModifiedDate'])\n",
    "# df.set_index('lastModifiedDate', inplace = True)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# #plot number of unique ID per week\n",
    "# df.resample('1W').count()['id'].plot()\n",
    "# plt.title('Number of unique id per week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get google method\n",
    "GET_METHOD='get'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'authority': 'trends.google.com',\n",
    "    'accept': 'application/json, text/plain, */*',\n",
    "    'accept-language': 'el-GR,el;q=0.9,en;q=0.8,es;q=0.7',\n",
    "    'content-type': 'application/json;charset=UTF-8',\n",
    "    'cookie': '__utma=10102256.1937595387.1677588086.1677588086.1678441622.2; __utmc=10102256; __utmz=10102256.1678441622.2.2.utmcsr=trends.google.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utmt=1; __utmb=10102256.13.9.1678442016068; CONSENT=YES+GB.en-GB+; HSID=AwrWd8APwv-yBWgzh; SSID=AeXCoum7ArBP5_-aa; APISID=CH4IjtEJhVzXdXGB/AFPE6uuFtOUDiSjnb; SAPISID=FcPgZF83fs0zxFml/Ad59_bwdrgg_kZ4q4; __Secure-1PAPISID=FcPgZF83fs0zxFml/Ad59_bwdrgg_kZ4q4; __Secure-3PAPISID=FcPgZF83fs0zxFml/Ad59_bwdrgg_kZ4q4; SID=TwhPHvTugfJu62Xh-HCJkOIPdoEDrL6q-6Eu9itbEI8mmKw9wzJdgT6c48lYdsNyN4E5xA.; __Secure-1PSID=TwhPHvTugfJu62Xh-HCJkOIPdoEDrL6q-6Eu9itbEI8mmKw9mFTrJ0j2r8zMRcq3v-A7Dg.; __Secure-3PSID=TwhPHvTugfJu62Xh-HCJkOIPdoEDrL6q-6Eu9itbEI8mmKw9xQIlYIR6TyZD2qXkeuopSA.; OGPC=19031986-1:; AEC=ARSKqsLpZW_sbZN2NdijlA8HPzuRHa1TPtYLHLGgaOIZpt8oJZL9PYZZYQ; SEARCH_SAMESITE=CgQI4ZcB; 1P_JAR=2023-03-10-09; NID=511=bYRTpZST7bJyL0z371h4Y79EMA1j9QqQFUpi8vJsSmiWdINx5gKruSDljEBAFfs9FYsxRrmP7vulT_MdtU2xEXQSW837vsgNY9s0i2WZAeFETmMEDrju3d_HgA2Wxy5DrFrIOaOiFu6LkpD7pY4wF4qrMZ38BzvW4NkYX_fUI7bFzHXsg24iHara1hPmPIXOSl6wQgsssfGHUntOI9PgY_eXaAEJbY7VgTr1hjNvEDlFSYOuzLvHSzo9kX9ALXA5-WOICbuLdAucZc3hJKo1dUKM51JCkzLsUHm99MWA86Icz-dmMW8ybQZhEUd2YgsBHHn5MV8uSVpcZ53n4_KL7r6sOpfWZ0ZXairmL3NH-hHz4Vyq; _gid=GA1.3.1682047475.1678441583; OTZ=6935626_48_48_123900_44_436380; _gat_gtag_UA_4401283=1; _ga=GA1.3.1937595387.1677588086; SIDCC=AFvIBn_I_znBUYDEoxfE1jUbrp_F8T607DZhlzI9o_gQoZmA4OxNjglOrH8Q8er3Cv4uzoWYkX9Z; __Secure-1PSIDCC=AFvIBn_Nhc9nywxJ_UrRYogvErcX48ygHEiBzjRRZtPe-mIwBTe_M7UbvKR4d-rAuhYyGJi-Dm0; __Secure-3PSIDCC=AFvIBn8vpeAOp5e0oAWBAETEzSClsyQlm3vQJhAQP7T7Z51q1K7zHDm_-CSGFEPasFw0sRHoJDU; _ga_VWZPXDNJJB=GS1.1.1678441583.2.1.1678442016.0.0.0',\n",
    "    'origin': 'https://trends.google.com',\n",
    "    'referer': 'https://trends.google.com/trends/explore?date=now%201-d&q=Adele&hl=en-GB',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"110\", \"Not A(Brand\";v=\"24\", \"Google Chrome\";v=\"110\"',\n",
    "    'sec-ch-ua-arch': '\"x86\"',\n",
    "    'sec-ch-ua-bitness': '\"64\"',\n",
    "    'sec-ch-ua-full-version': '\"110.0.5481.177\"',\n",
    "    'sec-ch-ua-full-version-list': '\"Chromium\";v=\"110.0.5481.177\", \"Not A(Brand\";v=\"24.0.0.0\", \"Google Chrome\";v=\"110.0.5481.177\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-model': '',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-ch-ua-platform-version': '\"13.2.1\"',\n",
    "    'sec-ch-ua-wow64': '?0',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "    'x-client-data': 'CIq2yQEIprbJAQjEtskBCKmdygEIkufKAQiVocsBCPyqzAEI9/XMAQib/swBCI6MzQEIlZbNAQiols0BCOGXzQEI5JfNAQjzl80BCMyYzQEI2JjNAQjzmc0BCLSazQEI0uGsAg==',\n",
    "}\n",
    "\n",
    "\n",
    "class TrendReq(UTrendReq):\n",
    "    def _get_data(self, url, method=GET_METHOD, trim_chars=0, **kwargs):\n",
    "        return super()._get_data(url, method=GET_METHOD, trim_chars=trim_chars, headers=headers, **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract google search for the keyword\n",
    "def get_interest_over_time(keyword, start_date = '2019-01-01', end_date=f'{date.today()}'):\n",
    "    #keywords needs to be a list \n",
    "    #need to make sure the total number of characters is less than 100 for Google and terms are fewer than 5 \n",
    "    # terms = 0\n",
    "    # chars = 0\n",
    "    # for word in keywords:\n",
    "    #     chars += len(word)\n",
    "    #     if chars > 99:\n",
    "    #         break\n",
    "    #     else:\n",
    "    #         terms += 1\n",
    "    # if terms > 5:\n",
    "    #     terms = 5\n",
    "    # keywords = keywords[:terms]\n",
    "    print(keyword)\n",
    "    if len(keyword)>99:\n",
    "        print('KEYWORD IS TOO LONG FOR THIS SEARCH')\n",
    "        return None\n",
    "    \n",
    "    #let's get python trends \n",
    "    pytrend = TrendReq(                    \n",
    "    # proxies=['https://34.203.233.13:80','https://35.201.123.31:880'], \n",
    "    # #hl='en-US', tz=360, timeout=(10,25)\n",
    "    retries=2, backoff_factor=0.1, requests_args={'verify':False})\n",
    "    google_df = pytrend.build_payload(kw_list= [keyword], timeframe= '{} {}'.format(str(start_date),str(end_date)))\n",
    "    google_df = pytrend.interest_over_time()\n",
    "    if 'isPartial' in google_df.columns:\n",
    "        google_df = google_df.drop('isPartial', axis = 'columns')\n",
    "\n",
    "    return google_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anya_m/Documents/venv/lib64/python3.7/site-packages/urllib3/connectionpool.py:1052: InsecureRequestWarning: Unverified HTTPS request is being made to host 'trends.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/home/anya_m/Documents/venv/lib64/python3.7/site-packages/urllib3/connectionpool.py:1052: InsecureRequestWarning: Unverified HTTPS request is being made to host 'trends.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/home/anya_m/Documents/venv/lib64/python3.7/site-packages/urllib3/connectionpool.py:1052: InsecureRequestWarning: Unverified HTTPS request is being made to host 'trends.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/home/anya_m/Documents/venv/lib64/python3.7/site-packages/urllib3/connectionpool.py:1052: InsecureRequestWarning: Unverified HTTPS request is being made to host 'trends.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "google_searches = get_interest_over_time(keyword, start_date = start_date, end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# def get_dw_timeseries(df_clean, keyword, resolution = 'weekly', start = 2019, end = 2023):\n",
    "#     #TODO: check this function after replacing loop \n",
    "    \n",
    "#     not_keyword_indices = [] #TODO: do this without a loop (create extra boolean column and assign True if keyword is there)\n",
    "#     for i, row in enumerate(df_clean['keywordStrings']):\n",
    "#         if keyword not in row:\n",
    "#             not_keyword_indices.append(i)\n",
    "\n",
    "#     df_clean = df_clean.drop(not_keyword_indices)\n",
    "#     df_clean['datetimes']= pd.to_datetime(df_clean['lastModifiedDate'])\n",
    "#     df_clean['yearweek'] = df_clean['datetimes'].apply(lambda x: str(x.year)+str(x.week))\n",
    "\n",
    "#     all_weeks = [str(year)+str(week).zfill(2) for week in range(1, 53) for year in range(start, end)] #TODO is *100 here necessary?\n",
    "#     not_in_df = list(set(all_weeks) - set(df_clean['yearweek'].tolist()))\n",
    "#     dw_mentions = dict(Counter(df_clean['yearweek'].tolist()))\n",
    "#     for key_ in not_in_df:\n",
    "#         dw_mentions[key_] = 0 \n",
    "    \n",
    "#     df_dw_mentions = pd.DataFrame.from_dict(dw_mentions, orient='index', columns=['val'])\n",
    "#     df_dw_mentions['week_str'] = [str(i) for i in df_dw_mentions.index]\n",
    "#     df_dw_mentions = df_dw_mentions.sort_values(by='week_str')\n",
    "\n",
    "#     return df_dw_mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dw_timeseries(df_clean, keyword, resolution = 'weekly', start_date = '2019-01-01', end_date=f'{date.today()}'):\n",
    "    #TODO: check this function after replacing loop \n",
    "    \n",
    "    start_dt = d.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = d.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    not_keyword_indices = [] #TODO: do this without a loop (create extra boolean column and assign True if keyword is there)\n",
    "    for i, row in enumerate(df_clean['keywordStringsCleanAfterFuzz']):\n",
    "        if keyword not in row:\n",
    "            not_keyword_indices.append(i)\n",
    "\n",
    "    df_clean = df_clean.drop(not_keyword_indices)\n",
    "    df_clean['datetimes']= pd.to_datetime(df_clean['lastModifiedDate'])\n",
    "    df_clean['yearweek'] = df_clean['dt_lastModifiedDate'].apply(lambda x: str(x.strftime(\"%Y\"))+str(x.strftime(\"%W\")))\n",
    "\n",
    "    all_weeks = get_all_weeks(start_dt, end_dt)\n",
    "    not_in_df = list(set(all_weeks) - set(df_clean['yearweek'].tolist()))\n",
    "    dw_mentions = dict(Counter(df_clean['yearweek'].tolist()))\n",
    "    for key_ in not_in_df:\n",
    "        dw_mentions[key_] = 0 \n",
    "    \n",
    "    df_dw_mentions = pd.DataFrame.from_dict(dw_mentions, orient='index', columns=['val'])\n",
    "    # df_dw_mentions['week_str'] = [str(i) for i in df_dw_mentions.index]\n",
    "    # df_dw_mentions = df_dw_mentions.sort_values(by='week_str')\n",
    "    #TODO: can we sort without making a new column etc. this is gonna be computationally expensive\n",
    "    df_dw_mentions.index = df_dw_mentions.index.astype(int)\n",
    "    df_dw_mentions = df_dw_mentions.sort_index()\n",
    "\n",
    "    return df_dw_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          lastModifiedDate  \\\n",
       "0  2019-01-01T03:57:28.904Z  2019-01-01T03:57:28.904Z   \n",
       "1  2019-01-01T06:11:50.527Z  2019-01-01T06:11:50.527Z   \n",
       "2  2019-01-01T06:14:35.563Z  2019-01-01T06:14:35.563Z   \n",
       "3  2019-01-01T08:26:11.599Z  2019-01-01T08:26:11.599Z   \n",
       "4  2019-01-01T09:05:00.736Z  2019-01-01T09:05:00.736Z   \n",
       "\n",
       "                                      keywordStrings  \\\n",
       "0                [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "1  [English Channel, migration, boats, illegal im...   \n",
       "2  [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "3                   [Japan, Tokyo, Harajuku, attack]   \n",
       "4  [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "\n",
       "                        keywordStringsCleanAfterFuzz  \n",
       "0                [nasa, osiris-rex, bennu, asteroid]  \n",
       "1  [english channel, migration, boats, illegal im...  \n",
       "2  [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "3                   [japan, tokyo, harajuku, attack]  \n",
       "4  [asia, bangladesh, elections, kamal hossain, s...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anya_m/Documents/venv/lib64/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  if sys.path[0] == \"\":\n"
     ]
    }
   ],
   "source": [
    "#dw_mentions=get_dw_timeseries(df_clean, keyword, resolution = 'weekly', start = 2019, end = 2023)\n",
    "dw_mentions = get_dw_timeseries(df_clean, keyword, start_date = start_date, end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>donald trump</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-06</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-13</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-27</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-03</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-06</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-20</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            donald trump\n",
       "date                    \n",
       "2019-01-06            16\n",
       "2019-01-13            16\n",
       "2019-01-20            15\n",
       "2019-01-27            12\n",
       "2019-02-03            13\n",
       "...                  ...\n",
       "2020-11-29            15\n",
       "2020-12-06            14\n",
       "2020-12-13            15\n",
       "2020-12-20            15\n",
       "2020-12-27            15\n",
       "\n",
       "[104 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_mentions\n",
    "google_searches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
