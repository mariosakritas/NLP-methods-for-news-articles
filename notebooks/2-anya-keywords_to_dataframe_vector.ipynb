{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put keywords merged by Fuzzy matching into the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n",
    "# from fuzzywuzzy.process import dedupe\n",
    "import functools\n",
    "from rapidfuzz import process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/raw/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the subset of the data for 1 Jan 2019 - 1 Jan 2020 based on lastModifiedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 03:57:28.904000+00:00\n",
      "2022-01-01 02:35:51.098000+00:00\n",
      "60278\n",
      "150367\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by='lastModifiedDate') #sort dataframe\n",
    "\n",
    "datetimes = pd.to_datetime(df['lastModifiedDate'])\n",
    "df['ts_lastModifiedDate']=datetimes\n",
    "#df.iloc[ts_start]['ts_lastModifiedDate']\n",
    "\n",
    "#find start index for subset 2019-2022\n",
    "ts_start=datetimes[(datetimes > pd.Timestamp(year=2019, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2019, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_start)\n",
    "#find end date for subset 2019-2022\n",
    "ts_end=datetimes[(datetimes > pd.Timestamp(year=2022, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2022, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_end)\n",
    "\n",
    "start_date=datetimes[datetimes == ts_start]\n",
    "end_date=datetimes[datetimes == ts_end]\n",
    "\n",
    "#find index for the chosen start and end dates\n",
    "start_index=start_date.index[0]\n",
    "print(start_index)\n",
    "df[df.index == start_date.index[0]]\n",
    "\n",
    "end_index=end_date.index[0]\n",
    "print(end_index)\n",
    "df[df.index == end_date.index[0]]\n",
    "\n",
    "df_subset=df[start_index:end_index]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put output of fuzzyFuzzy into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8000 keywords from the original 10k\n",
    "file='../data/interim/out_dedupl_10k_kw_only_140323_threshold_90.csv'\n",
    "# # #read from csv output of FuzzyWuzzy DEDUP\n",
    "ded_kw=pd.read_csv(file,names = ['ind', 'keyword'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>state repression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>climate consensus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Canary Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Angela I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>8330</td>\n",
       "      <td>Albertville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8331</th>\n",
       "      <td>8331</td>\n",
       "      <td>Asmaa Abdalla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>8332</td>\n",
       "      <td>factory farming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>8333</td>\n",
       "      <td>Deichtorhallen Hamburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>8334</td>\n",
       "      <td>COVIS-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8335 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ind                 keyword\n",
       "0        0                 malware\n",
       "1        1        state repression\n",
       "2        2       climate consensus\n",
       "3        3          Canary Islands\n",
       "4        4                Angela I\n",
       "...    ...                     ...\n",
       "8330  8330             Albertville\n",
       "8331  8331           Asmaa Abdalla\n",
       "8332  8332         factory farming\n",
       "8333  8333  Deichtorhallen Hamburg\n",
       "8334  8334                COVIS-19\n",
       "\n",
       "[8335 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ded_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated=list(ded_kw['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from rapidfuzz import process \n",
    "## speeding up put_clean_kw_into_df\n",
    "# 1) apply to every over each line in dataframe\n",
    "# 2) loop over each keyword in the line\n",
    "# 3) find process.extractOne a substitute from deduplicated list\n",
    "# 4) create a new column in dataframe with merged keywords\n",
    "\n",
    "\n",
    "def put_clean_kw_into_df_fast(row: list) -> list:\n",
    "    #df.applymap(lambda x: len(str(x)))\n",
    "    line_wr = []\n",
    "    #ratio_line_wr = []\n",
    "\n",
    "    for n in range(0, len(row)):\n",
    "        #print(process.extractOne(df_subset['keywordStrings'][i][n],deduplicated)) #print word and ratio \n",
    "        line_wr.append(process.extractOne(row[n], deduplicated)[0])\n",
    "        #ratio_line_wr.append(process.extractOne(row[n], deduplicated)[1])\n",
    "\n",
    "    return line_wr\n",
    "\n",
    "\n",
    "#df_subset['keywordStrings'].iloc.apply(put_clean_kw_into_df_fast)\n",
    "df_subset['keywordStrings'].apply(put_clean_kw_into_df_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lines=put_clean_kw_into_df(df_subset['keywordStrings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NSA', 'OSIRIS-REx', 'Benny Gantz', 'double asteroid'],\n",
       " ['English Channel crossings',\n",
       "  'Remigration',\n",
       "  'ghost boats',\n",
       "  'illegal immigration'],\n",
       " ['Brazil military', 'Jair Bolsanaro', 'Economics', 'AOC', 'Goncalo Guedes'],\n",
       " ['emperor of Japan', 'Paralympics Tokyo 2020', 'Karaj', 'bomb attack'],\n",
       " ['Radio Free Asia',\n",
       "  'Bangaldesh',\n",
       "  'elections',\n",
       "  'Al Hol',\n",
       "  'Sheikh Hasina',\n",
       "  'Major League Baseball',\n",
       "  'IAA'],\n",
       " ['Iranian nuclear program',\n",
       "  'UN sanctions',\n",
       "  'Syrian American Medical Society',\n",
       "  'Iran nuclear deal',\n",
       "  'JCPoA',\n",
       "  'UN sanctions',\n",
       "  'Tehran',\n",
       "  'Rouhani',\n",
       "  'Ayatollah Khamenei']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
