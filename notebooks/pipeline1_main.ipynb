{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1: Google-To-DW pipeline\n",
    "\n",
    "The aim of this notebook is to be able to answer the questions: Is DW covering what customers want\n",
    "\n",
    "Approach: Extract trending topics on Google and compare to what DW covers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../reports/illustrations/pipeline1.png\" width=800 />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried 2 different approaches:\n",
    "\n",
    "**Approach 1**: we used pre-trained models such as Chat GPT and zero-shot learning. \\\n",
    "This approach was overal less effective. Our attempts can be found in pipeline2_playground_approach1_*.ipynb\n",
    "\n",
    "**Approach 2**: we trained our own models \\\n",
    "The most performing models are sumarised here. Our other attempts can be found in pipeline2_playground_approach2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../reports/illustrations/pipeline2_approaches.png\" width=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import functions from source folder\n",
    "sys.path.append('../src/') \n",
    "from data.preprocess_keywords import make_cleaned_keywords_df\n",
    "from data.make_datasets import get_data, get_daily_trending_searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify wanted time range\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2019-02-01'\n",
    "\n",
    "# Where data files will be stored\n",
    "path_to_data_files = '../data/interim/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract trending topics from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts trending topic from Google if the file does not exist, else loads it\n",
    "# If error with the number of requests, change the header in make_datasets.py \n",
    "# (https://stackoverflow.com/questions/50571317/pytrends-the-request-failed-google-returned-a-response-with-code-429#:~:text=I%20am%20trustworthy.-,Solution,Visit%20the%20Google%20Trend%20page%20and%20perform%20a%20search%20for,-a%20trend%3B%20it)\n",
    "\n",
    "google_file = path_to_data_files + start_date + '_' + end_date + '_World_daily_trending_searches.json'\n",
    "\n",
    "if os.path.isfile(google_file) == False:\n",
    "    df_google = get_daily_trending_searches(path_to_data_files, start_date, end_date = end_date)\n",
    "else:\n",
    "    df_google = pd.read_json(google_file, orient ='split', compression = 'infer') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data file in specific date range\n",
    "clean_data_file = '../data/interim/clean_keywords_' + start_date + '_' + end_date + '.json'\n",
    "\n",
    "# Generates the clean data file if it does not exist\n",
    "if os.path.isfile(clean_data_file) == False:\n",
    "\n",
    "    # Path to raw data\n",
    "    data_file = '../data/raw/CMS_2010_to_June_2022_ENGLISH.json'\n",
    "\n",
    "    # Load and extract data within time range\n",
    "    df_subset = get_data(data_file, start_date, end_date)\n",
    "\n",
    "    # Cleans keywords and saves data as a dataframe\n",
    "    make_cleaned_keywords_df(df_subset, start_date, end_date)\n",
    "\n",
    "\n",
    "# Loads the clean data file\n",
    "df_dw = pd.read_json(clean_data_file, orient ='split', compression = 'infer')\n",
    "\n",
    "# Remove rows witn no category\n",
    "df_dw.dropna(subset=['cleanFocusCategory'], inplace = True)\n",
    "df_dw.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models: map google keywords to DW category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>Date</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>cleanFocusParentCategory</th>\n",
       "      <th>cleanFocusCategory</th>\n",
       "      <th>teaser</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46912921</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>Science</td>\n",
       "      <td>Science</td>\n",
       "      <td>The OSIRIS-REx spacecraft had arrived at the l...</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46911356</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>Law and Justice</td>\n",
       "      <td>Law and Justice</td>\n",
       "      <td>The UK is withdrawing patrol ships from overse...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46909694</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Brazil is inaugurating President Jair Bolsonar...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46912694</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>Law and Justice</td>\n",
       "      <td>Crime</td>\n",
       "      <td>A man with an \"intent to murder\" has driven a ...</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46910092</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>In an exclusive interview with DW, Kamal Hossa...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          lastModifiedDate       Date  \\\n",
       "0  46912921  2019-01-01T03:57:28.904Z 2019-01-01   \n",
       "1  46911356  2019-01-01T06:11:50.527Z 2019-01-01   \n",
       "2  46909694  2019-01-01T06:14:35.563Z 2019-01-01   \n",
       "3  46912694  2019-01-01T08:26:11.599Z 2019-01-01   \n",
       "4  46910092  2019-01-01T09:05:00.736Z 2019-01-01   \n",
       "\n",
       "                                      keywordStrings cleanFocusParentCategory  \\\n",
       "0                [NASA, OSIRIS-REx, Bennu, asteroid]                  Science   \n",
       "1  [English Channel, migration, boats, illegal im...          Law and Justice   \n",
       "2  [Brazil, Jair Bolsonaro, Chicago economics, Ha...                 Politics   \n",
       "3                   [Japan, Tokyo, Harajuku, attack]          Law and Justice   \n",
       "4  [Asia, Bangladesh, elections, Kamal Hossain, S...                 Politics   \n",
       "\n",
       "  cleanFocusCategory                                             teaser  \\\n",
       "0            Science  The OSIRIS-REx spacecraft had arrived at the l...   \n",
       "1    Law and Justice  The UK is withdrawing patrol ships from overse...   \n",
       "2           Politics  Brazil is inaugurating President Jair Bolsonar...   \n",
       "3              Crime  A man with an \"intent to murder\" has driven a ...   \n",
       "4           Politics  In an exclusive interview with DW, Kamal Hossa...   \n",
       "\n",
       "                        keywordStringsCleanAfterFuzz  \n",
       "0                [nasa, osiris-rex, bennu, asteroid]  \n",
       "1  [english channel, migration, boats, illegal im...  \n",
       "2  [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "3                   [japan, tokyo, harajuku, attack]  \n",
       "4  [asia, bangladesh, elections, kamal hossain, s...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data from DW\n",
    "df_dw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>formattedValue</th>\n",
       "      <th>link</th>\n",
       "      <th>topic_mid</th>\n",
       "      <th>topic_title</th>\n",
       "      <th>topic_type</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174300</td>\n",
       "      <td>Breakout</td>\n",
       "      <td>/trends/explore?q=/m/02vxn&amp;date=2019-01-02+201...</td>\n",
       "      <td>/m/02vxn</td>\n",
       "      <td>Film</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>39500</td>\n",
       "      <td>Breakout</td>\n",
       "      <td>/trends/explore?q=/m/014dgf&amp;date=2019-01-02+20...</td>\n",
       "      <td>/m/014dgf</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>39700</td>\n",
       "      <td>Breakout</td>\n",
       "      <td>/trends/explore?q=/m/0jg24&amp;date=2019-01-02+201...</td>\n",
       "      <td>/m/0jg24</td>\n",
       "      <td>Image</td>\n",
       "      <td>Topic</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>39750</td>\n",
       "      <td>Breakout</td>\n",
       "      <td>/trends/explore?q=/m/0mgkg&amp;date=2019-01-02+201...</td>\n",
       "      <td>/m/0mgkg</td>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>E-commerce company</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>39900</td>\n",
       "      <td>Breakout</td>\n",
       "      <td>/trends/explore?q=/m/0glpjll&amp;date=2019-01-02+2...</td>\n",
       "      <td>/m/0glpjll</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Social networking service</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     value formattedValue                                               link  \\\n",
       "0   174300       Breakout  /trends/explore?q=/m/02vxn&date=2019-01-02+201...   \n",
       "24   39500       Breakout  /trends/explore?q=/m/014dgf&date=2019-01-02+20...   \n",
       "23   39700       Breakout  /trends/explore?q=/m/0jg24&date=2019-01-02+201...   \n",
       "22   39750       Breakout  /trends/explore?q=/m/0mgkg&date=2019-01-02+201...   \n",
       "21   39900       Breakout  /trends/explore?q=/m/0glpjll&date=2019-01-02+2...   \n",
       "\n",
       "     topic_mid topic_title                 topic_type       date location  \n",
       "0     /m/02vxn        Film                      Topic 2019-01-02    World  \n",
       "24   /m/014dgf       Sales                      Topic 2019-01-02    World  \n",
       "23    /m/0jg24       Image                      Topic 2019-01-02    World  \n",
       "22    /m/0mgkg  Amazon.com         E-commerce company 2019-01-02    World  \n",
       "21  /m/0glpjll   Instagram  Social networking service 2019-01-02    World  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data from Google\n",
    "df_google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load a pre-trained word2vec model from google- you may need to download this first\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "pretrained_w2v_model_dir = '/home/marios/local_data_s2ds/GoogleNews-vectors-negative300.bin'\n",
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format(pretrained_w2v_model_dir, binary=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define corpus and do count vectorization \n",
    "corpus = [l for l in df['keywordStrings'].apply(lambda x: ', '.join(x))]\n",
    "# Count vectorization of text\n",
    "# Creating the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# Converting the text to numeric data\n",
    "X = vectorizer.fit_transform(corpus) \n",
    "# Preparing Data frame For machine learning\n",
    "# Priority column acts as a target variable and other columns as predictors\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "CountVectorizedData['Category']=df_dw['cleanFocusParentCategory'].values\n",
    "print(CountVectorizedData.shape)\n",
    "CountVectorizedData.head()\n",
    "WordsVocab=CountVectorizedData.columns[:-1]\n",
    "print(f'Number of words after count vectorization: {len(WordsVocab)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes aboout an hour to run on my local machine, onnly rerun if you want to overwrite\n",
    "W2Vec_Data=FunctionText2Vec(corpus)\n",
    "output_dir = '/home/marios/local_data_s2ds/'\n",
    "file_name = 'w2v_data_ALL_dirty.npy'\n",
    "np.save(op.join(output_dir, file_name), W2Vec_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you already have the embeddings you can load directly\n",
    "word_embedding_dir = '/home/marios/local_data_s2ds/w2v_data_ALL_dirty.npy'\n",
    "W2Vec_Data = pd.DataFrame(np.load(word_embedding_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the target variable\n",
    "W2Vec_Data.reset_index(inplace=True, drop=True)\n",
    "W2Vec_Data['Category']=CountVectorizedData['Category']\n",
    "DataForML=W2Vec_Data.copy()\n",
    "\n",
    "# making the string categories into integers so we can do ML\n",
    "primary_categories = np.unique(DataForML.Category).tolist()\n",
    "primary_categories = sorted(primary_categories)\n",
    "prim_cat_dict = {}\n",
    "for i, cat in enumerate(primary_categories):\n",
    "    prim_cat_dict[cat] = i+1\n",
    "DataForML.Category = DataForML.Category.apply(lambda x: prim_cat_dict[x] if x in prim_cat_dict else x)\n",
    "DataForML.Category  = DataForML.Category.astype(int)\n",
    "np.unique(DataForML.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable=DataForML.columns[-1]\n",
    "Predictors=DataForML.columns[:-1]\n",
    "X=DataForML[Predictors].values\n",
    "y=DataForML[TargetVariable].values\n",
    "PredictorScaler=MinMaxScaler()\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    "# Generating the standardized values of X\n",
    "X=PredictorScalerFit.transform(X)\n",
    "# Split the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0, stratify=y)\n",
    "# try SVC- fit, evaluate and save -OR load model furtheer down if you have it\n",
    "model = SVC(C=20, kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "evaluate_metrics(y_test, preds)\n",
    "output_dir = '/home/marios/local_data_s2ds/'\n",
    "file_name = 'SVC_model_category_classification.npy'\n",
    "np.save(op.join(output_dir, file_name), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you'd like to optimize hyperparameters- run this over the weekend maybe\n",
    "\n",
    "# params_grid = {\n",
    "#     'C': [25, 50, 150],\n",
    "#     'kernel': ['poly', 'rbf', 'sigmoid']\n",
    "# }\n",
    "# model = SVC()\n",
    "# # Define a GridSearchCV to search the best parameters\n",
    "# grid_search_balanced = GridSearchCV(estimator = model, \n",
    "#                            param_grid = params_grid, \n",
    "#                            scoring='f1',\n",
    "#                            cv = 3, verbose = 1)\n",
    "# # Search the best parameters with training data\n",
    "# model_fit_balanced = grid_search_balanced.fit(X_train, y_train)\n",
    "# best_params_balanced = grid_search_balanced.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model directly if you have it\n",
    "model = np.load('/home/marios/local_data_s2ds/SVC_model_category_classification.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorization of text for google searches extracted\n",
    "# Creating the vectorizer\n",
    "corpus2 = df_google['topic_title'].tolist()\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# Converting the text to numeric data\n",
    "X = vectorizer.fit_transform(corpus2) \n",
    "# Preparing Data frame For machine learning\n",
    "# Priority column acts as a target variable and other columns as predictors\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "print(CountVectorizedData.shape)\n",
    "CountVectorizedData.head()\n",
    "WordsVocab=CountVectorizedData.columns\n",
    "print(f'Number of words after count vectorization: {len(WordsVocab)}')\n",
    "W2Vec_Data2=FunctionText2Vec(corpus2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samee preparation done for training data being done for test data.\n",
    "Predictors=W2Vec_Data2.columns\n",
    "X_google_test=W2Vec_Data2[Predictors].values\n",
    "PredictorScaler=MinMaxScaler()\n",
    "PredictorScalerFit=PredictorScaler.fit(X_google_test)\n",
    "# Generating the standardized values of X\n",
    "X_google_test=PredictorScalerFit.transform(X_google_test)\n",
    "\n",
    "#now let's predict using model \n",
    "preds = model.predict(X_google_test)\n",
    "df_google['predicted_category'] = preds\n",
    "#let's convert predictions from integeres back to the original categories to interpret them \n",
    "prim_num_dict = {}\n",
    "for (num, cat) in zip(prim_cat_dict.values(), prim_cat_dict.keys()):\n",
    "    prim_num_dict[num] = cat\n",
    "prim_num_dict\n",
    "df_google['predicted_category'] = df_google['predicted_category'].apply(lambda x: prim_num_dict[x] if x in prim_num_dict else x)\n",
    "df_google['predicted_category']  = df_google['predicted_category'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's find if each keyword was published on by DW- this takes 10-15 mins \n",
    "df_google['dw_published'] = np.zeros(df_google.shape[0])\n",
    "for i in range(df_google.shape[0]):\n",
    "    #slice original df based on dates\n",
    "    print(i)\n",
    "    start_date = df_google.date.iloc[i]\n",
    "    end_date = df_google.date.iloc[i] + relativedelta(months=1)\n",
    "    df_slice = truncate_data(df, start_date, end_date)\n",
    "    kws_of_slice = [w for sublist in df_slice.keywordStrings for w in sublist]\n",
    "    if str(df_google.topic_title[i]) in kws_of_slice:\n",
    "        print(str(df_google.topic_title[i]))\n",
    "        df_google.dw_published[i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leet's group by eeach category, get the ratio of 1.0's and 0.0's and plot it \n",
    "cats = np.unique(df_google.predicted_category)\n",
    "fig,ax = plt.subplots()\n",
    "for cat in cats:\n",
    "    print(cat)\n",
    "    df_sub = df_google[df_google.predicted_category == cat]\n",
    "    ratio = np.sum(df_sub.dw_published)/df_sub.shape[0]\n",
    "    print(ratio)\n",
    "    ax.bar(cat, ratio)\n",
    "fig.autofmt_xdate(rotation=75)\n",
    "\n",
    "ax.set_ylabel('Proportion of google trends covered \\nby DW within a month after trending')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def truncate_data(df, start_date, end_date): \n",
    "    df['dt_lastModifiedDate'] = df.lastModifiedDate.apply(lambda x: d.datetime.strptime(x[:10], '%Y-%m-%d') if x is not None else x)\n",
    "    df = df.sort_values(by = 'dt_lastModifiedDate')\n",
    "    mask = np.logical_and(df['dt_lastModifiedDate']>=start_date, df['dt_lastModifiedDate']<end_date)\n",
    "    df_subset = df[mask]\n",
    "    return df_subset\n",
    "\n",
    "# this function makes use thee pretrained google model to get word embeddings onn the vectorizeed input \n",
    "def FunctionText2Vec(inpTextData):\n",
    "    # Converting the text to numeric data\n",
    "    X = vectorizer.transform(inpTextData)\n",
    "    CountVecData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVecData.shape[0]):\n",
    "        # initiating a sentence with all zeros\n",
    "        Sentence = np.zeros(300)\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVecData.iloc[i , :]>=1]:\n",
    "            #print(word)\n",
    "            if word in GoogleModel.key_to_index.keys():    \n",
    "                Sentence=Sentence+GoogleModel[word]\n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data=W2Vec_Data.append(pd.DataFrame([Sentence]))\n",
    "    return(W2Vec_Data)\n",
    "\n",
    "def evaluate_metrics(yt, yp):\n",
    "    results_pos = {}\n",
    "    results_pos['accuracy'] = accuracy_score(yt, yp)\n",
    "    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='weighted')\n",
    "    results_pos['recall'] = recall\n",
    "    results_pos['precision'] = precision\n",
    "    results_pos['f1score'] = f_beta\n",
    "    return results_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare trending topics and DW covered categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
