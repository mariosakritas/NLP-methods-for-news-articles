{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to clean the keyword column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import itertools # to flatten lists of lists\n",
    "import collections # to count\n",
    "from rapidfuzz import process as pr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/raw/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "  \n",
    "# returns JSON object as a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "# convert to data frame\n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the subset of the data for 1 Jan 2019 - 1 Jan 2020 based on lastModifiedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 03:57:28.904000+00:00\n",
      "2022-01-01 02:35:51.098000+00:00\n",
      "60278\n",
      "150367\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by='lastModifiedDate') #sort dataframe\n",
    "\n",
    "datetimes = pd.to_datetime(df['lastModifiedDate'])\n",
    "df['ts_lastModifiedDate']=datetimes\n",
    "#df.iloc[ts_start]['ts_lastModifiedDate']\n",
    "\n",
    "#find start index for subset 2019-2022\n",
    "ts_start=datetimes[(datetimes > pd.Timestamp(year=2019, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2019, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_start)\n",
    "\n",
    "#find end date for subset 2019-2022\n",
    "ts_end=datetimes[(datetimes > pd.Timestamp(year=2022, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2022, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_end)\n",
    "\n",
    "start_date=datetimes[datetimes == ts_start]\n",
    "end_date=datetimes[datetimes == ts_end]\n",
    "\n",
    "#find index for the chosen start and end dates\n",
    "start_index=start_date.index[0]\n",
    "print(start_index)\n",
    "df[df.index == start_date.index[0]]\n",
    "\n",
    "end_index=end_date.index[0]\n",
    "print(end_index)\n",
    "df[df.index == end_date.index[0]]\n",
    "\n",
    "df_subset=df[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'shortTitle', 'text', 'teaser', 'shortTeaser', 'kicker',\n",
       "       'regions', 'keywords', 'keywordStrings', 'thematicFocusCategory',\n",
       "       'navigations', 'categories', 'departments', 'firstPublicationDate',\n",
       "       'lastModifiedDate', 'contentDate', 'relatedAutoTopics', 'contentLinks',\n",
       "       'articles', 'isOpinion', 'geographicLocations', 'contentAssociations',\n",
       "       'mainContentImageLink', 'images', 'externalLinks', 'topStory',\n",
       "       'language', 'ts_lastModifiedDate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(df_subset.keywordStrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used mainly for visualisation, get indices of keywords with a certain substring\n",
    "def get_items_with_substring(lst_lst_keywords, substring):\n",
    "    indices = [i for i, lst_kw in enumerate(lst_lst_keywords) if any(list(map(lambda x: substring in x, lst_kw)))]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 1: put everything in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lower = [list(map(str.casefold, x)) for x in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: ['NASA', 'OSIRIS-REx', 'Bennu', 'asteroid']\n",
      "after:  ['nasa', 'osiris-rex', 'bennu', 'asteroid']\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "print('before:', keywords[0])\n",
    "print('after: ', keywords_lower[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 2: split keywords that haven't been split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keywords: kw.split splits the keyword in a list of multiple keywords based on substring, itertools.chain flattens the list of lists\n",
    "keywords_lower_split = [list(itertools.chain(*[kw.split(', ') for kw in lst_kw])) for lst_kw in keywords_lower]\n",
    "keywords_lower_split = [list(itertools.chain(*[kw.split(' - ') for kw in lst_kw])) for lst_kw in keywords_lower_split] # spaces around '-' to not confuse with the ones within words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 13 \n",
      "\n",
      "before: ['freedom of speech, press freedom, freedom of expression']\n",
      "after:  ['freedom of speech', 'press freedom', 'freedom of expression'] \n",
      "\n",
      "before: ['media', 'women', 'gender', 'freedom of speech, press freedom, freedom of expression', 'dw akademie', 'gender parity']\n",
      "after:  ['media', 'women', 'gender', 'freedom of speech', 'press freedom', 'freedom of expression', 'dw akademie', 'gender parity'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_unsplit_keywords = get_items_with_substring(keywords_lower, ', ') + get_items_with_substring(keywords_lower, ' - ')\n",
    "print('Nb of keywords changed:', len(items_with_unsplit_keywords), '\\n')\n",
    "for i in range(2):\n",
    "    print('before:', keywords_lower[items_with_unsplit_keywords[i]])\n",
    "    print('after: ', keywords_lower_split[items_with_unsplit_keywords[i]], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 3: remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '\\u2002' and '.', '\" ', '\"', 'keywords: ' (replace with empty)\n",
    "keywords_lower_split_clean = keywords_lower_split\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\\u2002', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('.', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\" ', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\"', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('keywords: ', ''), lst_kw)) for lst_kw in keywords_lower_split_clean]\n",
    "\n",
    "# Replace '\\xa0' with space\n",
    "keywords_lower_split_clean = [list(map(lambda x: x.replace('\\xa0', ' '), lst_kw)) for lst_kw in keywords_lower_split_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 85 \n",
      "\n",
      "before: ['israel', 'syria', 'iran', 'hezbollah', '\"islamic state\"']\n",
      "after:  ['israel', 'syria', 'iran', 'hezbollah', 'islamic state'] \n",
      "\n",
      "before: ['emmanuel macron', 'letter', 'national debate', '\"yellow vests\"', 'marine le pen']\n",
      "after:  ['emmanuel macron', 'letter', 'national debate', 'yellow vests', 'marine le pen'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_unwanted_characters = get_items_with_substring(keywords_lower, substring = '\"')\n",
    "print('Nb of keywords changed:', len(items_with_unwanted_characters), '\\n')\n",
    "for i in range(2):\n",
    "    print('before:', keywords_lower_split[items_with_unwanted_characters[i]])\n",
    "    print('after: ', keywords_lower_split_clean[items_with_unwanted_characters[i]], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 4: Clean sentences \n",
    "Heuristic: remove keywords that have more than 6 spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spaces = 6 # if there are more spaces than this number, the keyword is removed\n",
    "keywords_lower_split_clean_short = [[kw for kw in lst_kw if kw.count(' ')<n_spaces] for lst_kw in keywords_lower_split_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of keywords changed: 67 \n",
      "\n",
      "before: ['germany', 'gerd müller', 'german federal ministry for economic cooperation and development', 'zambia', 'malawi', 'namibia', 'angela merkel', 'olaf scholz']\n",
      "after:  ['germany', 'gerd müller', 'zambia', 'malawi', 'namibia', 'angela merkel', 'olaf scholz'] \n",
      "\n",
      "before: ['morocco', 'misogyny', 'domestic violence', 'feminism', 'euro-mediterranean foundation of support to human rights defenders']\n",
      "after:  ['morocco', 'misogyny', 'domestic violence', 'feminism'] \n",
      "\n",
      "before: ['adama dieng', 'united nations', 'special advisor on the prevention of genocide', 'genocide', 'south sudan', 'central african republic', 'africa']\n",
      "after:  ['adama dieng', 'united nations', 'genocide', 'south sudan', 'central african republic', 'africa'] \n",
      "\n",
      "before: ['afd', 'office for the protection of the constitution', 'thomas haldenwang']\n",
      "after:  ['afd', 'thomas haldenwang'] \n",
      "\n",
      "before: ['mormon', 'rome', 'church', 'church of jesus christ of latter-day saints']\n",
      "after:  ['mormon', 'rome', 'church'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualisation only (can remove later on)\n",
    "items_with_sentences = [i for i,lst_kw in enumerate(keywords_lower_split_clean) if any(list(map(lambda x: x.count(' ')>=n_spaces, lst_kw)))]\n",
    "print('Nb of keywords changed:', len(items_with_sentences), '\\n')\n",
    "for i in range(5):\n",
    "    print('before:', keywords_lower_split_clean[items_with_sentences[i]])\n",
    "    print('after: ', keywords_lower_split_clean_short[items_with_sentences[i]], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords before cleaning: 32682\n",
      "Number of unique keywords before cleaning: 30243\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords)))))\n",
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords_lower_split_clean_short)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NASA', 'OSIRIS-REx', 'Bennu', 'asteroid'], ['English Channel', 'migration', 'boats', 'illegal immigration']]\n",
      "[['nasa', 'osiris-rex', 'bennu', 'asteroid'], ['english channel', 'migration', 'boats', 'illegal immigration']]\n"
     ]
    }
   ],
   "source": [
    "#print(keywords[0:2])\n",
    "#print(keywords_lower_split_clean_short[0:2])\n",
    "\n",
    "#df_2019_2020 = df_subset.copy()\n",
    "#df_2019_2020['keywordStringsClean'] = keywords_lower_split_clean_short\n",
    "\n",
    "#filepath = '../data/interim/clean_keywords_2019-2021_before_FuzzyWuzzy.csv'\n",
    "#df_2019_2020.to_csv(filepath, index=False)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count occurence of each keyword\n",
    "Will be used to know which one to keep in fuzzy wuzzy (the most used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat = list(itertools.chain(*keywords_lower_split_clean_short)) # Flatten list\n",
    "keywords_freq = collections.Counter(keywords_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>3817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germany</th>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19</th>\n",
       "      <td>2641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>china</th>\n",
       "      <td>1669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russia</th>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donald trump</th>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundesliga</th>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              freq\n",
       "coronavirus   3817\n",
       "germany       3299\n",
       "covid-19      2641\n",
       "china         1669\n",
       "russia        1354\n",
       "donald trump  1333\n",
       "asia          1260\n",
       "us            1087\n",
       "eu            1078\n",
       "bundesliga     905"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For visualisation, can be removed\n",
    "keywords_freq_df = pd.DataFrame.from_dict(keywords_freq, orient='index', columns = ['freq'])\n",
    "keywords_freq_df.sort_values(by='freq', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid Fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique ones and remove the empty entry\n",
    "unique_keywords = list(set(keywords_flat))\n",
    "unique_keywords.remove('')\n",
    "\n",
    "# run rapid fuzz\n",
    "ratio_array= pr.cdist(unique_keywords, unique_keywords, score_cutoff = 90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find words correlating together and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = pd.DataFrame(ratio_array, columns = unique_keywords)\n",
    "\n",
    "# Count number of non zero values in each row\n",
    "nb_non_zero = np.count_nonzero(np.asarray(ratio_array), axis=1) \n",
    "\n",
    "# Save indices of rows with more than 1 non-zero value\n",
    "indices_correlating_rows = [i for i, el in enumerate(list(nb_non_zero)) if el>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of similar keywords\n",
    "all_similar_kw = []\n",
    "for i in indices_correlating_rows:\n",
    "    similar_words = [keyword for val, keyword in zip(list(df_array.iloc[i]), unique_keywords) if val!=0]\n",
    "    all_similar_kw.append(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of similar keywords\n",
    "all_similar_kw = []\n",
    "for i in range(0,40):\n",
    "    similar_words = [keyword for val, keyword in zip(list(df_array.iloc[indices_correlating_rows[i]]), unique_keywords) if val!=0]\n",
    "    all_similar_kw.append(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify similar keywords\n",
    "Replace by most frequent one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs for 14min !!\n",
    "# Split in the ones which have equal word number, and the ones which don'try\n",
    "# TODO: Problem: UN climate summit, US department of justice: only change if same number of words?\n",
    "\n",
    "similar_kws_same_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))==1]\n",
    "similar_kws_diff_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))!=1]\n",
    "\n",
    "# Replace\n",
    "keywords_replaced = keywords_lower_split_clean_short\n",
    "\n",
    "for sim_kws in similar_kws_same_word_nb:\n",
    "    \n",
    "    # Make list of frequencies for those similar kws\n",
    "    sim_kws_freq = [keywords_freq[word] for word in sim_kws]\n",
    "    \n",
    "    for kw in sim_kws:\n",
    "\n",
    "        # the new keyword is the one with the highest frequency\n",
    "        right_kw = sim_kws[sim_kws_freq.index(max(sim_kws_freq))]\n",
    "\n",
    "        # replace similar keywords by the most frequent one\n",
    "        keywords_replaced = [list(map(lambda x: right_kw if x==kw else x, lst_kw)) for lst_kw in keywords_replaced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq of: sausage  =  7\n",
      "freq of: sausages  =  3\n",
      "\n",
      "freq of: minumum wage  =  1\n",
      "freq of: minimum wage  =  10\n",
      "\n",
      "kw_before: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khamenei']\n",
      "kw_after: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khomeini'] \n",
      "\n",
      "kw_before: [\"new year's\", 'countdown', 'resolutions']\n",
      "kw_after: [\"new year's\", 'countdown', 'solutions'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example for visualisation, can be removed later\n",
    "for sim_kws in all_similar_kw[0:2]:\n",
    "    for word in sim_kws:\n",
    "        print('freq of:', word, ' = ', keywords_freq[word])\n",
    "    print('')\n",
    "\n",
    "n_show = 2 # how many examples to show\n",
    "\n",
    "i_show = 0\n",
    "for kw_before, kw_after in zip(keywords_lower_split_clean_short, keywords_replaced):\n",
    "    if kw_before != kw_after and i_show < n_show:\n",
    "        print('kw_before:', kw_before)\n",
    "        print('kw_after:', kw_after, '\\n')\n",
    "        i_show += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords before cleaning: 32682\n",
      "Number of unique keywords after first clean: 30243\n",
      "Number of unique keywords after fuzzywuzzy replacing: 27962\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords)))))\n",
    "print('Number of unique keywords after first clean:', len(set(list(itertools.chain(*keywords_lower_split_clean_short)))))\n",
    "print('Number of unique keywords after fuzzywuzzy replacing:', len(set(list(itertools.chain(*keywords_replaced)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lastModifiedDate  \\\n",
       "0  2019-01-01T03:57:28.904Z   \n",
       "1  2019-01-01T06:11:50.527Z   \n",
       "2  2019-01-01T06:14:35.563Z   \n",
       "3  2019-01-01T08:26:11.599Z   \n",
       "4  2019-01-01T09:05:00.736Z   \n",
       "\n",
       "                                      keywordStrings  \\\n",
       "0                [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "1  [English Channel, migration, boats, illegal im...   \n",
       "2  [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "3                   [Japan, Tokyo, Harajuku, attack]   \n",
       "4  [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "\n",
       "                        keywordStringsCleanAfterFuzz  \n",
       "0                [nasa, osiris-rex, bennu, asteroid]  \n",
       "1  [english channel, migration, boats, illegal im...  \n",
       "2  [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "3                   [japan, tokyo, harajuku, attack]  \n",
       "4  [asia, bangladesh, elections, kamal hossain, s...  "
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dates = list(df_subset['lastModifiedDate'])\n",
    "list_kws = list(df_subset['keywordStrings'])\n",
    "list_new_kws = keywords_replaced\n",
    "\n",
    "df_2019_2020 = pd.DataFrame(list(zip(list_dates, list_kws, list_new_kws)), columns=['lastModifiedDate', 'keywordStrings', 'keywordStringsCleanAfterFuzz'])\n",
    "\n",
    "df_2019_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/interim/clean_keywords_2019-2021_after_FuzzyWuzzy.json'\n",
    "\n",
    "# storing the data in JSON format\n",
    "df_2019_2020.to_json(filepath, orient = 'split', compression = 'infer', index = 'true')\n",
    " \n",
    "# reading the JSON file\n",
    "# df_loaded = pd.read_json(filepath, orient ='split', compression = 'infer')\n",
    "# flat_keywords = list(itertools.chain(*list(df['keywordStringsCleanAfterFuzz'])))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
