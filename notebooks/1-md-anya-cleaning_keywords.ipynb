{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to clean the keyword column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import itertools # to flatten lists of lists\n",
    "import collections # to count\n",
    "from rapidfuzz import process as pr\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from data.preprocess_keywords import clean_keywords, get_items_with_substring\n",
    "from data.make_datasets import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs for 30s\n",
    "# Load and extract data within time range\n",
    "data_file = '../data/raw/CMS_2010_to_June_2022_ENGLISH.json'\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2022-01-01'\n",
    "\n",
    "df_subset = get_data(data_file, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_unique_kws(lst_lst_keywords):\n",
    "    return len(set(list(itertools.chain(*lst_lst_keywords))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DONE. Number of unique keywords went from 32682 to 30228\n"
     ]
    }
   ],
   "source": [
    "lst_lst_keywords = list(df_subset.keywordStrings)\n",
    "lst_lst_keywords_clean = clean_keywords(lst_lst_keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count occurence of each keyword\n",
    "Will be used to know which one to keep in fuzzy wuzzy (the most used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat = list(chain(*lst_lst_keywords_clean)) # Flatten list\n",
    "keywords_freq = collections.Counter(keywords_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>3817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germany</th>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19</th>\n",
       "      <td>2641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>china</th>\n",
       "      <td>1669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russia</th>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donald trump</th>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundesliga</th>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              freq\n",
       "coronavirus   3817\n",
       "germany       3299\n",
       "covid-19      2641\n",
       "china         1669\n",
       "russia        1354\n",
       "donald trump  1333\n",
       "asia          1260\n",
       "us            1087\n",
       "eu            1078\n",
       "bundesliga     905"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For visualisation, can be removed\n",
    "keywords_freq_df = pd.DataFrame.from_dict(keywords_freq, orient='index', columns = ['freq'])\n",
    "keywords_freq_df.sort_values(by='freq', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid Fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Runs for 2min !\n",
    "# extract unique ones and remove the empty entry\n",
    "unique_keywords = list(set(keywords_flat))\n",
    "unique_keywords.remove('')\n",
    "\n",
    "# run rapid fuzz\n",
    "ratio_array= pr.cdist(unique_keywords, unique_keywords, score_cutoff = 90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find words correlating together and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>russian spy</th>\n",
       "      <th>butterfly</th>\n",
       "      <th>taybeh brewing company</th>\n",
       "      <th>robot kitchen</th>\n",
       "      <th>afghansitan</th>\n",
       "      <th>march</th>\n",
       "      <th>hedgehogs</th>\n",
       "      <th>school</th>\n",
       "      <th>greenfield sites</th>\n",
       "      <th>crisis</th>\n",
       "      <th>...</th>\n",
       "      <th>meghan markle</th>\n",
       "      <th>left and right-wing extremism</th>\n",
       "      <th>shutdown</th>\n",
       "      <th>uganda election</th>\n",
       "      <th>chlorpyrifos</th>\n",
       "      <th>mett</th>\n",
       "      <th>new zealand volcano</th>\n",
       "      <th>alexander lukasheko</th>\n",
       "      <th>missing</th>\n",
       "      <th>rectal probes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   russian spy  butterfly  taybeh brewing company  robot kitchen  afghansitan  \\\n",
       "0        100.0        0.0                     0.0            0.0          0.0   \n",
       "1          0.0      100.0                     0.0            0.0          0.0   \n",
       "2          0.0        0.0                   100.0            0.0          0.0   \n",
       "3          0.0        0.0                     0.0          100.0          0.0   \n",
       "4          0.0        0.0                     0.0            0.0        100.0   \n",
       "\n",
       "   march  hedgehogs  school  greenfield sites  crisis  ...  meghan markle  \\\n",
       "0    0.0        0.0     0.0               0.0     0.0  ...            0.0   \n",
       "1    0.0        0.0     0.0               0.0     0.0  ...            0.0   \n",
       "2    0.0        0.0     0.0               0.0     0.0  ...            0.0   \n",
       "3    0.0        0.0     0.0               0.0     0.0  ...            0.0   \n",
       "4    0.0        0.0     0.0               0.0     0.0  ...            0.0   \n",
       "\n",
       "   left and right-wing extremism  shutdown  uganda election  chlorpyrifos  \\\n",
       "0                            0.0       0.0              0.0           0.0   \n",
       "1                            0.0       0.0              0.0           0.0   \n",
       "2                            0.0       0.0              0.0           0.0   \n",
       "3                            0.0       0.0              0.0           0.0   \n",
       "4                            0.0       0.0              0.0           0.0   \n",
       "\n",
       "   mett  new zealand volcano  alexander lukasheko  missing  rectal probes  \n",
       "0   0.0                  0.0                  0.0      0.0            0.0  \n",
       "1   0.0                  0.0                  0.0      0.0            0.0  \n",
       "2   0.0                  0.0                  0.0      0.0            0.0  \n",
       "3   0.0                  0.0                  0.0      0.0            0.0  \n",
       "4   0.0                  0.0                  0.0      0.0            0.0  \n",
       "\n",
       "[5 rows x 30227 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_array = pd.DataFrame(ratio_array, columns = unique_keywords)\n",
    "\n",
    "# Count number of non zero values in each row\n",
    "nb_non_zero = np.count_nonzero(np.asarray(ratio_array), axis=1) \n",
    "\n",
    "# Save indices of rows with more than 1 non-zero value\n",
    "indices_correlating_rows = [i for i, el in enumerate(list(nb_non_zero)) if el>1]\n",
    "\n",
    "df_array.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Runs for 1min !\n",
    "# Make a list of similar keywords\n",
    "all_similar_kw = []\n",
    "for i in indices_correlating_rows:\n",
    "    \n",
    "    similar_words = [keyword for val, keyword in zip(list(df_array.iloc[i]), unique_keywords) if val!=0]\n",
    "    \n",
    "    # Only adds it if it's not there already\n",
    "    if similar_words not in all_similar_kw:\n",
    "        all_similar_kw.append(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['european fires', 'european firms']\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "items, _, _ = get_items_with_substring(all_similar_kw, 'european fi')\n",
    "for i in items:\n",
    "    print(all_similar_kw[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sausage', 'sausages'],\n",
       " ['minumum wage', 'minimum wage'],\n",
       " ['un climate summit', 'climate summit'],\n",
       " ['sex abuse scandals', 'sexual abuse scandals', 'sex abuse scandal'],\n",
       " ['champiosn league',\n",
       "  'champions leage',\n",
       "  'champions league',\n",
       "  'champion league']]"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_similar_kw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = [keyword.split() for keyword in df.firstKeyword] # tokenise: split into words\n",
    "#model_ft = FastText(tokens, vector_size = 500) # model_ft.wv['pollution']: get word vector for a specific word; model_ft.wv.similar_by_word('pollution', topn=5): Look for the 5 most similar words\n",
    "\n",
    "#word_vectors = [np.array(model_ft.wv[token]) for token in tokens]\n",
    "#word_vectors_av = np.array([np.average(word_vector, axis=0) for word_vector in word_vectors]) # if: ['dw','akademie'] = 2 vectors = we average them\n",
    "\n",
    "#features = word_vectors_av\n",
    "#features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords = ['ayatollah khomeini', 'ayatollah khameini', 'ayatollah khomenei', 'ayatollah ali khameini']\n",
    "# rapid fuzz, keeps track of similarities above 90\n",
    "#similarity_matrix= pr.cdist(keywords, keywords, score_cutoff = 90) \n",
    "#similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Problem, should be considered as the same keyword and not different ones..\n",
    "## In the line above, add a new word to a sublist instead of making a new list of some elements match ?\n",
    "#items = get_items_with_substring(all_similar_kw, 'ayatollah')\n",
    "#print('Eeach new line is considered as a single keyword:')\n",
    "#for i in items:\n",
    "#    print(all_similar_kw[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify similar keywords\n",
    "Replace by most frequent one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in the ones which have equal word number, and the ones which don't, becase they need different processing\n",
    "similar_kws_same_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))==1]\n",
    "similar_kws_diff_word_nb = [sim_kws for sim_kws in all_similar_kw if len(set([kw.count(' ') for kw in sim_kws]))!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['european fires', 'european firms']\n"
     ]
    }
   ],
   "source": [
    "#### TEST\n",
    "\n",
    "# TODO: problem: ['european fires', 'european firms']: word embedings\n",
    "# TODO: problem: [\"new year's\", 'countdown', 'resolutions'] replaced by [\"new year's\", 'countdown', 'solutions']\n",
    "\n",
    "item = get_items_with_substring(similar_kws_same_word_nb, 'european firms')\n",
    "for i in item:\n",
    "    print(similar_kws_same_word_nb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Focus on similar_kws_diff_word_nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Runs for 1min50 !\n",
    "\n",
    "## Focus on similar_kws_same_word_nb\n",
    "\n",
    "# TODO: problem: ['european fires', 'european firms']: word embedings\n",
    "# TODO: problem: [\"new year's\", 'countdown', 'resolutions'] replaced by [\"new year's\", 'countdown', 'solutions']\n",
    "\n",
    "# Replace\n",
    "right_kw = [sim_kws[np.argmax([keywords_freq[word] for word in sim_kws])] for sim_kws in similar_kws_same_word_nb]\n",
    "\n",
    "keywords_flat = list(chain.from_iterable(keywords_lower_split_clean_short))\n",
    "\n",
    "replacement_only = [[right_kw[i] for i, j in enumerate(similar_kws_same_word_nb) if word in j] for word in keywords_flat]\n",
    "\n",
    "keywords_flat_post = [replacement_only[i][0] if replacement_only[i] != [] else keywords_flat[i] for i in range(len(keywords_flat))]\n",
    "\n",
    "def gen_list_of_lists(original_list, new_structure):\n",
    "    assert len(original_list) == sum(new_structure), \\\n",
    "    \"The number of elements in the original list and desired structure don't match\"\n",
    "    list_of_lists = [[original_list[i + sum(new_structure[:j])] for i in range(new_structure[j])] \\\n",
    "                     for j in range(len(new_structure))]\n",
    "    return list_of_lists\n",
    "\n",
    "keywords_replaced = gen_list_of_lists(keywords_flat_post, [len(x) for x in keywords_lower_split_clean_short])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq of: sausage  =  7\n",
      "freq of: sausages  =  3\n",
      "\n",
      "freq of: minumum wage  =  1\n",
      "freq of: minimum wage  =  10\n",
      "\n",
      "kw_before: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khamenei']\n",
      "kw_after: ['iran', 'sanctions', 'civil society', 'nuclear deal', 'jcpoa', 'us sanctions', 'tehran', 'hassan rouhani', 'ayatollah khomeini'] \n",
      "\n",
      "kw_before: [\"new year's\", 'countdown', 'resolutions']\n",
      "kw_after: [\"new year's\", 'countdown', 'solutions'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example for visualisation, can be removed later\n",
    "for sim_kws in all_similar_kw[0:2]:\n",
    "    for word in sim_kws:\n",
    "        print('freq of:', word, ' = ', keywords_freq[word])\n",
    "    print('')\n",
    "\n",
    "n_show = 2 # how many examples to show\n",
    "\n",
    "i_show = 0\n",
    "for kw_before, kw_after in zip(keywords_lower_split_clean_short, keywords_replaced):\n",
    "    if kw_before != kw_after and i_show < n_show:\n",
    "        print('kw_before:', kw_before)\n",
    "        print('kw_after:', kw_after, '\\n')\n",
    "        i_show += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique keywords before cleaning: 32682\n",
      "Number of unique keywords after first clean: 30228\n",
      "Number of unique keywords after rapidfuzz replacing: 27988\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique keywords before cleaning:', len(set(list(itertools.chain(*keywords)))))\n",
    "print('Number of unique keywords after first clean:', len(set(list(itertools.chain(*keywords_lower_split_clean_short)))))\n",
    "print('Number of unique keywords after rapidfuzz replacing:', len(set(list(itertools.chain(*keywords_replaced)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lastModifiedDate</th>\n",
       "      <th>keywordStrings</th>\n",
       "      <th>keywordStringsCleanAfterFuzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>2019-01-01T03:57:28.904Z</td>\n",
       "      <td>[NASA, OSIRIS-REx, Bennu, asteroid]</td>\n",
       "      <td>[nasa, osiris-rex, bennu, asteroid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>2019-01-01T06:11:50.527Z</td>\n",
       "      <td>[English Channel, migration, boats, illegal im...</td>\n",
       "      <td>[english channel, migration, boats, illegal im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>2019-01-01T06:14:35.563Z</td>\n",
       "      <td>[Brazil, Jair Bolsonaro, Chicago economics, Ha...</td>\n",
       "      <td>[brazil, jair bolsonaro, chicago economics, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>2019-01-01T08:26:11.599Z</td>\n",
       "      <td>[Japan, Tokyo, Harajuku, attack]</td>\n",
       "      <td>[japan, tokyo, harajuku, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>2019-01-01T09:05:00.736Z</td>\n",
       "      <td>[Asia, Bangladesh, elections, Kamal Hossain, S...</td>\n",
       "      <td>[asia, bangladesh, elections, kamal hossain, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          lastModifiedDate  \\\n",
       "0  2019-01-01T03:57:28.904Z  2019-01-01T03:57:28.904Z   \n",
       "1  2019-01-01T06:11:50.527Z  2019-01-01T06:11:50.527Z   \n",
       "2  2019-01-01T06:14:35.563Z  2019-01-01T06:14:35.563Z   \n",
       "3  2019-01-01T08:26:11.599Z  2019-01-01T08:26:11.599Z   \n",
       "4  2019-01-01T09:05:00.736Z  2019-01-01T09:05:00.736Z   \n",
       "\n",
       "                                      keywordStrings  \\\n",
       "0                [NASA, OSIRIS-REx, Bennu, asteroid]   \n",
       "1  [English Channel, migration, boats, illegal im...   \n",
       "2  [Brazil, Jair Bolsonaro, Chicago economics, Ha...   \n",
       "3                   [Japan, Tokyo, Harajuku, attack]   \n",
       "4  [Asia, Bangladesh, elections, Kamal Hossain, S...   \n",
       "\n",
       "                        keywordStringsCleanAfterFuzz  \n",
       "0                [nasa, osiris-rex, bennu, asteroid]  \n",
       "1  [english channel, migration, boats, illegal im...  \n",
       "2  [brazil, jair bolsonaro, chicago economics, ha...  \n",
       "3                   [japan, tokyo, harajuku, attack]  \n",
       "4  [asia, bangladesh, elections, kamal hossain, s...  "
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ids = list(df_subset['lastModifiedDate'])\n",
    "list_dates = list(df_subset['lastModifiedDate'])\n",
    "list_kws = list(df_subset['keywordStrings'])\n",
    "list_new_kws = keywords_replaced\n",
    "\n",
    "df_2019_2020 = pd.DataFrame(list(zip(list_ids, list_dates, list_kws, list_new_kws)), columns=['id', 'lastModifiedDate', 'keywordStrings', 'keywordStringsCleanAfterFuzz'])\n",
    "\n",
    "df_2019_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/interim/clean_keywords_2019-2021_after_RapidFuzz.json'\n",
    "\n",
    "# storing the data in JSON format\n",
    "df_2019_2020.to_json(filepath, orient = 'split', compression = 'infer', index = 'true')\n",
    " \n",
    "# reading the JSON file\n",
    "# filepath = '../data/interim/clean_keywords_2019-2021_after_FuzzyWuzzy.json'\n",
    "# df_loaded = pd.read_json(filepath, orient ='split', compression = 'infer')\n",
    "# flat_keywords = list(itertools.chain(*list(df['keywordStringsCleanAfterFuzz'])))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
