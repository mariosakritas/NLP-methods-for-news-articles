{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team DatenWelle\n",
    "\n",
    "## Keyword merging with FuzzyWuzzy\n",
    "\n",
    "This notebook loads the data from JSON format and performs some keyword cleaning and merging misspelled duplicates with fuzzyWuuzy package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git pull\n",
    "#!git status\n",
    "#!git add 1-anya_exploratory_analysis.ipynb\n",
    "#!git commit -m 'made a set of keywords 2019-2022 and tried fuzzuwuzzy dedupe(licate) function on 10000 keywords'\n",
    "#!git push\n",
    "#!pip install -r ../requirements.txt\n",
    "\n",
    "#after installed new libraries\n",
    "#!pip freeze > requirements.txt\n",
    "#!git add requirements.txt \n",
    "#!git add out_dedupl_100323.csv test.csv\n",
    "#!git commit -m 'added output files f fuzzy wuzzy dedupe'\n",
    "#!git commit -m 'added library fuzzywuzzy'\n",
    "#!git push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy.process import dedupe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/raw/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the subset of the data for 1 Jan 2019 - 1 Jan 2020 based on lastModifiedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 03:57:28.904000+00:00\n",
      "2022-01-01 02:35:51.098000+00:00\n",
      "60278\n",
      "150367\n"
     ]
    }
   ],
   "source": [
    "datetimes = pd.to_datetime(df['lastModifiedDate'])\n",
    "df['ts_lastModifiedDate']=datetimes\n",
    "#df.iloc[ts_start]['ts_lastModifiedDate']\n",
    "\n",
    "\n",
    "#find start index for subset 2019-2022\n",
    "ts_start=datetimes[(datetimes > pd.Timestamp(year=2019, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2019, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_start)\n",
    "#find end date for subset 2019-2022\n",
    "ts_end=datetimes[(datetimes > pd.Timestamp(year=2022, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2022, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_end)\n",
    "\n",
    "start_date=datetimes[datetimes == ts_start]\n",
    "end_date=datetimes[datetimes == ts_end]\n",
    "\n",
    "#find index for the chosen start and end dates\n",
    "start_index=start_date.index[0]\n",
    "print(start_index)\n",
    "df[df.index == start_date.index[0]]\n",
    "\n",
    "end_index=end_date.index[0]\n",
    "print(end_index)\n",
    "df[df.index == end_date.index[0]]\n",
    "\n",
    "df_subset=df[start_index:end_index]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [{'name': 'DRC'}, {'name': 'M23'}, {'name': 'F...\n",
       "1         [{'name': 'telephone'}, {'name': 'hotline'}, {...\n",
       "2         [{'name': 'fiscal cliff'}, {'name': 'Obama'}, ...\n",
       "3         [{'name': 'Kim Jong Un'}, {'name': 'Kim Jong I...\n",
       "4         [{'name': 'fiscal cliff'}, {'name': 'US Senate...\n",
       "                                ...                        \n",
       "175654    [{'name': 'Turkey'}, {'name': 'Recep Tayipp Er...\n",
       "175655    [{'name': 'pollution'}, {'name': 'gold mine'},...\n",
       "175656    [{'name': 'war'}, {'name': 'Ukraine'}, {'name'...\n",
       "175657    [{'name': 'France'}, {'name': 'vegetarian'}, {...\n",
       "175658          [{'name': 'Ecuador'}, {'name': 'protests'}]\n",
       "Name: keywords, Length: 175659, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords'] # is keywords in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         {FDLR, Rwanda, Susan Rice, UN security council...\n",
       "1         {federal government, telephone, Catholic Churc...\n",
       "2         {spending cuts, fiscal cliff, debt ceiling, Ob...\n",
       "3         {pyongyang, south korea, Kim Jong Il, Seoul, l...\n",
       "4         {fiscal cliff bill, recession, fiscal cliff, S...\n",
       "                                ...                        \n",
       "175654    {Finland Sweden, NATO, Turkey, Recep Tayipp Er...\n",
       "175655     {Anagold, cyanide, Turkey, pollution, gold mine}\n",
       "175656    {call-up, draft, Russian attack, Russia, speci...\n",
       "175657                 {France, vegetarian, steak, sausage}\n",
       "175658                                  {protests, Ecuador}\n",
       "Name: keywords, Length: 175659, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create series of keywords sets\n",
    "def get_keywords(row):\n",
    "    if row is None:\n",
    "        return None\n",
    "    else:\n",
    "        res_set = set()\n",
    "        for name_dict in row:\n",
    "            res_set.add(name_dict['name'])\n",
    "        return res_set\n",
    "\n",
    "df['keywords'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract individual keywords from the sets of sets\n",
    "\n",
    "# should work but it is very slow for now with current gpus\n",
    "# 10000 articles in 7 seconds\n",
    "# df_subset (90090 articles) runs in 10 minutes 10 seconds\n",
    "\n",
    "# sets=df_subset['keywords'].apply(get_keyword1) #full dataset\n",
    "sets=df_subset['keywords'].apply(get_keywords)  #2019-2021 subset\n",
    "#sets=sets[0:10000] #10000 articles\n",
    " \n",
    "kw=functools.reduce(set.union, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90090/90090 [11:16<00:00, 133.25it/s] \n"
     ]
    }
   ],
   "source": [
    "# # another way to extract individual keywords from the sets of sets that doesn't crash kernel is interrupted\n",
    "# # so it might be more stable when later applied to the entire dataset\n",
    "\n",
    "# #runs for 10000 articles in 5 seconds\n",
    "# #runs for df_subset in  11 min 16 sec \n",
    "\n",
    "# sets=df_subset['keywords'].apply(get_keywords)\n",
    "# #sets=sets[0:10000]\n",
    "# from tqdm import tqdm\n",
    "# def get_unique_keywords(sets):\n",
    "#     result_set = set()\n",
    "#     for row_set in tqdm(sets.values):\n",
    "#         #result_set.union(row_set)\n",
    "#         result_set = result_set.union(row_set)\n",
    "#     return result_set\n",
    "\n",
    "# unique_keywords = get_unique_keywords(sets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the set of DW keywords before fuzzyWuzzy into the file\n",
    "pd.Series(list(unique_keywords)).to_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101171"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets_10000=sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with FuzzyWuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function from tutorial to get simplest matching ratio\n",
    "\n",
    "# Str1 = \"Apple Inc.\"\n",
    "# Str2 = \"apple Inc\"\n",
    "# Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "# print(Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fuzzywuzzy.process in fuzzywuzzy:\n",
      "\n",
      "NAME\n",
      "    fuzzywuzzy.process - # encoding: utf-8\n",
      "\n",
      "FUNCTIONS\n",
      "    dedupe(contains_dupes, threshold=70, scorer=<function token_set_ratio at 0x7f06af223e60>)\n",
      "        This convenience function takes a list of strings containing duplicates and uses fuzzy matching to identify\n",
      "        and remove duplicates. Specifically, it uses the process.extract to identify duplicates that\n",
      "        score greater than a user defined threshold. Then, it looks for the longest item in the duplicate list\n",
      "        since we assume this item contains the most entity information and returns that. It breaks string\n",
      "        length ties on an alphabetical sort.\n",
      "        \n",
      "        Note: as the threshold DECREASES the number of duplicates that are found INCREASES. This means that the\n",
      "            returned deduplicated list will likely be shorter. Raise the threshold for fuzzy_dedupe to be less\n",
      "            sensitive.\n",
      "        \n",
      "        Args:\n",
      "            contains_dupes: A list of strings that we would like to dedupe.\n",
      "            threshold: the numerical value (0,100) point at which we expect to find duplicates.\n",
      "                Defaults to 70 out of 100\n",
      "            scorer: Optional function for scoring matches between the query and\n",
      "                an individual processed choice. This should be a function\n",
      "                of the form f(query, choice) -> int.\n",
      "                By default, fuzz.token_set_ratio() is used and expects both query and\n",
      "                choice to be strings.\n",
      "        \n",
      "        Returns:\n",
      "            A deduplicated list. For example:\n",
      "        \n",
      "                In: contains_dupes = ['Frodo Baggin', 'Frodo Baggins', 'F. Baggins', 'Samwise G.', 'Gandalf', 'Bilbo Baggins']\n",
      "                In: fuzzy_dedupe(contains_dupes)\n",
      "                Out: ['Frodo Baggins', 'Samwise G.', 'Bilbo Baggins', 'Gandalf']\n",
      "    \n",
      "    extract(query, choices, processor=<function full_process at 0x7f06af223710>, scorer=<function WRatio at 0x7f06af2250e0>, limit=5)\n",
      "        Select the best match in a list or dictionary of choices.\n",
      "        \n",
      "        Find best matches in a list or dictionary of choices, return a\n",
      "        list of tuples containing the match and its score. If a dictionary\n",
      "        is used, also returns the key for each match.\n",
      "        \n",
      "        Arguments:\n",
      "            query: An object representing the thing we want to find.\n",
      "            choices: An iterable or dictionary-like object containing choices\n",
      "                to be matched against the query. Dictionary arguments of\n",
      "                {key: value} pairs will attempt to match the query against\n",
      "                each value.\n",
      "            processor: Optional function of the form f(a) -> b, where a is the query or\n",
      "                individual choice and b is the choice to be used in matching.\n",
      "        \n",
      "                This can be used to match against, say, the first element of\n",
      "                a list:\n",
      "        \n",
      "                lambda x: x[0]\n",
      "        \n",
      "                Defaults to fuzzywuzzy.utils.full_process().\n",
      "            scorer: Optional function for scoring matches between the query and\n",
      "                an individual processed choice. This should be a function\n",
      "                of the form f(query, choice) -> int.\n",
      "                By default, fuzz.WRatio() is used and expects both query and\n",
      "                choice to be strings.\n",
      "            limit: Optional maximum for the number of elements returned. Defaults\n",
      "                to 5.\n",
      "        \n",
      "        Returns:\n",
      "            List of tuples containing the match and its score.\n",
      "        \n",
      "            If a list is used for choices, then the result will be 2-tuples.\n",
      "            If a dictionary is used, then the result will be 3-tuples containing\n",
      "            the key for each match.\n",
      "        \n",
      "            For example, searching for 'bird' in the dictionary\n",
      "        \n",
      "            {'bard': 'train', 'dog': 'man'}\n",
      "        \n",
      "            may return\n",
      "        \n",
      "            [('train', 22, 'bard'), ('man', 0, 'dog')]\n",
      "    \n",
      "    extractBests(query, choices, processor=<function full_process at 0x7f06af223710>, scorer=<function WRatio at 0x7f06af2250e0>, score_cutoff=0, limit=5)\n",
      "        Get a list of the best matches to a collection of choices.\n",
      "        \n",
      "        Convenience function for getting the choices with best scores.\n",
      "        \n",
      "        Args:\n",
      "            query: A string to match against\n",
      "            choices: A list or dictionary of choices, suitable for use with\n",
      "                extract().\n",
      "            processor: Optional function for transforming choices before matching.\n",
      "                See extract().\n",
      "            scorer: Scoring function for extract().\n",
      "            score_cutoff: Optional argument for score threshold. No matches with\n",
      "                a score less than this number will be returned. Defaults to 0.\n",
      "            limit: Optional maximum for the number of elements returned. Defaults\n",
      "                to 5.\n",
      "        \n",
      "        Returns: A a list of (match, score) tuples.\n",
      "    \n",
      "    extractOne(query, choices, processor=<function full_process at 0x7f06af223710>, scorer=<function WRatio at 0x7f06af2250e0>, score_cutoff=0)\n",
      "        Find the single best match above a score in a list of choices.\n",
      "        \n",
      "        This is a convenience method which returns the single best choice.\n",
      "        See extract() for the full arguments list.\n",
      "        \n",
      "        Args:\n",
      "            query: A string to match against\n",
      "            choices: A list or dictionary of choices, suitable for use with\n",
      "                extract().\n",
      "            processor: Optional function for transforming choices before matching.\n",
      "                See extract().\n",
      "            scorer: Scoring function for extract().\n",
      "            score_cutoff: Optional argument for score threshold. If the best\n",
      "                match is found, but it is not greater than this number, then\n",
      "                return None anyway (\"not a good enough match\").  Defaults to 0.\n",
      "        \n",
      "        Returns:\n",
      "            A tuple containing a single match and its score, if a match\n",
      "            was found that was above score_cutoff. Otherwise, returns None.\n",
      "    \n",
      "    extractWithoutOrder(query, choices, processor=<function full_process at 0x7f06af223710>, scorer=<function WRatio at 0x7f06af2250e0>, score_cutoff=0)\n",
      "        Select the best match in a list or dictionary of choices.\n",
      "        \n",
      "        Find best matches in a list or dictionary of choices, return a\n",
      "        generator of tuples containing the match and its score. If a dictionary\n",
      "        is used, also returns the key for each match.\n",
      "        \n",
      "        Arguments:\n",
      "            query: An object representing the thing we want to find.\n",
      "            choices: An iterable or dictionary-like object containing choices\n",
      "                to be matched against the query. Dictionary arguments of\n",
      "                {key: value} pairs will attempt to match the query against\n",
      "                each value.\n",
      "            processor: Optional function of the form f(a) -> b, where a is the query or\n",
      "                individual choice and b is the choice to be used in matching.\n",
      "        \n",
      "                This can be used to match against, say, the first element of\n",
      "                a list:\n",
      "        \n",
      "                lambda x: x[0]\n",
      "        \n",
      "                Defaults to fuzzywuzzy.utils.full_process().\n",
      "            scorer: Optional function for scoring matches between the query and\n",
      "                an individual processed choice. This should be a function\n",
      "                of the form f(query, choice) -> int.\n",
      "        \n",
      "                By default, fuzz.WRatio() is used and expects both query and\n",
      "                choice to be strings.\n",
      "            score_cutoff: Optional argument for score threshold. No matches with\n",
      "                a score less than this number will be returned. Defaults to 0.\n",
      "        \n",
      "        Returns:\n",
      "            Generator of tuples containing the match and its score.\n",
      "        \n",
      "            If a list is used for choices, then the result will be 2-tuples.\n",
      "            If a dictionary is used, then the result will be 3-tuples containing\n",
      "            the key for each match.\n",
      "        \n",
      "            For example, searching for 'bird' in the dictionary\n",
      "        \n",
      "            {'bard': 'train', 'dog': 'man'}\n",
      "        \n",
      "            may return\n",
      "        \n",
      "            ('train', 22, 'bard'), ('man', 0, 'dog')\n",
      "\n",
      "FILE\n",
      "    /home/anya_m/Documents/venv/lib64/python3.7/site-packages/fuzzywuzzy/process.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## !!VERY SLOW!!! \n",
    "# # Took 186 minutes to run for 10000 articles\n",
    "\n",
    "# #fuzzy.process.dedupe function returns a list without duplicates. by default it is using 70% similarity ratio\n",
    "# #to explore similarity ratio for individual words use fuzzy.process.extract i.e. process.extract('angela merkel',unique_keywords,limit=20)\n",
    "\n",
    "# print(len(unique_keywords))\n",
    "# ded_kw=dedupe(unique_keywords)\n",
    "# print(len(ded_kw))\n",
    "\n",
    "# #write the deduplicated keywords into the file\n",
    "# pd.Series(list(ded_kw)).to_csv('../data/interim/out_dedupl_10k_articles_only_100323.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read from csv isntead of running DEDUP\n",
    "ded_kw=pd.read_csv('../data/interim/out_dedupl_10k_articles_only_100323.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>daniel bieler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>EADS Astrium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Chosen Soren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Gabbana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11906</th>\n",
       "      <td>11906</td>\n",
       "      <td>dowry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11907</th>\n",
       "      <td>11907</td>\n",
       "      <td>Hellfire missiles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11908</th>\n",
       "      <td>11908</td>\n",
       "      <td>Carlos Moltini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11909</th>\n",
       "      <td>11909</td>\n",
       "      <td>boj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11910</th>\n",
       "      <td>11910</td>\n",
       "      <td>exceutive pay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11911 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                  0\n",
       "0               0      daniel bieler\n",
       "1               1       EADS Astrium\n",
       "2               2       Chosen Soren\n",
       "3               3            Gabbana\n",
       "4               4                DJV\n",
       "...           ...                ...\n",
       "11906       11906              dowry\n",
       "11907       11907  Hellfire missiles\n",
       "11908       11908     Carlos Moltini\n",
       "11909       11909                boj\n",
       "11910       11910      exceutive pay\n",
       "\n",
       "[11911 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ded_kw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring ratio of similarity for individual  keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('angela merkel',unique_keywords,limit=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #example from the fuzzywuzzy tooturial on token ratio\n",
    "# Str1 = \"The supreme court case of Nixon vs The United States\"\n",
    "# Str2 = \"Nixon v. United States\"\n",
    "# Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "# Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "# Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
    "# Token_Set_Ratio = fuzz.token_set_ratio(Str1,Str2)\n",
    "# print(Ratio)\n",
    "# print(Partial_Ratio)\n",
    "# print(Token_Sort_Ratio)\n",
    "# print(Token_Set_Ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=df['keywordStrings'] # is keywords in strings\n",
    "keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
