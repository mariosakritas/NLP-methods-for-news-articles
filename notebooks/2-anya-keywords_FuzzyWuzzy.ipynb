{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team DatenWelle\n",
    "\n",
    "## Keyword merging with FuzzyWuzzy\n",
    "\n",
    "This notebook loads the data from JSON format and performs some keyword cleaning and merging misspelled duplicates with fuzzyWuuzy package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git pull\n",
    "#!git status\n",
    "#!git add 2-anya-keywords_FuzzyWuzzy.ipynb #1-anya_exploratory_analysis.ipynb\n",
    "#!git commit -m 'added a thing to put keywords back into the dataframe (incomplete)'\n",
    "#!git push\n",
    "#!pip install -r ../requirements.txt\n",
    "\n",
    "#after installed new libraries\n",
    "#!pip freeze > requirements.txt\n",
    "#!git add requirements.txt \n",
    "#!git add out_dedupl_100323.csv test.csv\n",
    "#!git commit -m 'added output files f fuzzy wuzzy dedupe'\n",
    "#!git commit -m 'added library fuzzywuzzy'\n",
    "#!git push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy.process import dedupe\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/raw/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the subset of the data for 1 Jan 2019 - 1 Jan 2020 based on lastModifiedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='lastModifiedDate') #sort dataframe\n",
    "\n",
    "datetimes = pd.to_datetime(df['lastModifiedDate'])\n",
    "df['ts_lastModifiedDate']=datetimes\n",
    "#df.iloc[ts_start]['ts_lastModifiedDate']\n",
    "\n",
    "#find start index for subset 2019-2022\n",
    "ts_start=datetimes[(datetimes > pd.Timestamp(year=2019, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2019, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_start)\n",
    "#find end date for subset 2019-2022\n",
    "ts_end=datetimes[(datetimes > pd.Timestamp(year=2022, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2022, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_end)\n",
    "\n",
    "start_date=datetimes[datetimes == ts_start]\n",
    "end_date=datetimes[datetimes == ts_end]\n",
    "\n",
    "#find index for the chosen start and end dates\n",
    "start_index=start_date.index[0]\n",
    "print(start_index)\n",
    "df[df.index == start_date.index[0]]\n",
    "\n",
    "end_index=end_date.index[0]\n",
    "print(end_index)\n",
    "df[df.index == end_date.index[0]]\n",
    "\n",
    "df_subset=df[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_subset=df_subset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_subset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords'] # is keywords in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of keywords sets\n",
    "def get_keywords(row):\n",
    "    if row is None:\n",
    "        return None\n",
    "    else:\n",
    "        res_set = set()\n",
    "        for name_dict in row:\n",
    "            res_set.add(name_dict['name'])\n",
    "        return res_set\n",
    "\n",
    "df['keywords'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract individual keywords from the sets of sets\n",
    "\n",
    "# should work but it is very slow for now with current gpus\n",
    "# 10000 articles in 7 seconds\n",
    "# df_subset (90090 articles) runs in 10 minutes 10 seconds\n",
    "\n",
    "# sets=df_subset['keywords'].apply(get_keyword1) #full dataset\n",
    "sets=df_subset['keywords'].apply(get_keywords)  #2019-2021 subset\n",
    "#sets=sets[0:10000] #10000 articles\n",
    " \n",
    "kw=functools.reduce(set.union, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # another way to extract individual keywords from the sets of sets that doesn't crash kernel is interrupted\n",
    "# # so it might be more stable when later applied to the entire dataset\n",
    "\n",
    "# #runs for 10000 articles in 5 seconds\n",
    "# #runs for df_subset in  11 min 16 sec \n",
    "\n",
    "# sets=df_subset['keywords'].apply(get_keywords)\n",
    "# #sets=sets[0:10000]\n",
    "# from tqdm import tqdm\n",
    "# def get_unique_keywords(sets):\n",
    "#     result_set = set()\n",
    "#     for row_set in tqdm(sets.values):\n",
    "#         #result_set.union(row_set)\n",
    "#         result_set = result_set.union(row_set)\n",
    "#     return result_set\n",
    "\n",
    "# unique_keywords = get_unique_keywords(sets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the set of DW keywords before fuzzyWuzzy into the file\n",
    "pd.Series(list(unique_keywords)).to_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy.csv')\n",
    "pd.Series(list(kw)).to_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_keywords=kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load set of DW keywords before fuzzyWuzzy into the file\n",
    "uni_kw=pd.read_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keywords=set(uni_kw['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets_10000=sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with FuzzyWuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function from tutorial to get simplest matching ratio\n",
    "\n",
    "# Str1 = \"Apple Inc.\"\n",
    "# Str2 = \"apple Inc\"\n",
    "# Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "# print(Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(fuzz.token_set_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !!VERY SLOW!!! \n",
    "# Took 186 minutes to run for 10000 articles\n",
    "\n",
    "#fuzzy.process.dedupe function returns a list without duplicates. by default it is using 70% similarity ratio\n",
    "#to explore similarity ratio for individual words use fuzzy.process.extract i.e. process.extract('angela merkel',unique_keywords,limit=20)\n",
    "\n",
    "print(len(unique_keywords))\n",
    "#ded_kw=dedupe(unique_keywords)\n",
    "ded_kw=dedupe(unique_keywords,threshold=90)\n",
    "print(len(ded_kw))\n",
    "\n",
    "#write the deduplicated keywords into the file\n",
    "#pd.Series(list(ded_kw)).to_csv('../data/interim/out_dedupl_10k_articles_only_100323.csv')\n",
    "pd.Series(list(ded_kw)).to_csv('../data/interim/out_dedupl_2019-2021_articles_only_100323.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read from csv isntead of running DEDUP\n",
    "#ded_kw=pd.read_csv('../data/interim/out_dedupl_10k_articles_only_100323.csv')\n",
    "ded_kw=pd.read_csv('../data/interim/out_dedupl_2019-2021_articles_only_100323.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated=set(ded_kw['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring ratio of similarity for individual  keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('angela merkel',unique_keywords,limit=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('planetary defense conference',unique_keywords,limit=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('Chosen Soren',unique_keywords,limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('Sex pistols',unique_keywords,limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('UEFA',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('UAE',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('United Arab Emirates',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('war in Ukraine',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('UK',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('United Kingdom',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #example from the fuzzywuzzy tutorial on token ratio\n",
    "# Str1 = \"The supreme court case of Nixon vs The United States\"\n",
    "# Str2 = \"Nixon v. United States\"\n",
    "# Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "# Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "# Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
    "# Token_Set_Ratio = fuzz.token_set_ratio(Str1,Str2)\n",
    "# print(Ratio)\n",
    "# print(Partial_Ratio)\n",
    "# print(Token_Sort_Ratio)\n",
    "# print(Token_Set_Ratio)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Putting back\" merged clean keywords into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kw=list(unique_keywords)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['keywordStrings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# len(df_subset['keywordStrings'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #n=0\n",
    "# df_subset['keywordStrings'][i][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) loop over each line in dataframe\n",
    "# 2) loop over each keyword in the line\n",
    "# 3) find process.extractOne a substitute from deduplicated list\n",
    "# 4) create a new column in dataframe with merged keywords\n",
    "\n",
    "\n",
    "def put_clean_kw_into_df(dataframe):\n",
    "    i = 0\n",
    "    sample_line = []\n",
    "    sample_ratio = []\n",
    "    while i <= len(dataframe): # 10:\n",
    "        print(i)\n",
    "        n = 0\n",
    "        line_wr = []\n",
    "        ratio_line_wr = []\n",
    "        #print(line_wr)\n",
    "        while n < len(dataframe[i]):\n",
    "            #print(process.extractOne(df_subset['keywordStrings'][i][n],deduplicated)) #print word and ratio \n",
    "            line_wr.append(process.extractOne(dataframe[i][n], deduplicated)[0])\n",
    "            ratio_line_wr.append(process.extractOne(dataframe[i][n], deduplicated)[1])\n",
    "            n = n+1\n",
    "\n",
    "        sample_line.append(line_wr)\n",
    "        sample_ratio.append(ratio_line_wr)\n",
    "\n",
    "        i = i+1\n",
    "    return sample_line\n",
    "        \n",
    "    \n",
    "df_lines=put_clean_kw_into_df(df_subset['keywordStrings'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['keywordStrings'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out list comprehension later\n",
    "# i=0\n",
    "# def put_clean_kw_into_df(dataframe):\n",
    "#     return [process.extractOne(dataframe[n], deduplicated)[0] for dataframe[n] in dataframe]\n",
    "\n",
    "# put_clean_kw_into_df(df_subset['keywordStrings'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_wr.append(str(word_ratio_line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ratio_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['cleaned_keywordStrings']=word_ratio[0]\n",
    "df_subset['cleaned_sim_ratio_keywordStrings']=word_ratio[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['cleaned_keywordStrings'][i][n]=word_ratio[0]\n",
    "df_subset['cleaned_sim_ratio_keywordStrings'][i][n]=word_ratio[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['cleaned_keywordStrings'][0][0]='lalal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
