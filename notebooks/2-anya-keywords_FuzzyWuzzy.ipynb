{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team DatenWelle\n",
    "\n",
    "## Keyword merging with FuzzyWuzzy\n",
    "\n",
    "This notebook loads the data from JSON format and performs some keyword cleaning and merging misspelled duplicates with fuzzyWuuzy package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git pull\n",
    "#!git status\n",
    "#!git add 2-anya-keywords_RapidFuzz.ipynb 2-anya-keywords_to_dataframe_vector.ipynb #2-anya-keywords_FuzzyWuzzy.ipynb #1-anya_exploratory_analysis.ipynb\n",
    "#!git commit -m 'playing with RapidFuzz (its much faster than FuzzyWuzzy!)'\n",
    "#!git push\n",
    "#!pip install -r ../requirements.txt\n",
    "\n",
    "#after installed new libraries\n",
    "#!pip freeze > requirements.txt\n",
    "#!git add requirements.txt \n",
    "#!git add out_dedupl_100323.csv test.csv\n",
    "#!git commit -m 'added output files f fuzzy wuzzy dedupe'\n",
    "#!git commit -m 'added library rapidfuzy'\n",
    "#!git push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy.process import dedupe\n",
    "import functools\n",
    "from rapidfuzz import process as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/raw/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the subset of the data for 1 Jan 2019 - 1 Jan 2020 based on lastModifiedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 03:57:28.904000+00:00\n",
      "2022-01-01 02:35:51.098000+00:00\n",
      "60278\n",
      "150367\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by='lastModifiedDate') #sort dataframe\n",
    "\n",
    "datetimes = pd.to_datetime(df['lastModifiedDate'])\n",
    "df['ts_lastModifiedDate']=datetimes\n",
    "#df.iloc[ts_start]['ts_lastModifiedDate']\n",
    "\n",
    "#find start index for subset 2019-2022\n",
    "ts_start=datetimes[(datetimes > pd.Timestamp(year=2019, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2019, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_start)\n",
    "#find end date for subset 2019-2022\n",
    "ts_end=datetimes[(datetimes > pd.Timestamp(year=2022, month=1, day=1).tz_localize('utc')) \n",
    "          & (datetimes < pd.Timestamp(year=2022, month=1, day=2).tz_localize('utc'))].min()\n",
    "print(ts_end)\n",
    "\n",
    "start_date=datetimes[datetimes == ts_start]\n",
    "end_date=datetimes[datetimes == ts_end]\n",
    "\n",
    "#find index for the chosen start and end dates\n",
    "start_index=start_date.index[0]\n",
    "print(start_index)\n",
    "df[df.index == start_date.index[0]]\n",
    "\n",
    "end_index=end_date.index[0]\n",
    "print(end_index)\n",
    "df[df.index == end_date.index[0]]\n",
    "\n",
    "df_subset=df[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33830"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_subset=df_subset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_subset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords'] # is keywords in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series of keywords sets\n",
    "def get_keywords(row):\n",
    "    if row is None:\n",
    "        return None\n",
    "    else:\n",
    "        res_set = set()\n",
    "        for name_dict in row:\n",
    "            res_set.add(name_dict['name'])\n",
    "        return res_set\n",
    "\n",
    "#df['keywords'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract individual keywords from the sets of sets\n",
    "\n",
    "# should work but it is very slow for now with current gpus\n",
    "# 10000 articles in 7 seconds\n",
    "# df_subset (90090 articles) runs in 10 minutes 10 seconds\n",
    "\n",
    "# sets=df_subset['keywords'].apply(get_keyword1) #full dataset\n",
    "sets=df_subset['keywords'].apply(get_keywords)  #2019-2021 subset\n",
    "#sets=sets[0:10000] #10000 articles\n",
    " \n",
    "kw=functools.reduce(set.union, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32704"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_kw=(pd.DataFrame(kw, columns = ['keyword']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32704"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(uni_kw['keyword']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 step putting everything in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after splitting: 30404\n"
     ]
    }
   ],
   "source": [
    "substring = ', '\n",
    "\n",
    "list_multikw = [kw for kw in keywords_clean if substring in kw] # keywords that did not get split\n",
    "new_keywords = [kw.split(substring) for kw in list_multikw] # make a list of new keywords (the splited multi kw)\n",
    "\n",
    "# Flatten list of list of new keywords\n",
    "flat_new_keywords = [item for sublist in new_keywords for item in sublist]\n",
    "flat_new_keywords.remove('') # remove empty values\n",
    "\n",
    "# Remove the non-seperated keywords\n",
    "for el in list_multikw:\n",
    "    keywords_clean.remove(el)\n",
    "\n",
    "# Add the seperated ones\n",
    "keywords_clean = keywords_clean + flat_new_keywords\n",
    "\n",
    "print('after splitting:', len(keywords_clean)) # number is higher because split long keyowrds into multiplev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 3: remove '\\u2002' and '.' and '\\xa0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30405\n"
     ]
    }
   ],
   "source": [
    "# Identify all single keywords that contain commas, splits them in mutliple keywords and saves this in a list\n",
    "new_keywords = []\n",
    "list_kw_todel = []\n",
    "substring = ', '\n",
    "for i,s in enumerate(keywords_clean):\n",
    "    if substring in s:\n",
    "        #print('row', i, ':',  s)\n",
    "        list_kw_todel.append(s)\n",
    "        new_keywords.append(keywords_clean[i].split(substring))\n",
    "\n",
    "flat_new_keywords = [item for sublist in new_keywords for item in sublist]\n",
    "\n",
    "# Remove the non-seperated keywords\n",
    "for el in list_kw_todel:\n",
    "    keywords_clean.remove(el)\n",
    "\n",
    "# Add the seperated ones\n",
    "keywords_clean = keywords_clean + flat_new_keywords\n",
    "\n",
    "print(len(keywords_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # another way to extract individual keywords from the sets of sets that doesn't crash kernel is interrupted\n",
    "# # so it might be more stable when later applied to the entire dataset\n",
    "\n",
    "# #runs for 10000 articles in 5 seconds\n",
    "# #runs for df_subset in  11 min 16 sec \n",
    "\n",
    "# sets=df_subset['keywords'].apply(get_keywords)\n",
    "# #sets=sets[0:10000]\n",
    "# from tqdm import tqdm\n",
    "# def get_unique_keywords(sets):\n",
    "#     result_set = set()\n",
    "#     for row_set in tqdm(sets.values):\n",
    "#         #result_set.union(row_set)\n",
    "#         result_set = result_set.union(row_set)\n",
    "#     return result_set\n",
    "\n",
    "# unique_keywords = get_unique_keywords(sets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the set of DW keywords before fuzzyWuzzy into the file\n",
    "# pd.Series(list(unique_keywords)).to_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy.csv')\n",
    "kw=keywords_clean\n",
    "pd.Series(list(kw)).to_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy_1503.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_keywords=kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load set of DW keywords before fuzzyWuzzy into the file\n",
    "#uni_kw=pd.read_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy.csv') \n",
    "uni_kw=pd.read_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy_1503.csv', names = ['ind', 'keyword'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>markus braun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>product development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>eu energy transition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>non-swimmers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>graft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ind               keyword\n",
       "0    0          markus braun\n",
       "1    1   product development\n",
       "2    2  eu energy transition\n",
       "3    3          non-swimmers\n",
       "4    4                 graft"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(uni_kw['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_keywords=set(uni_kw['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets_10000=sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with FuzzyWuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(fuzz.token_set_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## !!VERY SLOW!!! \n",
    "# # Took 186 minutes to run for 10000 articles\n",
    "# # took 35 minutes for 10000 keywords\n",
    "\n",
    "# #fuzzy.process.dedupe function returns a list without duplicates. by default it is using 70% similarity ratio\n",
    "# #to explore similarity ratio for individual words use fuzzy.process.extract i.e. process.extract('angela merkel',unique_keywords,limit=20)\n",
    "\n",
    "# print(len(unique_keywords))\n",
    "# #ded_kw=dedupe(unique_keywords)\n",
    "# #ded_kw=dedupe(unique_keywords, threshold = 90)\n",
    "# ded_kw=dedupe(list(unique_keywords)[:10000], threshold = 90)\n",
    "# print(len(ded_kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #write the deduplicated keywords into the file\n",
    "# #pd.Series(list(ded_kw)).to_csv('../data/interim/out_dedupl_10k_articles_only_100323.csv')\n",
    "# pd.Series(list(ded_kw)).to_csv('../data/interim/out_dedupl_10k_kw_only_140323_threshold_90.csv')\n",
    "# #pd.Series(list(ded_kw)).to_csv('../data/interim/out_dedupl_2019-2021_articles_only_100323_threshold_90.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #read from csv isntead of running DEDUP\n",
    "# #ded_kw=pd.read_csv('../data/interim/out_dedupl_10k_articles_only_100323.csv')\n",
    "# ded_kw=pd.read_csv('../data/interim/out_dedupl_2019-2021_articles_only_100323.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the RapidFuzz https://maxbachmann.github.io/RapidFuzz/Usage/process.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from rapidfuzz import process, fuzz #\n",
    "help(rapidfuzz.process.cdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process as pr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load set of DW keywords before fuzzyWuzzy into the file\n",
    "uni_kw=pd.read_csv('../data/interim/out_2019-2021_keywords_before_FuzzyWuzzy.csv') \n",
    "#unique_keywords=uni_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(list(uni_kw['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 20k by 20k is analysed in  19.6 sec!!!\n",
    "# for 32704 (2years of data) by 32704 ratios are calculated in 26 sec cut off 70\n",
    "\n",
    "#ratio_array=pr.cdist(unique_keywords,unique_keywords,score_cutoff = 70)\n",
    "ratio_array= pr.cdist(list(uni_kw['0']),list(uni_kw['0']),score_cutoff = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = pd.DataFrame(ratio_array, columns = list(uni_kw['0']), index=list(uni_kw['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rapidfuzz import process as pr\n",
    "\n",
    "#process.extract(\"new york jets\", choices, scorer=fuzz.WRatio, limit=2)\n",
    "#rapidfuzz.process.extract('angela merkel',unique_keywords,scorer=fuzz.WRatio,limit=40)\n",
    "pr.extract('angela merkel',unique_keywords,  limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated #=ded_kw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring ratio of similarity for individual  keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('angela merkel',unique_keywords,limit=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('planetary defense conference',unique_keywords,limit=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('Chosen Soren',unique_keywords,limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('Sex pistols',unique_keywords,limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('UEFA',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('UAE',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('United Arab Emirates',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('war in Ukraine',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('UK',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extract('United Kingdom',unique_keywords,limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #example from the fuzzywuzzy tutorial on token ratio\n",
    "# Str1 = \"The supreme court case of Nixon vs The United States\"\n",
    "# Str2 = \"Nixon v. United States\"\n",
    "# Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "# Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "# Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
    "# Token_Set_Ratio = fuzz.token_set_ratio(Str1,Str2)\n",
    "# print(Ratio)\n",
    "# print(Partial_Ratio)\n",
    "# print(Token_Sort_Ratio)\n",
    "# print(Token_Set_Ratio)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Putting back\" merged clean keywords into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kw=list(unique_keywords)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['keywordStrings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# len(df_subset['keywordStrings'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #n=0\n",
    "# df_subset['keywordStrings'][i][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## speeding up put_clean_kw_into_df\n",
    "# # 1) apply to every over each line in dataframe\n",
    "# # 2) loop over each keyword in the line\n",
    "# # 3) find process.extractOne a substitute from deduplicated list\n",
    "# # 4) create a new column in dataframe with merged keywords\n",
    "\n",
    "# def put_clean_kw_into_df_fast(dataframe):\n",
    "#     #df.applymap(lambda x: len(str(x)))\n",
    "\n",
    "#     dataframe.applymap\n",
    "\n",
    "#     return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) loop over each line in dataframe\n",
    "# # 2) loop over each keyword in the line\n",
    "# # 3) find process.extractOne a substitute from deduplicated list\n",
    "# # 4) create a new column in dataframe with merged keywords\n",
    "\n",
    "\n",
    "# def put_clean_kw_into_df(dataframe):\n",
    "#     i = 0\n",
    "#     sample_line = []\n",
    "#     sample_ratio = []\n",
    "#     while i <= 5: #len(dataframe): # 10:\n",
    "#         print(i)\n",
    "#         n = 0\n",
    "#         line_wr = []\n",
    "#         ratio_line_wr = []\n",
    "#         #print(line_wr)\n",
    "#         while n < len(dataframe[i]):\n",
    "#             #print(process.extractOne(df_subset['keywordStrings'][i][n],deduplicated)) #print word and ratio \n",
    "#             line_wr.append(process.extractOne(dataframe[i][n], deduplicated)[0])\n",
    "#             ratio_line_wr.append(process.extractOne(dataframe[i][n], deduplicated)[1])\n",
    "#             n = n+1\n",
    "\n",
    "#         sample_line.append(line_wr)\n",
    "#         sample_ratio.append(ratio_line_wr)\n",
    "\n",
    "#         i = i+1\n",
    "#     return sample_line\n",
    "        \n",
    "    \n",
    "# df_lines=put_clean_kw_into_df(df_subset['keywordStrings'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['keywordStrings'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df_lines).to_csv('../data/interim/out_df_lines_10k_kw_2years_subset_mess.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['keywordStrings'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out list comprehension later\n",
    "# i=0\n",
    "# def put_clean_kw_into_df(dataframe):\n",
    "#     return [process.extractOne(dataframe[n], deduplicated)[0] for dataframe[n] in dataframe]\n",
    "\n",
    "# put_clean_kw_into_df(df_subset['keywordStrings'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
